<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"akazaakane.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"giscus","storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"manual","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="Google Translate System Design IntroductionGoogle Translate is a widely used language-translation service offered by Google. Powered by machine-learning (ML) models, it translates text between more th">
<meta property="og:type" content="article">
<meta property="og:title" content="Design Google Translate">
<meta property="og:url" content="https://akazaakane.github.io/2025/06/08/Design%20Google%20Translate/index.html">
<meta property="og:site_name" content="随便写写">
<meta property="og:description" content="Google Translate System Design IntroductionGoogle Translate is a widely used language-translation service offered by Google. Powered by machine-learning (ML) models, it translates text between more th">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://akazaakane.github.io/img/2025/06/0301.webp">
<meta property="og:image" content="https://akazaakane.github.io/img/2025/06/f4e0ac0aa42ac2c8e1f8bac85dcc60dc20926b7c047fe6c379ee7e72dae921ea.webp">
<meta property="article:published_time" content="2025-06-08T05:00:00.000Z">
<meta property="article:modified_time" content="2025-06-11T22:21:13.340Z">
<meta property="article:author" content="yxckeis8">
<meta property="article:tag" content="技术">
<meta property="article:tag" content="Sytem Design">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://akazaakane.github.io/img/2025/06/0301.webp">

<link rel="canonical" href="https://akazaakane.github.io/2025/06/08/Design%20Google%20Translate/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>Design Google Translate | 随便写写</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">随便写写</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">随便写写 - 想写就写吧</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://akazaakane.github.io/2025/06/08/Design%20Google%20Translate/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/img/avatar.png">
      <meta itemprop="name" content="yxckeis8">
      <meta itemprop="description" content="芙蓉王源">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="随便写写">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Design Google Translate
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-06-08 00:00:00" itemprop="dateCreated datePublished" datetime="2025-06-08T00:00:00-05:00">2025-06-08</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-06-11 17:21:13" itemprop="dateModified" datetime="2025-06-11T17:21:13-05:00">2025-06-11</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="Google-Translate-System-Design"><a href="#Google-Translate-System-Design" class="headerlink" title="Google Translate System Design"></a>Google Translate System Design</h1><hr>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>Google Translate is a widely used language-translation service offered by Google. Powered by machine-learning (ML) models, it translates text between more than <strong>130 languages</strong> and serves <strong>over a billion users</strong> as of 2024.</p>
<p><img src="/img/2025/06/0301.webp" alt="Google Translate Overview"></p>
<hr>
<h2 id="Clarifying-Requirements"><a href="#Clarifying-Requirements" class="headerlink" title="Clarifying Requirements"></a>Clarifying Requirements</h2><ol>
<li>Real-Time translation vs Batch Translation (Model Architecture)</li>
<li>Text vs Audio vs Visual (Multi-modal)</li>
<li>Cloud vs On-Device (Model size, Inference Optimization)</li>
<li>Bilingual vs Multilingual</li>
</ol>
<p>To simplify the problem, we will limit the scope to Batch, Multilingual, Text and Cloud translation system.</p>
<hr>
<h2 id="Frame-the-Problem-as-an-ML-Task"><a href="#Frame-the-Problem-as-an-ML-Task" class="headerlink" title="Frame the Problem as an ML Task"></a>Frame the Problem as an ML Task</h2><h3 id="Specifying-the-System’s-Input-and-Output"><a href="#Specifying-the-System’s-Input-and-Output" class="headerlink" title="Specifying the System’s Input and Output"></a>Specifying the System’s Input and Output</h3><ul>
<li><strong>Input:</strong> A sequence of words in the <strong>source language</strong> and the <strong>desired target language</strong>.  </li>
<li><strong>Output:</strong> A sequence of words in the <strong>target language</strong>.</li>
</ul>
<h3 id="Choosing-a-Suitable-ML-Approach"><a href="#Choosing-a-Suitable-ML-Approach" class="headerlink" title="Choosing a Suitable ML Approach"></a>Choosing a Suitable ML Approach</h3><p>Language translation is a classic <strong>sequence-to-sequence (seq2seq)</strong> task. Modern systems favor <strong>Transformer-based encoder-decoder models</strong>:</p>
<ol>
<li><strong>Encoder</strong> – Converts the source sentence into contextual vectors.  </li>
<li><strong>Decoder</strong> – Generates the target sentence token-by-token, attending to the encoder’s output.</li>
</ol>
<p>Why Transformers?</p>
<ul>
<li>Handle <strong>long-range dependencies</strong> better than LSTMs/GRUs.  </li>
<li>The <strong>attention mechanism</strong> (originally proposed for translation [2]) lets the decoder focus on relevant source tokens.  </li>
<li>Encoder–decoder separation aligns naturally with translation.</li>
</ul>
<p>Other models Choices</p>
<ul>
<li>Non-Autoregressive Transformers (NAT): Decodes all tokens in parallel → much faster, but typically lower quality. Often used for speed-sensitive applications Can be combined with knowledge distillation to improve performance.</li>
<li>Prompt-based Translation using Decoder-only LLMs. Zero/few-shot with no retraining but Larger and slower than specialized MT models.</li>
</ul>
<hr>
<h2 id="Data-Preparation"><a href="#Data-Preparation" class="headerlink" title="Data Preparation"></a>Data Preparation</h2><p>Two data types feed the model:</p>
<ol>
<li><strong>General data</strong> – Large-scale multilingual text from the internet.  </li>
<li><strong>Translation data</strong> – ≈ 300 million sentence pairs (source + target).</li>
</ol>
<p>Preparation focuses on translation data.</p>
<h4 id="1-Primary-Data-Parallel-Corpora"><a href="#1-Primary-Data-Parallel-Corpora" class="headerlink" title="1. Primary Data: Parallel Corpora"></a>1. Primary Data: Parallel Corpora</h4><p>This is the most crucial data, consisting of texts that are direct, sentence-by-sentence translations of each other. It forms the core training material for the model.</p>
<ul>
<li>Public Datasets: High-quality, formal corpora from official sources like the United Nations (UN) and the European Parliament (Europarl). These provide a strong, clean baseline.</li>
<li>Web Crawling: Automatically crawling multilingual websites (like news sites or corporate pages) to find and extract translated pages. This provides a massive, diverse, but often “noisy” source of data that requires significant cleaning.</li>
</ul>
<h4 id="2-Data-Expansion-and-Augmentation"><a href="#2-Data-Expansion-and-Augmentation" class="headerlink" title="2. Data Expansion and Augmentation"></a>2. Data Expansion and Augmentation</h4><p>Because high-quality parallel data is limited, several techniques are used to create more training examples:</p>
<ul>
<li>User Feedback: Leveraging the “Suggest an edit” feature and contributions from the Translate Community. This creates a continuous improvement loop by using real-world corrections to fix model weaknesses.</li>
<li>Back-Translation: A powerful technique for low-resource languages. It involves taking monolingual text in the target language (e.g., German), using an existing model to translate it back to the source language (English), and then using this new synthetic {source, target} pair for training. This dramatically improves the translation’s fluency.</li>
<li>Mining Comparable Corpora: This method is used when texts are about the same topic but are not direct translations (e.g., English and German news articles about the same event).</li>
</ul>
<h3 id="1-·-Text-Pre-processing"><a href="#1-·-Text-Pre-processing" class="headerlink" title="1 · Text Pre-processing"></a>1 · Text Pre-processing</h3><ul>
<li>Remove <strong>missing</strong> or <strong>noisy</strong> pairs (HTML, wrong language).  </li>
<li><strong>Deduplicate</strong> sentence pairs.  </li>
<li><strong>Handle named entities</strong> with placeholders (e.g., <code>ENTITY_1</code>).  </li>
<li>Skip older steps (lower-casing, stop-word removal, stemming, punctuation stripping) because modern Transformers learn these patterns directly.</li>
</ul>
<hr>
<h2 id="Preprocessing-Steps-—-Modern-Translation-Pretraining"><a href="#Preprocessing-Steps-—-Modern-Translation-Pretraining" class="headerlink" title="Preprocessing Steps — Modern Translation Pretraining"></a><strong>Preprocessing Steps — Modern Translation Pretraining</strong></h2><h3 id="🔎-1️⃣-Data-Cleaning-Deduplication"><a href="#🔎-1️⃣-Data-Cleaning-Deduplication" class="headerlink" title="🔎 1️⃣ Data Cleaning & Deduplication"></a>🔎 <strong>1️⃣ Data Cleaning &amp; Deduplication</strong></h3><table>
<thead>
<tr>
<th>Task</th>
<th>Purpose</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Remove noisy sentences</strong></td>
<td>Drop sentence pairs with very long or very short sequences, or mismatched alignments.</td>
</tr>
<tr>
<td><strong>Filter profanity / sensitive content</strong></td>
<td>Ensure safe outputs.</td>
</tr>
<tr>
<td><strong>Deduplicate</strong></td>
<td>Remove duplicate sentence pairs and repeated monolingual data (web crawl has lots of duplication).</td>
</tr>
<tr>
<td><strong>Script normalization</strong></td>
<td>Normalize Unicode (NFC/NFD), convert different scripts consistently (e.g., Simplified ↔ Traditional Chinese).</td>
</tr>
</tbody></table>
<p>✅ <strong>Why:</strong> Reduces noise → improves model generalization.</p>
<hr>
<h3 id="🔎-2️⃣-Language-Identification-Tagging"><a href="#🔎-2️⃣-Language-Identification-Tagging" class="headerlink" title="🔎 2️⃣ Language Identification & Tagging"></a>🔎 <strong>2️⃣ Language Identification &amp; Tagging</strong></h3><table>
<thead>
<tr>
<th>Task</th>
<th>Purpose</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Language detection</strong></td>
<td>Auto-identify language of monolingual data (and confirm for parallel pairs).</td>
</tr>
<tr>
<td><strong>Assign language tags</strong></td>
<td>E.g., add &gt;&gt;fr&lt;&lt; or &gt;&gt;zh&lt;&lt; to source text so the model knows which language to translate into.</td>
</tr>
</tbody></table>
<p>✅ <strong>Why:</strong> Enables multilingual pretraining and zero-shot/few-shot transfer.</p>
<hr>
<h3 id="🔎-3️⃣-Tokenization"><a href="#🔎-3️⃣-Tokenization" class="headerlink" title="🔎 3️⃣ Tokenization"></a>🔎 <strong>3️⃣ Tokenization</strong></h3><table>
<thead>
<tr>
<th>Task</th>
<th>Purpose</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Subword tokenization (SentencePiece, BPE, WordPiece)</strong></td>
<td>Split words into common subword units to handle rare/unseen words.</td>
</tr>
<tr>
<td><strong>Multilingual vocabulary</strong></td>
<td>Train a shared tokenizer across all languages.</td>
</tr>
</tbody></table>
<p>✅ <strong>Why:</strong> Handles vocabulary for hundreds of languages in a scalable way.</p>
<hr>
<h3 id="🔎-4️⃣-Alignment-Verification-Parallel-Data"><a href="#🔎-4️⃣-Alignment-Verification-Parallel-Data" class="headerlink" title="🔎 4️⃣ Alignment Verification (Parallel Data)"></a>🔎 <strong>4️⃣ Alignment Verification (Parallel Data)</strong></h3><table>
<thead>
<tr>
<th>Task</th>
<th>Purpose</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Length ratio checks</strong></td>
<td>Filter out sentence pairs with extreme length mismatches.</td>
</tr>
<tr>
<td><strong>Translation quality scoring (optional)</strong></td>
<td>Use tools like LASER or BLEU filtering to keep only high-quality sentence pairs.</td>
</tr>
</tbody></table>
<p>✅ <strong>Why:</strong> Ensures that paired data teaches the model correct alignments.</p>
<hr>
<h3 id="🔎-5️⃣-Corruption-Masking-for-Monolingual"><a href="#🔎-5️⃣-Corruption-Masking-for-Monolingual" class="headerlink" title="🔎 5️⃣ Corruption / Masking (for Monolingual)"></a>🔎 <strong>5️⃣ Corruption / Masking (for Monolingual)</strong></h3><table>
<thead>
<tr>
<th>Task</th>
<th>Purpose</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Noise injection (shuffling, masking, deletion)</strong></td>
<td>Prepare monolingual data for Denoising Auto-Encoding (DAE) or MLM tasks.</td>
</tr>
</tbody></table>
<p>✅ <strong>Why:</strong> Teaches encoder-decoder models to recover from corrupted inputs → boosts robustness and learning.</p>
<hr>
<h3 id="🔎-6️⃣-Sampling-Balancing"><a href="#🔎-6️⃣-Sampling-Balancing" class="headerlink" title="🔎 6️⃣ Sampling & Balancing"></a>🔎 <strong>6️⃣ Sampling &amp; Balancing</strong></h3><table>
<thead>
<tr>
<th>Task</th>
<th>Purpose</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Upsampling low-resource languages</strong></td>
<td>Increase frequency of rare languages in the training stream.</td>
</tr>
<tr>
<td><strong>Downsampling high-resource languages</strong></td>
<td>Prevent overfitting to dominant languages like English or Spanish.</td>
</tr>
</tbody></table>
<p>✅ <strong>Why:</strong> Balances the multilingual training data.</p>
<hr>
<h3 id="🔎-7️⃣-Back-Translation-for-Monolingual-→-Synthetic-Parallel"><a href="#🔎-7️⃣-Back-Translation-for-Monolingual-→-Synthetic-Parallel" class="headerlink" title="🔎 7️⃣ Back-Translation (for Monolingual → Synthetic Parallel)"></a>🔎 <strong>7️⃣ Back-Translation (for Monolingual → Synthetic Parallel)</strong></h3><table>
<thead>
<tr>
<th>Task</th>
<th>Purpose</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Translate monolingual target sentences back into source languages</strong></td>
<td>Create synthetic parallel data when human-translated data is missing.</td>
</tr>
</tbody></table>
<p>✅ <strong>Why:</strong> Expands training data for low-resource pairs.</p>
<hr>
<h3 id="2-·-Text-Tokenization"><a href="#2-·-Text-Tokenization" class="headerlink" title="2 · Text Tokenization"></a>2 · Text Tokenization</h3><p>Word-level vocabularies explode in size across 130+ languages, so we use <strong>sub-word tokenization</strong>—specifically <strong>Byte-Pair Encoding (BPE)</strong> .</p>
<p>Example token-to-ID mapping:</p>
<table>
<thead>
<tr>
<th>Token</th>
<th>ID</th>
</tr>
</thead>
<tbody><tr>
<td><code>&lt;BOS&gt;</code></td>
<td>0</td>
</tr>
<tr>
<td><code>&lt;EOS&gt;</code></td>
<td>1</td>
</tr>
<tr>
<td>walking</td>
<td>2</td>
</tr>
<tr>
<td>bonjour</td>
<td>3</td>
</tr>
<tr>
<td>hello</td>
<td>4</td>
</tr>
<tr>
<td>fantastique</td>
<td>5</td>
</tr>
<tr>
<td>…</td>
<td>…</td>
</tr>
</tbody></table>
<hr>
<h3 id="1-Byte-Pair-Encoding-BPE"><a href="#1-Byte-Pair-Encoding-BPE" class="headerlink" title="1. Byte-Pair Encoding (BPE)"></a>1. <strong>Byte-Pair Encoding (BPE)</strong></h3><p><strong>Used in</strong>: GPT-2, RoBERTa</p>
<ul>
<li><strong>Idea</strong>: Start from characters, iteratively merge the most frequent pairs of tokens.</li>
<li><strong>Goal</strong>: Build a vocabulary of subwords to reduce OOV (out-of-vocabulary) issues.</li>
<li><strong>Deterministic</strong>: Given a trained vocabulary, tokenization is consistent.</li>
</ul>
<p><strong>Example</strong>:<br>Training corpus: <code>"low lower newest widest"</code></p>
<ol>
<li>Start with chars: <code>l o w</code>, <code>l o w e r</code>, etc.</li>
<li>Find most common pair (e.g., <code>'e'</code> + <code>'r'</code> → <code>'er'</code>), merge into <code>'er'</code></li>
<li>Repeat until vocab size is met.</li>
</ol>
<p>Tokenize <code>"lower"</code> → <code>['low', 'er']</code> if <code>'low'</code> and <code>'er'</code> exist in vocab.</p>
<hr>
<h3 id="2-Byte-Level-BPE-BBPE"><a href="#2-Byte-Level-BPE-BBPE" class="headerlink" title="2. Byte-Level BPE (BBPE)"></a>2. <strong>Byte-Level BPE (BBPE)</strong></h3><p><strong>Used in</strong>: GPT-3, GPT-4</p>
<ul>
<li><strong>Extension of BPE</strong>, but operates on <strong>bytes</strong>, not Unicode characters.</li>
<li>Handles all inputs (e.g., emojis, unseen scripts) without needing pre-normalization.</li>
<li>More robust to misspellings and multilingual input.</li>
<li>deals with unseen or non-standard characters best， since all text (even unknown Unicode symbols) can be represented as bytes</li>
</ul>
<p><strong>Example</strong>:<br>Input: <code>"apple 🍎"</code></p>
<ol>
<li>Break into raw bytes: <code>['a', 'p', 'p', 'l', 'e', ' ', '🍎']</code></li>
<li>Apply merges on bytes to form subwords: <code>['app', 'le', ' ', '🍎']</code> (assuming <code>'app'</code> and <code>'le'</code> exist)</li>
</ol>
<hr>
<h3 id="3-Unigram-Language-Model"><a href="#3-Unigram-Language-Model" class="headerlink" title="3. Unigram Language Model"></a>3. <strong>Unigram Language Model</strong></h3><p><strong>Used in</strong>: SentencePiece (T5, ALBERT)</p>
<ul>
<li>Trains a <strong>probabilistic model</strong> of subword units.</li>
<li>Starts with many subwords, gradually removes the least likely ones.</li>
<li>At inference, selects the most likely token sequence via Viterbi.</li>
</ul>
<p><strong>Example</strong>: <code>"international"</code><br>May be tokenized as <code>['inter', 'national']</code> or <code>['intern', 'ation', 'al']</code> depending on highest probability.</p>
<hr>
<h3 id="4-WordPiece"><a href="#4-WordPiece" class="headerlink" title="4. WordPiece"></a>4. <strong>WordPiece</strong></h3><p><strong>Used in</strong>: BERT</p>
<ul>
<li>Similar to BPE, but selects merges to <strong>maximize likelihood</strong> of training data under a language model.</li>
<li>Always starts with a <strong>whole word</strong>, splits into subwords with <code>##</code> prefix for non-initial parts.</li>
</ul>
<p><strong>Example</strong>: <code>"unhappiness"</code><br>→ <code>['un', '##happiness']</code> or <code>['un', '##hap', '##pi', '##ness']</code> depending on vocab.</p>
<hr>
<h3 id="Summary-Table"><a href="#Summary-Table" class="headerlink" title="Summary Table:"></a>Summary Table:</h3><table>
<thead>
<tr>
<th>Algorithm</th>
<th>Used In</th>
<th>Key Feature</th>
<th>Handles OOV?</th>
<th>Probabilistic?</th>
</tr>
</thead>
<tbody><tr>
<td>BPE</td>
<td>GPT-2, RoBERTa</td>
<td>Merges frequent char pairs</td>
<td>Yes</td>
<td>No</td>
</tr>
<tr>
<td>Byte-level BPE</td>
<td>GPT-3, GPT-4</td>
<td>Merges on byte sequences</td>
<td>Yes</td>
<td>No</td>
</tr>
<tr>
<td>WordPiece</td>
<td>BERT</td>
<td>Greedy merging with LM scoring</td>
<td>Yes</td>
<td>No</td>
</tr>
<tr>
<td>Unigram LM</td>
<td>T5, ALBERT</td>
<td>Picks best subwords probabilistically</td>
<td>Yes</td>
<td>Yes</td>
</tr>
</tbody></table>
<p>Let me know if you’d like to visualize token merges step by step.</p>
<hr>
<h2 id="Model-Development"><a href="#Model-Development" class="headerlink" title="Model Development"></a>Model Development</h2><h3 id="Architecture-Overview"><a href="#Architecture-Overview" class="headerlink" title="Architecture Overview"></a>Architecture Overview</h3><p>Encoder-decoder Transformer components:</p>
<table>
<thead>
<tr>
<th>Component</th>
<th>Encoder</th>
<th>Decoder</th>
</tr>
</thead>
<tbody><tr>
<td>Token Embedding</td>
<td>✅</td>
<td>✅</td>
</tr>
<tr>
<td>Positional Encoding</td>
<td>✅</td>
<td>✅</td>
</tr>
<tr>
<td><strong>Self-Attention</strong></td>
<td>Full (bi-directional)</td>
<td>Masked (causal)</td>
</tr>
<tr>
<td><strong>Cross-Attention</strong></td>
<td>—</td>
<td>✅</td>
</tr>
<tr>
<td>Prediction Head</td>
<td>—</td>
<td>✅</td>
</tr>
</tbody></table>
<p>Key differences:</p>
<ol>
<li><strong>Cross-Attention Layer</strong> – Decoder attends to encoder outputs.  </li>
<li><strong>Masked Self-Attention</strong> – Decoder can’t see future tokens.  </li>
<li><strong>Prediction Head</strong> – Linear + soft-max layer produces token probabilities.</li>
</ol>
<p>Other key information:</p>
<ul>
<li>Positional Embeddings: Rope, Sinusoidal</li>
<li>Residual Connection</li>
<li>MOE architecture</li>
<li>Self-Attention, Cross-Attention</li>
</ul>
<h3 id="Training-Strategy-by-Alex"><a href="#Training-Strategy-by-Alex" class="headerlink" title="Training Strategy by Alex"></a>Training Strategy by Alex</h3><table>
<thead>
<tr>
<th>Stage</th>
<th>Data</th>
<th>Objective</th>
<th>Loss</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Unsupervised Pre-training</strong></td>
<td>Multilingual general text</td>
<td><strong>Masked Language Modeling (MLM)</strong></td>
<td>Cross-entropy</td>
</tr>
<tr>
<td><strong>Supervised Finetuning</strong></td>
<td>Parallel sentence pairs (bilingual)</td>
<td><strong>Next-token prediction</strong></td>
<td>Cross-entropy</td>
</tr>
</tbody></table>
<ul>
<li>Bilingual vs. Multilingual:* This design opts for <strong>bilingual models</strong> (higher accuracy per pair, at higher ops cost).</li>
</ul>
<h3 id="ML-Objective-Loss-Function-—-Revised-✍️"><a href="#ML-Objective-Loss-Function-—-Revised-✍️" class="headerlink" title="ML Objective & Loss Function — Revised ✍️"></a>ML Objective &amp; Loss Function — Revised ✍️</h3><p>Below is a tighter, more technically precise version of the “ML objective and loss function” section, organised to show <em>why</em> each design choice matters when training a Neural Machine Translation (NMT) system.</p>
<hr>
<h4 id="1-Pre-training-Learning-Multilingual-Representations"><a href="#1-Pre-training-Learning-Multilingual-Representations" class="headerlink" title="1  |  Pre-training: Learning Multilingual Representations"></a>1  |  Pre-training: Learning Multilingual Representations</h4><p><strong>Data</strong><br>We keep the multilingual portions of large-scale corpora (C4, Wikipedia, Stack Exchange, Common-Crawl, etc.) and drop only the languages we never intend to translate. This maximises coverage while avoiding label noise from irrelevant scripts.</p>
<p><strong>Objective</strong> — <em>Masked Language Modeling</em> (MLM) with span corruption</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Input  :  The quick brown &lt;mask&gt; jumps over the &lt;mask&gt; dog .</span><br><span class="line">Target :                    fox                      lazy</span><br></pre></td></tr></table></figure>

<ul>
<li>Why not next-token prediction? In unsupervised pre-training the decoder could simply echo the current token, so the encoder would learn little. MLM forces the encoder to build a genuine contextual representation that the decoder must rely on.</li>
<li>Span masking (T5-style) hides contiguous chunks, encouraging longer-range reasoning and reducing the number of prediction steps.</li>
<li>15 % of tokens are selected; 80 % are replaced by <code>&lt;mask&gt;</code>, 10 % by random tokens, 10 % left unchanged to give the model noise robustness.</li>
</ul>
<p><strong>Loss</strong> — Cross-entropy over <em>only</em> the masked positions</p>

<mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -2.83ex;" xmlns="http://www.w3.org/2000/svg" width="24.219ex" height="4.979ex" role="img" focusable="false" viewBox="0 -950 10704.7 2200.9"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="4C" d="M62 -22T47 -22T32 -11Q32 -1 56 24T83 55Q113 96 138 172T180 320T234 473T323 609Q364 649 419 677T531 705Q559 705 578 696T604 671T615 645T618 623V611Q618 582 615 571T598 548Q581 531 558 520T518 509Q503 509 503 520Q503 523 505 536T507 560Q507 590 494 610T452 630Q423 630 410 617Q367 578 333 492T271 301T233 170Q211 123 204 112L198 103L224 102Q281 102 369 79T509 52H523Q535 64 544 87T579 128Q616 152 641 152Q656 152 656 142Q656 101 588 40T433 -22Q381 -22 289 1T156 28L141 29L131 20Q111 0 87 -11Z"></path></g></g><g data-mml-node="TeXAtom" transform="translate(723,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mtext"><path data-c="4D" d="M132 622Q125 629 121 631T105 634T62 637H29V683H135Q221 683 232 682T249 675Q250 674 354 398L458 124L562 398Q666 674 668 675Q671 681 683 682T781 683H887V637H854Q814 636 803 634T785 622V61Q791 51 802 49T854 46H887V0H876Q855 3 736 3Q605 3 596 0H585V46H618Q660 47 669 49T688 61V347Q688 424 688 461T688 546T688 613L687 632Q454 14 450 7Q446 1 430 1T410 7Q409 9 292 316L176 624V606Q175 588 175 543T175 463T175 356L176 86Q187 50 261 46H278V0H269Q254 3 154 3Q52 3 37 0H29V46H46Q78 48 98 56T122 69T132 86V622Z"></path><path data-c="4C" d="M128 622Q121 629 117 631T101 634T58 637H25V683H36Q48 680 182 680Q324 680 348 683H360V637H333Q273 637 258 635T233 622L232 342V129Q232 57 237 52Q243 47 313 47Q384 47 410 53Q470 70 498 110T536 221Q536 226 537 238T540 261T542 272T562 273H582V268Q580 265 568 137T554 5V0H25V46H58Q100 47 109 49T128 61V622Z" transform="translate(917,0)"></path><path data-c="4D" d="M132 622Q125 629 121 631T105 634T62 637H29V683H135Q221 683 232 682T249 675Q250 674 354 398L458 124L562 398Q666 674 668 675Q671 681 683 682T781 683H887V637H854Q814 636 803 634T785 622V61Q791 51 802 49T854 46H887V0H876Q855 3 736 3Q605 3 596 0H585V46H618Q660 47 669 49T688 61V347Q688 424 688 461T688 546T688 613L687 632Q454 14 450 7Q446 1 430 1T410 7Q409 9 292 316L176 624V606Q175 588 175 543T175 463T175 356L176 86Q187 50 261 46H278V0H269Q254 3 154 3Q52 3 37 0H29V46H46Q78 48 98 56T122 69T132 86V622Z" transform="translate(1542,0)"></path></g></g></g><g data-mml-node="mo" transform="translate(2789.6,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mo" transform="translate(3845.3,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="munder" transform="translate(4790,0)"><g data-mml-node="mo" transform="translate(60.4,0)"><path data-c="2211" d="M60 948Q63 950 665 950H1267L1325 815Q1384 677 1388 669H1348L1341 683Q1320 724 1285 761Q1235 809 1174 838T1033 881T882 898T699 902H574H543H251L259 891Q722 258 724 252Q725 250 724 246Q721 243 460 -56L196 -356Q196 -357 407 -357Q459 -357 548 -357T676 -358Q812 -358 896 -353T1063 -332T1204 -283T1307 -196Q1328 -170 1348 -124H1388Q1388 -125 1381 -145T1356 -210T1325 -294L1267 -449L666 -450Q64 -450 61 -448Q55 -446 55 -439Q55 -437 57 -433L590 177Q590 178 557 222T452 366T322 544L56 909L55 924Q55 945 60 948Z"></path></g><g data-mml-node="TeXAtom" transform="translate(0,-1115.5) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(345,0)"><path data-c="2208" d="M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(1012,0)"><g data-mml-node="mi"><path data-c="4D" d="M28 9Q28 37 43 63T73 90Q77 90 83 84T103 70T141 57H146Q162 57 178 79T222 167Q266 279 295 371T334 513T349 598T358 651T371 677Q397 705 432 705Q442 705 445 699T452 666Q453 661 453 659Q475 538 509 405T568 207L574 192Q581 178 587 164T594 150Q596 150 635 189T693 248Q765 324 863 438T1024 626T1089 701Q1093 705 1100 705Q1111 705 1111 682Q1111 675 1108 660T1099 611T1086 540Q1041 277 1041 144Q1041 98 1044 75T1050 48T1059 42Q1064 41 1075 46Q1102 61 1121 61Q1137 61 1137 50Q1137 28 1087 0T1000 -29Q983 -29 972 -23T955 -9T945 16T942 45T941 83V96Q941 158 952 256T974 422L985 489Q984 489 939 436T821 300T698 164Q665 128 620 85T568 37Q564 34 558 34Q550 34 546 37T535 54Q512 91 496 127T450 259T389 498L384 518Q349 367 294 223T198 15Q155 -50 117 -50Q87 -50 61 -35T30 -6Q28 2 28 9Z"></path></g></g></g></g><g data-mml-node="mi" transform="translate(6521.5,0)"><path data-c="6C" d="M42 46H56Q95 46 103 60V68Q103 77 103 91T103 124T104 167T104 217T104 272T104 329Q104 366 104 407T104 482T104 542T103 586T103 603Q100 622 89 628T44 637H26V660Q26 683 28 683L38 684Q48 685 67 686T104 688Q121 689 141 690T171 693T182 694H185V379Q185 62 186 60Q190 52 198 49Q219 46 247 46H263V0H255L232 1Q209 2 183 2T145 3T107 3T57 1L34 0H26V46H42Z"></path><path data-c="6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z" transform="translate(278,0)"></path><path data-c="67" d="M329 409Q373 453 429 453Q459 453 472 434T485 396Q485 382 476 371T449 360Q416 360 412 390Q410 404 415 411Q415 412 416 414V415Q388 412 363 393Q355 388 355 386Q355 385 359 381T368 369T379 351T388 325T392 292Q392 230 343 187T222 143Q172 143 123 171Q112 153 112 133Q112 98 138 81Q147 75 155 75T227 73Q311 72 335 67Q396 58 431 26Q470 -13 470 -72Q470 -139 392 -175Q332 -206 250 -206Q167 -206 107 -175Q29 -140 29 -75Q29 -39 50 -15T92 18L103 24Q67 55 67 108Q67 155 96 193Q52 237 52 292Q52 355 102 398T223 442Q274 442 318 416L329 409ZM299 343Q294 371 273 387T221 404Q192 404 171 388T145 343Q142 326 142 292Q142 248 149 227T179 192Q196 182 222 182Q244 182 260 189T283 207T294 227T299 242Q302 258 302 292T299 343ZM403 -75Q403 -50 389 -34T348 -11T299 -2T245 0H218Q151 0 138 -6Q118 -15 107 -34T95 -74Q95 -84 101 -97T122 -127T170 -155T250 -167Q319 -167 361 -139T403 -75Z" transform="translate(778,0)"></path></g><g data-mml-node="mo" transform="translate(7799.5,0)"><path data-c="2061" d=""></path></g><g data-mml-node="msub" transform="translate(7966.2,0)"><g data-mml-node="mi"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></g><g data-mml-node="mi" transform="translate(536,-150) scale(0.707)"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g></g><g data-mml-node="mo" transform="translate(8883.8,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(9272.8,0)"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mi" transform="translate(749,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(10315.7,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container>

<p>with optional label smoothing (ε ≈ 0.1) for better generalisation.</p>
<hr>
<h4 id="2-Supervised-Fine-tuning-Turning-a-General-LM-into-a-Translator"><a href="#2-Supervised-Fine-tuning-Turning-a-General-LM-into-a-Translator" class="headerlink" title="2  |  Supervised Fine-tuning: Turning a General LM into a Translator"></a>2  |  Supervised Fine-tuning: Turning a General LM into a Translator</h4><p>With sentence-aligned pairs <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="5.513ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 2436.7 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(389,0)"><g data-mml-node="mi"><path data-c="1D431" d="M227 0Q212 3 121 3Q40 3 28 0H21V62H117L245 213L109 382H26V444H34Q49 441 143 441Q247 441 265 444H274V382H246L281 339Q315 297 316 297Q320 297 354 341L389 382H352V444H360Q375 441 466 441Q547 441 559 444H566V382H471L355 246L504 63L545 62H586V0H578Q563 3 469 3Q365 3 347 0H338V62H366Q366 63 326 112T285 163L198 63L217 62H235V0H227Z"></path></g></g><g data-mml-node="mo" transform="translate(996,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(1440.7,0)"><g data-mml-node="mi"><path data-c="1D432" d="M84 -102Q84 -110 87 -119T102 -138T133 -149Q148 -148 162 -143T186 -131T206 -114T222 -95T234 -76T243 -59T249 -45T252 -37L269 0L96 382H26V444H34Q49 441 146 441Q252 441 270 444H279V382H255Q232 382 232 380L337 151L442 382H394V444H401Q413 441 495 441Q568 441 574 444H580V382H510L406 152Q298 -84 297 -87Q269 -139 225 -169T131 -200Q85 -200 54 -172T23 -100Q23 -64 44 -50T87 -35Q111 -35 130 -50T152 -92V-100H84V-102Z"></path></g></g><g data-mml-node="mo" transform="translate(2047.7,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container>:</p>
<ol>
<li><strong>Encoder</strong> ingests the full source sentence <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="1.373ex" height="1.005ex" role="img" focusable="false" viewBox="0 -444 607 444"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D431" d="M227 0Q212 3 121 3Q40 3 28 0H21V62H117L245 213L109 382H26V444H34Q49 441 143 441Q247 441 265 444H274V382H246L281 339Q315 297 316 297Q320 297 354 341L389 382H352V444H360Q375 441 466 441Q547 441 559 444H566V382H471L355 246L504 63L545 62H586V0H578Q563 3 469 3Q365 3 347 0H338V62H366Q366 63 326 112T285 163L198 63L217 62H235V0H227Z"></path></g></g></g></g></svg></mjx-container>.  </li>
<li><strong>Decoder</strong> is trained with <em>teacher forcing</em>: it receives the gold target prefix <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.452ex;" xmlns="http://www.w3.org/2000/svg" width="3.383ex" height="1.457ex" role="img" focusable="false" viewBox="0 -444 1495.4 644"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D432" d="M84 -102Q84 -110 87 -119T102 -138T133 -149Q148 -148 162 -143T186 -131T206 -114T222 -95T234 -76T243 -59T249 -45T252 -37L269 0L96 382H26V444H34Q49 441 146 441Q252 441 270 444H279V382H255Q232 382 232 380L337 151L442 382H394V444H401Q413 441 495 441Q568 441 574 444H580V382H510L406 152Q298 -84 297 -87Q269 -139 225 -169T131 -200Q85 -200 54 -172T23 -100Q23 -64 44 -50T87 -35Q111 -35 130 -50T152 -92V-100H84V-102Z"></path></g></g><g data-mml-node="TeXAtom" transform="translate(640,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><path data-c="3C" d="M694 -11T694 -19T688 -33T678 -40Q671 -40 524 29T234 166L90 235Q83 240 83 250Q83 261 91 266Q664 540 678 540Q681 540 687 534T694 519T687 505Q686 504 417 376L151 250L417 124Q686 -4 687 -5Q694 -11 694 -19Z"></path></g><g data-mml-node="mi" transform="translate(778,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g></g></g></g></g></svg></mjx-container> and predicts the next token <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.452ex;" xmlns="http://www.w3.org/2000/svg" width="2.139ex" height="1.457ex" role="img" focusable="false" viewBox="0 -444 945.3 644"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D432" d="M84 -102Q84 -110 87 -119T102 -138T133 -149Q148 -148 162 -143T186 -131T206 -114T222 -95T234 -76T243 -59T249 -45T252 -37L269 0L96 382H26V444H34Q49 441 146 441Q252 441 270 444H279V382H255Q232 382 232 380L337 151L442 382H394V444H401Q413 441 495 441Q568 441 574 444H580V382H510L406 152Q298 -84 297 -87Q269 -139 225 -169T131 -200Q85 -200 54 -172T23 -100Q23 -64 44 -50T87 -35Q111 -35 130 -50T152 -92V-100H84V-102Z"></path></g></g><g data-mml-node="mi" transform="translate(640,-150) scale(0.707)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g></g></g></g></svg></mjx-container>.  </li>
<li><strong>Loss</strong> — Token-level cross-entropy on <em>all</em> target positions</li>
</ol>

<mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -2.819ex;" xmlns="http://www.w3.org/2000/svg" width="29.353ex" height="7.044ex" role="img" focusable="false" viewBox="0 -1867.4 12974.1 3113.3"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="4C" d="M62 -22T47 -22T32 -11Q32 -1 56 24T83 55Q113 96 138 172T180 320T234 473T323 609Q364 649 419 677T531 705Q559 705 578 696T604 671T615 645T618 623V611Q618 582 615 571T598 548Q581 531 558 520T518 509Q503 509 503 520Q503 523 505 536T507 560Q507 590 494 610T452 630Q423 630 410 617Q367 578 333 492T271 301T233 170Q211 123 204 112L198 103L224 102Q281 102 369 79T509 52H523Q535 64 544 87T579 128Q616 152 641 152Q656 152 656 142Q656 101 588 40T433 -22Q381 -22 289 1T156 28L141 29L131 20Q111 0 87 -11Z"></path></g></g><g data-mml-node="TeXAtom" transform="translate(723,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mtext"><path data-c="43" d="M56 342Q56 428 89 500T174 615T283 681T391 705Q394 705 400 705T408 704Q499 704 569 636L582 624L612 663Q639 700 643 704Q644 704 647 704T653 705H657Q660 705 666 699V419L660 413H626Q620 419 619 430Q610 512 571 572T476 651Q457 658 426 658Q322 658 252 588Q173 509 173 342Q173 221 211 151Q232 111 263 84T328 45T384 29T428 24Q517 24 571 93T626 244Q626 251 632 257H660L666 251V236Q661 133 590 56T403 -21Q262 -21 159 83T56 342Z"></path><path data-c="45" d="M128 619Q121 626 117 628T101 631T58 634H25V680H597V676Q599 670 611 560T625 444V440H585V444Q584 447 582 465Q578 500 570 526T553 571T528 601T498 619T457 629T411 633T353 634Q266 634 251 633T233 622Q233 622 233 621Q232 619 232 497V376H286Q359 378 377 385Q413 401 416 469Q416 471 416 473V493H456V213H416V233Q415 268 408 288T383 317T349 328T297 330Q290 330 286 330H232V196V114Q232 57 237 52Q243 47 289 47H340H391Q428 47 452 50T505 62T552 92T584 146Q594 172 599 200T607 247T612 270V273H652V270Q651 267 632 137T610 3V0H25V46H58Q100 47 109 49T128 61V619Z" transform="translate(722,0)"></path></g></g></g><g data-mml-node="mo" transform="translate(2042.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mo" transform="translate(3098.6,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="munderover" transform="translate(4043.3,0)"><g data-mml-node="mo"><path data-c="2211" d="M60 948Q63 950 665 950H1267L1325 815Q1384 677 1388 669H1348L1341 683Q1320 724 1285 761Q1235 809 1174 838T1033 881T882 898T699 902H574H543H251L259 891Q722 258 724 252Q725 250 724 246Q721 243 460 -56L196 -356Q196 -357 407 -357Q459 -357 548 -357T676 -358Q812 -358 896 -353T1063 -332T1204 -283T1307 -196Q1328 -170 1348 -124H1388Q1388 -125 1381 -145T1356 -210T1325 -294L1267 -449L666 -450Q64 -450 61 -448Q55 -446 55 -439Q55 -437 57 -433L590 177Q590 178 557 222T452 366T322 544L56 909L55 924Q55 945 60 948Z"></path></g><g data-mml-node="TeXAtom" transform="translate(142.5,-1087.9) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mo" transform="translate(361,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(1139,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><g data-mml-node="TeXAtom" transform="translate(310.8,1237.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo" transform="translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(278,0)"><g data-mml-node="mi"><path data-c="1D432" d="M84 -102Q84 -110 87 -119T102 -138T133 -149Q148 -148 162 -143T186 -131T206 -114T222 -95T234 -76T243 -59T249 -45T252 -37L269 0L96 382H26V444H34Q49 441 146 441Q252 441 270 444H279V382H255Q232 382 232 380L337 151L442 382H394V444H401Q413 441 495 441Q568 441 574 444H580V382H510L406 152Q298 -84 297 -87Q269 -139 225 -169T131 -200Q85 -200 54 -172T23 -100Q23 -64 44 -50T87 -35Q111 -35 130 -50T152 -92V-100H84V-102Z"></path></g></g><g data-mml-node="mo" transform="translate(885,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g></g></g><g data-mml-node="mi" transform="translate(5654,0)"><path data-c="6C" d="M42 46H56Q95 46 103 60V68Q103 77 103 91T103 124T104 167T104 217T104 272T104 329Q104 366 104 407T104 482T104 542T103 586T103 603Q100 622 89 628T44 637H26V660Q26 683 28 683L38 684Q48 685 67 686T104 688Q121 689 141 690T171 693T182 694H185V379Q185 62 186 60Q190 52 198 49Q219 46 247 46H263V0H255L232 1Q209 2 183 2T145 3T107 3T57 1L34 0H26V46H42Z"></path><path data-c="6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z" transform="translate(278,0)"></path><path data-c="67" d="M329 409Q373 453 429 453Q459 453 472 434T485 396Q485 382 476 371T449 360Q416 360 412 390Q410 404 415 411Q415 412 416 414V415Q388 412 363 393Q355 388 355 386Q355 385 359 381T368 369T379 351T388 325T392 292Q392 230 343 187T222 143Q172 143 123 171Q112 153 112 133Q112 98 138 81Q147 75 155 75T227 73Q311 72 335 67Q396 58 431 26Q470 -13 470 -72Q470 -139 392 -175Q332 -206 250 -206Q167 -206 107 -175Q29 -140 29 -75Q29 -39 50 -15T92 18L103 24Q67 55 67 108Q67 155 96 193Q52 237 52 292Q52 355 102 398T223 442Q274 442 318 416L329 409ZM299 343Q294 371 273 387T221 404Q192 404 171 388T145 343Q142 326 142 292Q142 248 149 227T179 192Q196 182 222 182Q244 182 260 189T283 207T294 227T299 242Q302 258 302 292T299 343ZM403 -75Q403 -50 389 -34T348 -11T299 -2T245 0H218Q151 0 138 -6Q118 -15 107 -34T95 -74Q95 -84 101 -97T122 -127T170 -155T250 -167Q319 -167 361 -139T403 -75Z" transform="translate(778,0)"></path></g><g data-mml-node="mo" transform="translate(6932,0)"><path data-c="2061" d=""></path></g><g data-mml-node="msub" transform="translate(7098.6,0)"><g data-mml-node="mi"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></g><g data-mml-node="mi" transform="translate(536,-150) scale(0.707)"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g></g><g data-mml-node="mstyle" transform="translate(8016.3,0)"><g data-mml-node="mspace"></g></g><g data-mml-node="TeXAtom" data-mjx-texclass="OPEN" transform="translate(7849.3,0)"><g data-mml-node="mo" transform="translate(0 -0.5)"><path data-c="28" d="M152 251Q152 646 388 850H416Q422 844 422 841Q422 837 403 816T357 753T302 649T255 482T236 250Q236 124 255 19T301 -147T356 -251T403 -315T422 -340Q422 -343 416 -349H388Q359 -325 332 -296T271 -213T212 -97T170 56T152 251Z"></path></g></g><g data-mml-node="msub" transform="translate(8307.3,0)"><g data-mml-node="mi"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(523,-150) scale(0.707)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g></g><g data-mml-node="mo" transform="translate(9413.3,0)"><path data-c="2223" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="msub" transform="translate(9969.1,0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D432" d="M84 -102Q84 -110 87 -119T102 -138T133 -149Q148 -148 162 -143T186 -131T206 -114T222 -95T234 -76T243 -59T249 -45T252 -37L269 0L96 382H26V444H34Q49 441 146 441Q252 441 270 444H279V382H255Q232 382 232 380L337 151L442 382H394V444H401Q413 441 495 441Q568 441 574 444H580V382H510L406 152Q298 -84 297 -87Q269 -139 225 -169T131 -200Q85 -200 54 -172T23 -100Q23 -64 44 -50T87 -35Q111 -35 130 -50T152 -92V-100H84V-102Z"></path></g></g><g data-mml-node="TeXAtom" transform="translate(640,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><path data-c="3C" d="M694 -11T694 -19T688 -33T678 -40Q671 -40 524 29T234 166L90 235Q83 240 83 250Q83 261 91 266Q664 540 678 540Q681 540 687 534T694 519T687 505Q686 504 417 376L151 250L417 124Q686 -4 687 -5Q694 -11 694 -19Z"></path></g><g data-mml-node="mi" transform="translate(778,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g></g></g><g data-mml-node="mo" transform="translate(11464.5,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(11909.1,0)"><g data-mml-node="mi"><path data-c="1D431" d="M227 0Q212 3 121 3Q40 3 28 0H21V62H117L245 213L109 382H26V444H34Q49 441 143 441Q247 441 265 444H274V382H246L281 339Q315 297 316 297Q320 297 354 341L389 382H352V444H360Q375 441 466 441Q547 441 559 444H566V382H471L355 246L504 63L545 62H586V0H578Q563 3 469 3Q365 3 347 0H338V62H366Q366 63 326 112T285 163L198 63L217 62H235V0H227Z"></path></g></g><g data-mml-node="TeXAtom" data-mjx-texclass="CLOSE" transform="translate(12516.1,0)"><g data-mml-node="mo" transform="translate(0 -0.5)"><path data-c="29" d="M305 251Q305 -145 69 -349H56Q43 -349 39 -347T35 -338Q37 -333 60 -307T108 -239T160 -136T204 27T221 250T204 473T160 636T108 740T60 807T35 839Q35 850 50 850H56H69Q197 743 256 566Q305 425 305 251Z"></path></g></g></g></g></svg></mjx-container>


<p>plus the same label-smoothing trick.</p>
<hr>
<h3 id="Training-Strategy-for-Modern-Multilingual-NMT-Systems"><a href="#Training-Strategy-for-Modern-Multilingual-NMT-Systems" class="headerlink" title="Training Strategy for Modern Multilingual NMT Systems"></a><strong>Training Strategy for Modern Multilingual NMT Systems</strong></h3><p>Modern multilingual Neural Machine Translation (NMT) models, such as those used in services like Google Translate, follow a multi-stage training process to achieve high performance across many language pairs. The pipeline typically includes the following stages:</p>
<hr>
<h3 id="1-Pretraining-Cross-Lingual-Language-Modeling"><a href="#1-Pretraining-Cross-Lingual-Language-Modeling" class="headerlink" title="1. Pretraining (Cross-Lingual Language Modeling)"></a><strong>1. Pretraining (Cross-Lingual Language Modeling)</strong></h3><p><strong>Purpose:</strong><br>Learn universal language patterns and cross-lingual representations before any translation-specific training.</p>
<p><strong>Data Used:</strong><br>Large-scale monolingual corpora from many languages, often mined from web sources.</p>
<p><strong>Training Objectives:</strong></p>
<ul>
<li><strong>Masked Language Modeling (MLM)</strong>: Predict masked tokens in a sentence.</li>
<li><strong>Denoising Auto-Encoding</strong>: Reconstruct corrupted text sequences.</li>
<li><strong>Translation Language Modeling</strong> (less common): Learn cross-lingual alignment using bilingual input.</li>
</ul>
<p><strong>Model Behavior:</strong><br>The model learns general linguistic features across languages, which helps especially in low-resource scenarios.</p>
<hr>
<h3 id="2-Supervised-Fine-Tuning-Translation-Modeling"><a href="#2-Supervised-Fine-Tuning-Translation-Modeling" class="headerlink" title="2. Supervised Fine-Tuning (Translation Modeling)"></a><strong>2. Supervised Fine-Tuning (Translation Modeling)</strong></h3><p><strong>Purpose:</strong><br>Teach the model how to translate between languages using sentence-aligned parallel corpora.</p>
<p><strong>Data Used:</strong><br>Large-scale parallel corpora, mostly English-centric but also covering non-English pairs. Synthetic data (e.g., back-translated text) is also used to supplement low-resource languages.</p>
<p><strong>Training Strategy:</strong></p>
<ul>
<li>All language pairs are trained together in a single model (many-to-many).</li>
<li>Special tokens are added to indicate target language.</li>
<li>Shared subword vocabulary (e.g., BPE or SentencePiece) is used.</li>
<li>Sampling strategies are applied to balance low-resource and high-resource languages.</li>
</ul>
<p><strong>Augmentation Techniques:</strong></p>
<ul>
<li><strong>Back-Translation:</strong> Use a trained model to generate synthetic source sentences from target-language monolingual text.</li>
<li><strong>Knowledge Distillation:</strong> Train on simplified outputs from a teacher model.</li>
<li><strong>Forward-Translation:</strong> Less common, used if target-language monolingual data is limited.</li>
</ul>
<p><strong>Zero-shot Capability:</strong><br>A well-trained multilingual model can often translate between unseen language pairs by transferring knowledge from related pairs.</p>
<hr>
<h3 id="3-Domain-Adaptation-Specialized-Fine-Tuning"><a href="#3-Domain-Adaptation-Specialized-Fine-Tuning" class="headerlink" title="3. Domain Adaptation (Specialized Fine-Tuning)"></a><strong>3. Domain Adaptation (Specialized Fine-Tuning)</strong></h3><p><strong>Purpose:</strong><br>Improve translation quality in specific domains like healthcare, law, or conversational text.</p>
<p><strong>Data Used:</strong><br>Smaller, domain-specific parallel corpora, sometimes supplemented by synthetic domain data via back-translation.</p>
<p><strong>Training Strategy:</strong></p>
<ul>
<li>Fine-tune the general multilingual model on in-domain data.</li>
<li>Use techniques like early stopping, data mixing, or regularization to avoid overfitting.</li>
<li>Multi-stage adaptation (e.g., from multilingual → language pair → domain) often performs best.</li>
<li>Lightweight adapter modules can be added to specialize models per domain without altering the base model.</li>
</ul>
<hr>
<h3 id="Recent-Trends-and-Innovations"><a href="#Recent-Trends-and-Innovations" class="headerlink" title="Recent Trends and Innovations"></a><strong>Recent Trends and Innovations</strong></h3><ul>
<li><strong>Scaling Up:</strong> Large sparse models (e.g., Mixture-of-Experts) enable translation across hundreds of languages efficiently.</li>
<li><strong>Data Mining:</strong> Automated mining of high-quality sentence pairs from web data improves coverage, especially for low-resource languages.</li>
<li><strong>Balancing Techniques:</strong> Sampling strategies and vocabulary sharing improve performance across imbalanced datasets.</li>
<li><strong>LLMs for Translation:</strong> Large language models trained on multilingual data (e.g., GPT-4, PaLM) show strong translation abilities even without dedicated parallel data. They’re now being adapted for NMT use or employed to generate synthetic training data.</li>
<li><strong>Multitask and Modular Architectures:</strong> Supporting multiple tasks or inserting domain-specific adapters increases flexibility and robustness.</li>
</ul>
<hr>
<h3 id="End-to-End-Summary"><a href="#End-to-End-Summary" class="headerlink" title="End-to-End Summary"></a><strong>End-to-End Summary</strong></h3><p>Multilingual NMT systems are trained in stages—starting with monolingual self-supervised learning, followed by supervised translation training using real and synthetic parallel data, and optionally domain-specific fine-tuning. The training pipeline is carefully designed to handle large-scale, multilingual data while balancing quality across languages and domains. Ongoing advances in architecture, data quality, and LLM integration continue to push translation quality and coverage.</p>
<hr>
<h3 id="2-Training-Data-Collection"><a href="#2-Training-Data-Collection" class="headerlink" title="2. Training Data Collection"></a><strong>2. Training Data Collection</strong></h3><h4 id="A-Parallel-Corpora-Supervised"><a href="#A-Parallel-Corpora-Supervised" class="headerlink" title="A. Parallel Corpora (Supervised)"></a><strong>A. Parallel Corpora (Supervised)</strong></h4><ul>
<li><p><strong>Sources:</strong></p>
<ul>
<li><strong>Public Datasets:</strong> WMT, OPUS, Europarl, UN Corpus.</li>
<li><strong>Web Crawls:</strong> Filtered bilingual websites (e.g., Common Crawl, ParaCrawl).</li>
<li><strong>Official Documents:</strong> Government/legal text in multiple languages.</li>
</ul>
</li>
</ul>
<h4 id="B-Monolingual-Corpora-Unsupervised-Pretraining"><a href="#B-Monolingual-Corpora-Unsupervised-Pretraining" class="headerlink" title="B. Monolingual Corpora (Unsupervised/Pretraining)"></a><strong>B. Monolingual Corpora (Unsupervised/Pretraining)</strong></h4><ul>
<li><p><strong>Sources:</strong></p>
<ul>
<li><strong>C4, Wikipedia, CCNet, OSCAR, Gigaword, BookCorpus.</strong></li>
<li>Cleaned and language-labeled for the relevant set of translation languages.</li>
</ul>
</li>
</ul>
<h4 id="C-Quality-Filtering-Techniques"><a href="#C-Quality-Filtering-Techniques" class="headerlink" title="C. Quality Filtering Techniques"></a><strong>C. Quality Filtering Techniques</strong></h4><ul>
<li><strong>Length Ratio Filtering:</strong> Removes sentence pairs with large length mismatch.</li>
<li><strong>Language Identification:</strong> Ensures text is in the expected language.</li>
<li><strong>Dual-Model Agreement / BLEU Filtering:</strong> Keep only pairs with agreement between two translation models or high BLEU.</li>
<li><strong>Noise Detection:</strong> Drop automatically translated or low-confidence samples.</li>
</ul>
<h4 id="D-Data-Balancing"><a href="#D-Data-Balancing" class="headerlink" title="D. Data Balancing"></a><strong>D. Data Balancing</strong></h4><ul>
<li><p>Address <strong>low-resource language</strong> imbalance by:</p>
<ul>
<li><strong>Oversampling rare languages.</strong></li>
<li><strong>Sampling temperature strategies</strong> (e.g., square-root or inverse frequency).</li>
<li><strong>Synthetic data augmentation</strong> via back-translation or pivoting.</li>
</ul>
</li>
</ul>
<hr>
<h3 id="Sampling-Strategies-in-Generative-Models"><a href="#Sampling-Strategies-in-Generative-Models" class="headerlink" title="Sampling Strategies in Generative Models"></a>Sampling Strategies in Generative Models</h3><ol>
<li><strong>Deterministic methods</strong> (e.g., <em>greedy search</em>, <em>beam search</em>)</li>
<li><strong>Stochastic methods</strong> (e.g., <em>multinomial sampling</em>, <em>top-k</em>, <em>top-p</em>)</li>
</ol>
<p>In this chapter, we choose <strong>beam search</strong> for two key reasons:</p>
<h3 id="Why-Beam-Search"><a href="#Why-Beam-Search" class="headerlink" title="Why Beam Search?"></a>Why Beam Search?</h3><ul>
<li><p><strong>Translation Accuracy</strong>:<br>Beam search evaluates multiple candidate sequences and selects the most probable one, often resulting in more accurate and fluent translations.</p>
</li>
<li><p><strong>Consistency</strong>:<br>As a deterministic method, beam search always produces the same output given the same input. This is crucial for translation systems, where consistency and reliability outweigh diversity. Unlike creative applications, translation rarely benefits from random or surprising outputs.</p>
</li>
</ul>
<p>In contrast, <strong>stochastic sampling methods</strong> are better suited for tasks where <strong>diversity and creativity</strong> are desired, such as storytelling or dialogue generation. </p>
<hr>
<h2 id="Comparison-Deterministic-vs-Stochastic-Sampling"><a href="#Comparison-Deterministic-vs-Stochastic-Sampling" class="headerlink" title="Comparison: Deterministic vs. Stochastic Sampling"></a>Comparison: Deterministic vs. Stochastic Sampling</h2><table>
<thead>
<tr>
<th><strong>Characteristic</strong></th>
<th><strong>Deterministic Methods</strong></th>
<th><strong>Stochastic Methods</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>Approach</strong></td>
<td>Follows a predictable decision process</td>
<td>Samples from a probability distribution</td>
</tr>
<tr>
<td><strong>Efficiency</strong></td>
<td>Less efficient (tracks multiple paths)</td>
<td>More efficient due to randomness</td>
</tr>
<tr>
<td><strong>Output Quality</strong></td>
<td>Coherent and predictable</td>
<td>Diverse and imaginative</td>
</tr>
<tr>
<td><strong>Risk</strong></td>
<td>May produce repetitive sequences</td>
<td>May generate inappropriate or off-topic text</td>
</tr>
<tr>
<td><strong>Best For</strong></td>
<td>Tasks requiring consistency (e.g. translation)</td>
<td>Tasks requiring creativity (e.g. story generation)</td>
</tr>
<tr>
<td><strong>Common Methods</strong></td>
<td>Greedy search, beam search</td>
<td>Multinomial sampling, top-k, top-p sampling</td>
</tr>
</tbody></table>
<hr>
<h2 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h2><h3 id="Offline-Metrics"><a href="#Offline-Metrics" class="headerlink" title="Offline Metrics"></a>Offline Metrics</h3><h4 id="Key-Metrics"><a href="#Key-Metrics" class="headerlink" title="Key Metrics:"></a>Key Metrics:</h4><ul>
<li>BLEU (Bilingual Evaluation Understudy): One of the oldest and most widely used metrics, BLEU measures the n-gram precision between the machine-translated text and the reference translations. It focuses on the similarity of the output to high-quality human translations. While fast and easy to calculate, it can sometimes penalize translations that are semantically correct but lexically different from the references.</li>
<li>METEOR (Metric for Evaluation of Translation with Explicit ORdering): METEOR is an improvement upon BLEU that considers synonymy and stemming, leading to a better correlation with human judgment. It aligns words between the machine and reference translations and calculates a score based on precision, recall, and a fragmentation penalty.</li>
<li>COMET (Cross-lingual Optimized Metric for Evaluation of Translation): A more recent and powerful metric, COMET leverages large-scale pre-trained language models to assess the semantic similarity between the source text, the machine translation, and the reference translation. It has shown a very high correlation with human ratings.</li>
<li>BERTScore: This metric utilizes contextual embeddings from BERT to compare the similarity of tokens in the candidate and reference translations. This allows it to capture semantic nuances that n-gram-based metrics might miss.</li>
</ul>
<h4 id="Human-Evaluation"><a href="#Human-Evaluation" class="headerlink" title="Human Evaluation"></a>Human Evaluation</h4><ul>
<li>Adequacy and Fluency: This is a classic approach where human judges rate translations on two scales:</li>
<li>Adequacy: How well the meaning of the source text is preserved in the translation.</li>
<li>Fluency: How natural and grammatically correct the translated text sounds in the target language.</li>
<li>Direct Assessment (DA): In this method, human annotators assign a single, continuous score to a translation, often on a scale of 0 to 100, reflecting its overall quality. This approach has become increasingly popular in major translation conferences like WMT (Workshop on Machine Translation).</li>
<li>Ranking: Evaluators are presented with translations from multiple NMT systems for the same source sentence and are asked to rank them from best to worst. This is a relative measure that is useful for comparing the performance of different models.</li>
</ul>
<h4 id="LLM-as-a-Judge"><a href="#LLM-as-a-Judge" class="headerlink" title="LLM as a Judge"></a>LLM as a Judge</h4><p>The typical setup involves prompting an LLM with the source sentence, the candidate translation(s), and optionally a reference translation. The LLM is then instructed (via a carefully designed prompt) to perform tasks such as:</p>
<ul>
<li>Scoring translations on dimensions like adequacy, fluency, and faithfulness.</li>
<li>Choosing the better translation between multiple candidates (preference ranking).</li>
<li>Providing natural language feedback on errors or strengths in the output.</li>
</ul>
<p>LLMs can handle multilingual input and adapt to complex linguistic nuances, making them especially valuable for evaluating low-resource or morphologically rich languages where human evaluation is expensive or infeasible.</p>
<p>Companies often use few-shot or zero-shot prompting, and sometimes fine-tune LLMs with supervised data (e.g., human-labeled translation comparisons) to improve consistency and alignment with human judgments. These LLM-based evaluations correlate more closely with human ratings than BLEU scores and offer significant efficiency and scalability benefits.</p>
<p>For example:</p>
<ul>
<li>Google uses LLMs internally to benchmark translation quality across hundreds of languages.</li>
<li>OpenAI and others use LLMs to evaluate summarization, QA, and translation outputs in human preference studies.</li>
<li>Meta’s SEED framework and Anthropic’s Constitutional AI both involve LLM-based feedback loops to refine and assess generation quality.</li>
</ul>
<h3 id="Online-Metrics"><a href="#Online-Metrics" class="headerlink" title="Online Metrics"></a>Online Metrics</h3><ul>
<li><strong>User feedback</strong> – Rating prompt after translation (<em>Figure 3.25</em>).  </li>
<li><strong>User engagement</strong> – Usage frequency, session length, retention.</li>
</ul>
<hr>
<h2 id="Overall-ML-System-Design"><a href="#Overall-ML-System-Design" class="headerlink" title="Overall ML System Design"></a>Overall ML System Design</h2><h3 id="1-·-Language-Detector"><a href="#1-·-Language-Detector" class="headerlink" title="1 · Language Detector"></a>1 · Language Detector</h3><p>An <strong>encoder-only Transformer</strong> classifies the input language via:</p>
<ul>
<li><strong>Average pooling</strong> → prediction head, or  </li>
<li><strong>Last-token representation</strong> → prediction head.</li>
</ul>
<p><img src="/img/2025/06/f4e0ac0aa42ac2c8e1f8bac85dcc60dc20926b7c047fe6c379ee7e72dae921ea.webp" alt="Language Detector"></p>
<h3 id="2-·-Translation-Service"><a href="#2-·-Translation-Service" class="headerlink" title="2 · Translation Service"></a>2 · Translation Service</h3><ol>
<li>Detect language.  </li>
<li>Route to correct bilingual model.  </li>
<li>Perform beam search decoding.  </li>
<li>Detokenize and return text.</li>
</ol>
<hr>
<h2 id="Other-Talking-Points"><a href="#Other-Talking-Points" class="headerlink" title="Other Talking Points"></a>Other Talking Points</h2><p>If time remains, discuss:</p>
<ul>
<li>Model compression (quantization, distillation).  </li>
<li>On-device translation (Apple, offline mode).  </li>
<li>Handling new languages with limited corpora (few-shot, adapter layers).  </li>
<li>Real-time constraints for mobile typing suggestions.</li>
</ul>
<hr>
<h2 id="Real-Time-Translation"><a href="#Real-Time-Translation" class="headerlink" title="Real-Time Translation:"></a><strong>Real-Time Translation</strong>:</h2><h3 id="Replace-Full-Sequence-Encoder-Decoder-with-Streaming-Models"><a href="#Replace-Full-Sequence-Encoder-Decoder-with-Streaming-Models" class="headerlink" title="Replace Full-Sequence Encoder-Decoder with Streaming Models:"></a>Replace Full-Sequence Encoder-Decoder with <strong>Streaming Models</strong>:</h3><ul>
<li>Traditional Transformer waits for the <strong>entire source sentence</strong> before producing output → <strong>too slow</strong> for real-time.</li>
<li>Use a <strong>streaming architecture</strong> to process input incrementally:</li>
</ul>
<h4 id="Architectural-Options"><a href="#Architectural-Options" class="headerlink" title="Architectural Options:"></a>Architectural Options:</h4><table>
<thead>
<tr>
<th>Strategy</th>
<th>Description</th>
<th>Example</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Chunk-based Transformer</strong></td>
<td>Break input into fixed-size overlapping windows</td>
<td>SimulTrans, STACL</td>
</tr>
<tr>
<td><strong>Monotonic Attention</strong></td>
<td>Enforces left-to-right or wait-k policy</td>
<td>MoChA, Wait-k Transformer</td>
</tr>
<tr>
<td><strong>Transducer models</strong></td>
<td>Encoder-decoder-decoder (non-attention based)</td>
<td>RNN-T, Recurrent Neural Aligner</td>
</tr>
<tr>
<td><strong>Streaming Conformer</strong> (for speech)</td>
<td>Use <strong>causal attention</strong> and limited context for audio</td>
<td>Google’s real-time ST model</td>
</tr>
</tbody></table>
<h3 id="🛠-Techniques-to-Apply"><a href="#🛠-Techniques-to-Apply" class="headerlink" title="🛠 Techniques to Apply:"></a>🛠 Techniques to Apply:</h3><ul>
<li>Use <strong>causal self-attention</strong> (no future tokens).</li>
<li>Use <strong>incremental decoding</strong> (no full-sequence softmax).</li>
<li>Add <strong>latency-controlled decoding policies</strong> (e.g., wait-k beam search).</li>
</ul>
<hr>
<h2 id="On-Device-Translation"><a href="#On-Device-Translation" class="headerlink" title="On-Device Translation:"></a><strong>On-Device Translation</strong>:</h2><h3 id="Replace-Heavy-Transformer-with-Lightweight-Quantized-and-Distilled-Models"><a href="#Replace-Heavy-Transformer-with-Lightweight-Quantized-and-Distilled-Models" class="headerlink" title="Replace Heavy Transformer with Lightweight, Quantized, and Distilled Models:"></a>Replace Heavy Transformer with <strong>Lightweight, Quantized, and Distilled Models</strong>:</h3><h4 id="Architectural-Options-1"><a href="#Architectural-Options-1" class="headerlink" title="Architectural Options:"></a>Architectural Options:</h4><table>
<thead>
<tr>
<th>Model</th>
<th>Type</th>
<th>Notes</th>
</tr>
</thead>
<tbody><tr>
<td><strong>MobileBERT</strong>, <strong>TinyBERT</strong></td>
<td>Lightweight Transformers</td>
<td>Good encoder for on-device NMT</td>
</tr>
<tr>
<td><strong>DistilBART</strong>, <strong>DistilmBART</strong></td>
<td>Distilled encoder-decoder</td>
<td>Smaller decoder with similar quality</td>
</tr>
<tr>
<td><strong>mBART + pruning/quantization</strong></td>
<td>Multilingual</td>
<td>Use only specific heads/layers</td>
</tr>
<tr>
<td><strong>FNet</strong></td>
<td>Replaces self-attn with Fourier Transform</td>
<td>Super fast &amp; small</td>
</tr>
<tr>
<td><strong>Linear Attention Models</strong> (Performer, Linformer)</td>
<td>Approximate attention for low-memory devices</td>
<td>Good for constrained decoding</td>
</tr>
</tbody></table>
<h3 id="🛠-Key-Optimization-Techniques"><a href="#🛠-Key-Optimization-Techniques" class="headerlink" title="🛠 Key Optimization Techniques:"></a>🛠 Key Optimization Techniques:</h3><ul>
<li><strong>INT8/INT4 quantization</strong> (QAT or PTQ)</li>
<li><strong>Weight pruning</strong> + <strong>layer dropout</strong></li>
<li><strong>LoRA/Adapters</strong> to allow per-language tuning without retraining the full model</li>
<li>Deploy with <strong>ONNX</strong>, <strong>TensorRT</strong>, <strong>TensorFlow Lite</strong>, or <strong>CoreML</strong></li>
</ul>
<hr>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%8A%80%E6%9C%AF/" rel="tag"># 技术</a>
              <a href="/tags/sytem-design/" rel="tag"># Sytem Design</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2025/06/04/Minimax%20Speech%202.0/" rel="prev" title="Minimax Speech 2.0">
      <i class="fa fa-chevron-left"></i> Minimax Speech 2.0
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Google-Translate-System-Design"><span class="nav-number">1.</span> <span class="nav-text">Google Translate System Design</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Introduction"><span class="nav-number">1.1.</span> <span class="nav-text">Introduction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Clarifying-Requirements"><span class="nav-number">1.2.</span> <span class="nav-text">Clarifying Requirements</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Frame-the-Problem-as-an-ML-Task"><span class="nav-number">1.3.</span> <span class="nav-text">Frame the Problem as an ML Task</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Specifying-the-System%E2%80%99s-Input-and-Output"><span class="nav-number">1.3.1.</span> <span class="nav-text">Specifying the System’s Input and Output</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Choosing-a-Suitable-ML-Approach"><span class="nav-number">1.3.2.</span> <span class="nav-text">Choosing a Suitable ML Approach</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Data-Preparation"><span class="nav-number">1.4.</span> <span class="nav-text">Data Preparation</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-Primary-Data-Parallel-Corpora"><span class="nav-number">1.4.0.1.</span> <span class="nav-text">1. Primary Data: Parallel Corpora</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-Data-Expansion-and-Augmentation"><span class="nav-number">1.4.0.2.</span> <span class="nav-text">2. Data Expansion and Augmentation</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%C2%B7-Text-Pre-processing"><span class="nav-number">1.4.1.</span> <span class="nav-text">1 · Text Pre-processing</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Preprocessing-Steps-%E2%80%94-Modern-Translation-Pretraining"><span class="nav-number">1.5.</span> <span class="nav-text">Preprocessing Steps — Modern Translation Pretraining</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%F0%9F%94%8E-1%EF%B8%8F%E2%83%A3-Data-Cleaning-Deduplication"><span class="nav-number">1.5.1.</span> <span class="nav-text">🔎 1️⃣ Data Cleaning &amp; Deduplication</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%F0%9F%94%8E-2%EF%B8%8F%E2%83%A3-Language-Identification-Tagging"><span class="nav-number">1.5.2.</span> <span class="nav-text">🔎 2️⃣ Language Identification &amp; Tagging</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%F0%9F%94%8E-3%EF%B8%8F%E2%83%A3-Tokenization"><span class="nav-number">1.5.3.</span> <span class="nav-text">🔎 3️⃣ Tokenization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%F0%9F%94%8E-4%EF%B8%8F%E2%83%A3-Alignment-Verification-Parallel-Data"><span class="nav-number">1.5.4.</span> <span class="nav-text">🔎 4️⃣ Alignment Verification (Parallel Data)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%F0%9F%94%8E-5%EF%B8%8F%E2%83%A3-Corruption-Masking-for-Monolingual"><span class="nav-number">1.5.5.</span> <span class="nav-text">🔎 5️⃣ Corruption &#x2F; Masking (for Monolingual)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%F0%9F%94%8E-6%EF%B8%8F%E2%83%A3-Sampling-Balancing"><span class="nav-number">1.5.6.</span> <span class="nav-text">🔎 6️⃣ Sampling &amp; Balancing</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%F0%9F%94%8E-7%EF%B8%8F%E2%83%A3-Back-Translation-for-Monolingual-%E2%86%92-Synthetic-Parallel"><span class="nav-number">1.5.7.</span> <span class="nav-text">🔎 7️⃣ Back-Translation (for Monolingual → Synthetic Parallel)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%C2%B7-Text-Tokenization"><span class="nav-number">1.5.8.</span> <span class="nav-text">2 · Text Tokenization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-Byte-Pair-Encoding-BPE"><span class="nav-number">1.5.9.</span> <span class="nav-text">1. Byte-Pair Encoding (BPE)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-Byte-Level-BPE-BBPE"><span class="nav-number">1.5.10.</span> <span class="nav-text">2. Byte-Level BPE (BBPE)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-Unigram-Language-Model"><span class="nav-number">1.5.11.</span> <span class="nav-text">3. Unigram Language Model</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-WordPiece"><span class="nav-number">1.5.12.</span> <span class="nav-text">4. WordPiece</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Summary-Table"><span class="nav-number">1.5.13.</span> <span class="nav-text">Summary Table:</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Model-Development"><span class="nav-number">1.6.</span> <span class="nav-text">Model Development</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Architecture-Overview"><span class="nav-number">1.6.1.</span> <span class="nav-text">Architecture Overview</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Training-Strategy-by-Alex"><span class="nav-number">1.6.2.</span> <span class="nav-text">Training Strategy by Alex</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ML-Objective-Loss-Function-%E2%80%94-Revised-%E2%9C%8D%EF%B8%8F"><span class="nav-number">1.6.3.</span> <span class="nav-text">ML Objective &amp; Loss Function — Revised ✍️</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-Pre-training-Learning-Multilingual-Representations"><span class="nav-number">1.6.3.1.</span> <span class="nav-text">1  |  Pre-training: Learning Multilingual Representations</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-Supervised-Fine-tuning-Turning-a-General-LM-into-a-Translator"><span class="nav-number">1.6.3.2.</span> <span class="nav-text">2  |  Supervised Fine-tuning: Turning a General LM into a Translator</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Training-Strategy-for-Modern-Multilingual-NMT-Systems"><span class="nav-number">1.6.4.</span> <span class="nav-text">Training Strategy for Modern Multilingual NMT Systems</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-Pretraining-Cross-Lingual-Language-Modeling"><span class="nav-number">1.6.5.</span> <span class="nav-text">1. Pretraining (Cross-Lingual Language Modeling)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-Supervised-Fine-Tuning-Translation-Modeling"><span class="nav-number">1.6.6.</span> <span class="nav-text">2. Supervised Fine-Tuning (Translation Modeling)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-Domain-Adaptation-Specialized-Fine-Tuning"><span class="nav-number">1.6.7.</span> <span class="nav-text">3. Domain Adaptation (Specialized Fine-Tuning)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Recent-Trends-and-Innovations"><span class="nav-number">1.6.8.</span> <span class="nav-text">Recent Trends and Innovations</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#End-to-End-Summary"><span class="nav-number">1.6.9.</span> <span class="nav-text">End-to-End Summary</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-Training-Data-Collection"><span class="nav-number">1.6.10.</span> <span class="nav-text">2. Training Data Collection</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#A-Parallel-Corpora-Supervised"><span class="nav-number">1.6.10.1.</span> <span class="nav-text">A. Parallel Corpora (Supervised)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#B-Monolingual-Corpora-Unsupervised-Pretraining"><span class="nav-number">1.6.10.2.</span> <span class="nav-text">B. Monolingual Corpora (Unsupervised&#x2F;Pretraining)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#C-Quality-Filtering-Techniques"><span class="nav-number">1.6.10.3.</span> <span class="nav-text">C. Quality Filtering Techniques</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#D-Data-Balancing"><span class="nav-number">1.6.10.4.</span> <span class="nav-text">D. Data Balancing</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Sampling-Strategies-in-Generative-Models"><span class="nav-number">1.6.11.</span> <span class="nav-text">Sampling Strategies in Generative Models</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Why-Beam-Search"><span class="nav-number">1.6.12.</span> <span class="nav-text">Why Beam Search?</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comparison-Deterministic-vs-Stochastic-Sampling"><span class="nav-number">1.7.</span> <span class="nav-text">Comparison: Deterministic vs. Stochastic Sampling</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Evaluation"><span class="nav-number">1.8.</span> <span class="nav-text">Evaluation</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Offline-Metrics"><span class="nav-number">1.8.1.</span> <span class="nav-text">Offline Metrics</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Key-Metrics"><span class="nav-number">1.8.1.1.</span> <span class="nav-text">Key Metrics:</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Human-Evaluation"><span class="nav-number">1.8.1.2.</span> <span class="nav-text">Human Evaluation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#LLM-as-a-Judge"><span class="nav-number">1.8.1.3.</span> <span class="nav-text">LLM as a Judge</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Online-Metrics"><span class="nav-number">1.8.2.</span> <span class="nav-text">Online Metrics</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Overall-ML-System-Design"><span class="nav-number">1.9.</span> <span class="nav-text">Overall ML System Design</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%C2%B7-Language-Detector"><span class="nav-number">1.9.1.</span> <span class="nav-text">1 · Language Detector</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%C2%B7-Translation-Service"><span class="nav-number">1.9.2.</span> <span class="nav-text">2 · Translation Service</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Other-Talking-Points"><span class="nav-number">1.10.</span> <span class="nav-text">Other Talking Points</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Real-Time-Translation"><span class="nav-number">1.11.</span> <span class="nav-text">Real-Time Translation:</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Replace-Full-Sequence-Encoder-Decoder-with-Streaming-Models"><span class="nav-number">1.11.1.</span> <span class="nav-text">Replace Full-Sequence Encoder-Decoder with Streaming Models:</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Architectural-Options"><span class="nav-number">1.11.1.1.</span> <span class="nav-text">Architectural Options:</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%F0%9F%9B%A0-Techniques-to-Apply"><span class="nav-number">1.11.2.</span> <span class="nav-text">🛠 Techniques to Apply:</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#On-Device-Translation"><span class="nav-number">1.12.</span> <span class="nav-text">On-Device Translation:</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Replace-Heavy-Transformer-with-Lightweight-Quantized-and-Distilled-Models"><span class="nav-number">1.12.1.</span> <span class="nav-text">Replace Heavy Transformer with Lightweight, Quantized, and Distilled Models:</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Architectural-Options-1"><span class="nav-number">1.12.1.1.</span> <span class="nav-text">Architectural Options:</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%F0%9F%9B%A0-Key-Optimization-Techniques"><span class="nav-number">1.12.2.</span> <span class="nav-text">🛠 Key Optimization Techniques:</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="yxckeis8"
      src="/img/avatar.png">
  <p class="site-author-name" itemprop="name">yxckeis8</p>
  <div class="site-description" itemprop="description">芙蓉王源</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">9</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">12</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/AkazaAkane" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;AkazaAkane" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://linkedin.com/in/yuhaoc2022" title="LinkedIn → https:&#x2F;&#x2F;linkedin.com&#x2F;in&#x2F;yuhaoc2022" rel="noopener" target="_blank"><i class="fab fa-linkedin fa-fw"></i>LinkedIn</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://www.raychase.net/" title="https:&#x2F;&#x2F;www.raychase.net&#x2F;" rel="noopener" target="_blank">四火的唠嗑</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://hanzilu.com/wordpress/" title="http:&#x2F;&#x2F;hanzilu.com&#x2F;wordpress&#x2F;" rel="noopener" target="_blank">韩师傅就是我</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://www.ruanyifeng.com/" title="https:&#x2F;&#x2F;www.ruanyifeng.com&#x2F;" rel="noopener" target="_blank">阮一峰的网络日志</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://shengxu.blog/" title="https:&#x2F;&#x2F;shengxu.blog&#x2F;" rel="noopener" target="_blank">Sheng Xu's Blog</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://pyemma.github.io/" title="https:&#x2F;&#x2F;pyemma.github.io&#x2F;" rel="noopener" target="_blank">Coding Monkey's Blog</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://yage.ai/" title="https:&#x2F;&#x2F;yage.ai&#x2F;" rel="noopener" target="_blank">Computing Life</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://yynnyy.cn/" title="https:&#x2F;&#x2F;yynnyy.cn&#x2F;" rel="noopener" target="_blank">随缘随笔 | Insights Flow</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://www.wxyaonline.top/archive" title="https:&#x2F;&#x2F;www.wxyaonline.top&#x2F;archive" rel="noopener" target="_blank">Wxya</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://mickqian.github.io/" title="https:&#x2F;&#x2F;mickqian.github.io&#x2F;" rel="noopener" target="_blank">Mick's Blog</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">yxckeis8</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  

  

</body>
</html>
