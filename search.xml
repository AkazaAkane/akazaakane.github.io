<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>2025-03-15 本周播客记录</title>
    <url>/2025/03/15/2025-03-15%20%E6%9C%AC%E5%91%A8%E6%92%AD%E5%AE%A2%E8%AE%B0%E5%BD%95/</url>
    <content><![CDATA[<h3 id="硅谷101-E183：比特币巨鲸策略-microstrategy"><a href="#硅谷101-E183：比特币巨鲸策略-microstrategy" class="headerlink" title="硅谷101 E183：比特币巨鲸策略 microstrategy"></a>硅谷101 E183：比特币巨鲸策略 microstrategy</h3><p>这篇播客讲述了美股微策略公司的运行逻辑，这个公司会发债/融资大量购买比特币，提供相对于btc etf更大的流动性。因为自带高杠杆加上high volatility，hf或者其他玩家很喜欢买他。并且他是美股，所以很多没办法买etf的资金（国外的养老基金，州基金）也可以买。而被制裁的国家现在也在推荐多弄一些btc储备。这个公司ceo非常会做营销，并且宣传的卖点就是高波动，最近在跟美国政府的交流中建议美国多搞点不止比特币的储备。</p>
<p>更深有感触的一点是，嘉宾最后提到了链上经济的意义，在这个国与国冲突加剧的时间，谁能主导链上经济那么谁将会主导全球的经济。比如cn控制资金外流，但是假如有了一个很大的链上经济那么流动性全都跑过去了。这个嘉宾也之前也讲了tether，usdt稳定币的公司，你越买稳定币相当于直接买美债了（tether已经是美国第18大债主，超越了很多国家）。所以，为了防止美元霸权，一些国家也在推行自己的链上交易货币。我觉得自己可能也需要搞个冷钱包屯点btc，大概 5% - 10%总资产比较合适。btc可能是偏离了传统价值投资，但是他作为新链上经济的鼻祖有着不可替代的价值。</p>
<p>所以目前看，虽然科技上美国不一定能继续霸权，国内川皇马一龙乱搞通胀已经越来越高了，但是美元的霸权目前来看全世界毫无替代。</p>
<h3 id="潜空间：季雨，谁困住了ai产业大型机化的计算机形态与变革的可能"><a href="#潜空间：季雨，谁困住了ai产业大型机化的计算机形态与变革的可能" class="headerlink" title="潜空间：季雨，谁困住了ai产业大型机化的计算机形态与变革的可能"></a>潜空间：季雨，谁困住了ai产业大型机化的计算机形态与变革的可能</h3><p>官方笔记：<a href="https://miracleplus.feishu.cn/docx/SngpdNt4XoNXHvxzFkFcJNd5nGh">https://miracleplus.feishu.cn/docx/SngpdNt4XoNXHvxzFkFcJNd5nGh</a></p>
<p>作者回顾了人工智能发展的历史，并说明大模型的scaling阶段是处在l2 - l3的阶段。但是它上限就在这里，尽管o1带来了rl post training的范式，目前大模型的能力上线就是语言这个复杂系统的上线。（嘉宾顺口提及了复杂系统会带来全新的能力，比如每个人的组成大家都知道，但是这个社会由于特别多的人的相互作用，变成了一个复杂的系统，产生了远超于每个人本身组成的能力）然后嘉宾回顾了一下pc时代和互联网时代，发现是因为大模型时代缺乏一个”更低的成本，完整的功能，并支持开放和兼容”的生态，也就是说缺少一个llm时代的商业模型。</p>
<p><img src="/img/2025/03/image-20250315211858.png"> </p>
<p>在大型机 - 个人机的时代，intel发明的微型芯片使得每个人都能接触到计算机和智能时代，并且人们著需要一次买断就能后续一直使用。在互联网时代，最伟大的发明是”羊毛出在羊身上”，也是人类历史上最伟大的商业模式：广告。用户通过出售注意力获得服务，催生了推荐系统的研究。但是目前，nvda这种高溢价卖显卡和其他公司卖token的商业模式明显不如前两个，所以短期也不能真正的改变世界。作者认为，现在买家觉得成本太高，开发者的roi又很低，所以这种超算的模式需要转换到个人设备上，才能开启新的时代。至于他自己的公司我没怎么听，但是这个历史讲的别有有意思。一方面我很认同他的观点，另一方面他也cover到了我很多没想到的地方（pc时代）。</p>
<h3 id="高能量-160-161：解读政府工作报告，ai人才争夺战"><a href="#高能量-160-161：解读政府工作报告，ai人才争夺战" class="headerlink" title="高能量 160 - 161：解读政府工作报告，ai人才争夺战"></a>高能量 160 - 161：解读政府工作报告，ai人才争夺战</h3><p>第一个节目是解读政府的报告，强调了政府对科技的重视，具体没什么印象了。</p>
<p>第二个节目是一个ai 猎头公司，讲述国内公司对于ai人才的追求。13年的时候，美国的人才并不愿意回去，因为待遇差（钱少事多）。但是从2024以来，越来越多的人才开始回流。成体的趋势有一点马太效应，巨头愿意花大钱抢顶级人才，但是差一点的人才并不好找工作。嘉宾预言，次一级别的人才可能需要进入传统公司，比如国内某租房软件吸引了一大波人才，做了ai转型。然后强调了宇宙厂字节不惜一切代价，重金挖人，张一鸣亲自1对1接触很多ai人才。</p>
<p>我的感想是，本人也是属于次级人才，所以很能体会嘉宾说的只有头部人才好找工作的问题。那么除了让自己不断学习成为头部人才，短期内也要考虑非科技行业。</p>
<h3 id="晚点聊85：-国家从无到有"><a href="#晚点聊85：-国家从无到有" class="headerlink" title="晚点聊85： 国家从无到有"></a>晚点聊85： 国家从无到有</h3><p>从零开始建设国家比起点小说要困难的多，即使开了外挂也得好多年。</p>
<h3 id="人民公园说ai：豆包只是产品的中间态"><a href="#人民公园说ai：豆包只是产品的中间态" class="headerlink" title="人民公园说ai：豆包只是产品的中间态"></a>人民公园说ai：豆包只是产品的中间态</h3><p>讲了字节系的豆包/扣子开发者大会。感受是字节系确实nb，愿意烧钱也烧的起钱，不论是产品，科研，人才全部都拿下了，目前感觉是国内唯一t0。阿里也不错，其他几家拉跨了一点。</p>
<h3 id="硬地骇客88：-开发翻译产品"><a href="#硬地骇客88：-开发翻译产品" class="headerlink" title="硬地骇客88： 开发翻译产品"></a>硬地骇客88： 开发翻译产品</h3><p>嘉宾是字节系的前pm，自己通过用chatgpt开发了一个漫画翻译app，实现了盈利。</p>
<h3 id="六岔路口：宠物需要的情绪价值很难替代"><a href="#六岔路口：宠物需要的情绪价值很难替代" class="headerlink" title="六岔路口：宠物需要的情绪价值很难替代"></a>六岔路口：宠物需要的情绪价值很难替代</h3><p>阿里高管出来创业，做狗粮品牌。强调了创业更难，但是自己会有心理上的轻松，团队也相对松散。嘉宾强调了现在人和宠物的链接，并且人的belief会影响他对于宠物产品的消费：比如说，假如一个人很重视饮食的健康，那么他在选购狗粮时也会买强调这个饮食健康的品牌。</p>
<h3 id="科技早知道：从deepseek到manus"><a href="#科技早知道：从deepseek到manus" class="headerlink" title="科技早知道：从deepseek到manus"></a>科技早知道：从deepseek到manus</h3><p>女嘉宾号称是前open ai研究员，但是听上去感觉对ai了解极其有限。比如，她对于开源项目的商业化缺乏了解，不清楚开源到底靠什么赚钱的。并且也有一些明显错误的技术认识，比如”开源模型的api一定比自己部署贵”。尽管徐老师和男嘉宾尽力了，但是由于女嘉宾占的篇幅比较多带不动。听了差不多等于没听。</p>
]]></content>
      <tags>
        <tag>podcast</tag>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title>关于运动的思考</title>
    <url>/2025/03/14/%E5%85%B3%E4%BA%8E%E8%BF%90%E5%8A%A8%E7%9A%84%E6%80%9D%E8%80%83/</url>
    <content><![CDATA[<h2 id="关于运动的思考"><a href="#关于运动的思考" class="headerlink" title="关于运动的思考"></a>关于运动的思考</h2><p>这篇文章记录了我对运动和康复领域的一些思考。</p>
<h3 id="热身和康复"><a href="#热身和康复" class="headerlink" title="热身和康复"></a>热身和康复</h3><p>在哥大读研的时候，我经常去哥大健身房运动，毕竟纽约免费的离我近的健身房实在是不多。由于健身房规模有限，器械少学生多，很多时候都不能按照自己的计划来。我犯的最大的错就是，为了追求单词健身时间的最小化，经常省略热身和拉伸环节。当然更多的是思想上的欠缺，觉得说好像热身和拉伸无所谓，所以我付出了膝盖和肩膀受伤的惨痛代价，并且直到今日还时不时有后遗症。</p>
<p>根据我个人的理解，热身主要是两方面，生理上的准备和神经/心理上的准备。生理上的准备就是，当我们身体热起来以后，肌肉会膨胀，关节会分泌润滑液，所以phyiscally 运动表现就是更好。心理上的准备是，比如我们看到一个特别重的物体，那我们在想要把他举起来之前，我们的大脑皮层会会告诉神经：这玩意老重了，你得小心点多用里。那么，我们就会蹲下来，小心的很用力举起来。反之，看到一个很轻的东西比如一个泡沫轴，那我们就随便直接弯腰去捡了。</p>
<p>所以现在我在健身和跑步前一定会热身，练完也需要拉伸和放松。这个确实能帮助我避免伤病，起码我现在膝盖和肩膀的老问题不怎么反复了。但是呢，也造成了我现在肌肉增长速度的放缓：我现在可能会做更多功能性训练，而不是肌肥大训练。并且训练的时候，宁愿少长点肌肉也要保证运动的安全，可能是我个性如此。就像巴菲特还是芒格说的，我如果我知道要死在什么地方那我就不会去，所以我并不喜欢极限运动，甚至包括滑雪和过山车。</p>
<p>运动前热身的优点和必要性可以从以下几个方面进行简短描述：</p>
<ol>
<li><p><strong>提高身体温度和血液循环</strong>：热身可以逐渐升高体温，促进血液循环，使血液流向肌肉，增加氧气供应，从而为高强度运动做好准备。</p>
</li>
<li><p><strong>减少受伤风险</strong>：热身通过增加肌肉弹性、关节活动范围和神经传导速度，降低肌肉拉伤和关节损伤的风险。</p>
</li>
<li><p><strong>提升运动表现</strong>：热身可以增强肌肉力量和速度，改善肌肉协调能力，减少肌肉黏滞性，从而提高运动表现。</p>
</li>
<li><p><strong>心理准备</strong>：热身有助于集中注意力，调整心理状态，为接下来的运动做好心理准备。</p>
</li>
</ol>
<h3 id="运动的好处"><a href="#运动的好处" class="headerlink" title="运动的好处"></a>运动的好处</h3><p>根据metaso，运动的好处如下：</p>
<p>运动的好处可以从多个方面进行总结，包括身体健康、心理健康、社交能力和生活质量的提升。以下是运动的主要好处：</p>
<ol>
<li><p><strong>身体健康</strong>：</p>
<ul>
<li><strong>增强心肺功能</strong>：运动可以提高心肺功能，增强血液循环，降低心血管疾病的风险。</li>
<li><strong>控制体重</strong>：通过燃烧脂肪和增加肌肉量，运动有助于减肥和维持健康的体重。</li>
<li><strong>改善代谢</strong>：运动促进新陈代谢，提高身体对糖和脂肪的利用效率，预防糖尿病和肥胖。</li>
<li><strong>增强免疫力</strong>：规律运动可以提高免疫系统的功能，减少疾病的发生。</li>
<li><strong>预防慢性疾病</strong>：运动有助于降低高血压、高血糖、心脏病等慢性疾病的风险。</li>
<li><strong>改善骨骼健康</strong>：运动可以增强骨密度，预防骨质疏松症。</li>
</ul>
</li>
<li><p><strong>心理健康</strong>：</p>
<ul>
<li><strong>缓解压力和焦虑</strong>：运动可以释放内啡肽等“快乐激素”，帮助减轻压力和焦虑。</li>
<li><strong>改善情绪</strong>：运动能够提升心情，增强自信心，减少抑郁情绪。</li>
<li><strong>提高专注力</strong>：运动有助于集中注意力，提升大脑的工作效率。</li>
</ul>
</li>
<li><p><strong>社交能力</strong>：</p>
<ul>
<li><strong>扩大社交圈</strong>：通过团队运动或健身活动，可以结交新朋友，培养合作精神和团队意识。</li>
<li><strong>增进亲子关系</strong>：共同参与运动可以增进家庭成员之间的交流和理解。</li>
</ul>
</li>
<li><p><strong>生活质量</strong>：</p>
<ul>
<li><strong>提升活力</strong>：运动可以增强体力和耐力，使人更加精力充沛。</li>
<li><strong>改善睡眠质量</strong>：规律运动有助于调整生物钟，改善睡眠质量。</li>
<li><strong>延缓衰老</strong>：运动可以延缓身体机能的衰退，保持身体的灵活性和活动能力。</li>
</ul>
</li>
</ol>
<p>我在atl待的这段时间经常感到抑郁，所以我会每天都运动。当然身体健康我可能没感受到，还是时不时生病或者运动受伤，但是心理健康有明显的改善。</p>
<h3 id="不同的快乐激素"><a href="#不同的快乐激素" class="headerlink" title="不同的快乐激素"></a>不同的快乐激素</h3><h3 id="内啡肽、催产素与多巴胺的功效及产生行为总结"><a href="#内啡肽、催产素与多巴胺的功效及产生行为总结" class="headerlink" title="内啡肽、催产素与多巴胺的功效及产生行为总结"></a>内啡肽、催产素与多巴胺的功效及产生行为总结</h3><p>内啡肽，多巴胺和催产素都是快乐激素。我的理解是，内啡肽需要的是长期并略有痛苦的行为，比如运动。多巴胺是最短期的激素，比如打游戏，运动，性都可以提供。但是在现代社会下，多巴胺也非常容易被滥用的，比如电子游戏和黄色很容易让你快乐，但是快乐完只会感受到空虚。相比之下，运动完的快乐会更持久，也更令人开心。虽然很多时候知道，但是没办法控制自己，因为现在状态确实不好，压力很大。</p>
<h4 id="1-内啡肽（Endorphins）"><a href="#1-内啡肽（Endorphins）" class="headerlink" title="1. 内啡肽（Endorphins）"></a><strong>1. 内啡肽（Endorphins）</strong></h4><p><strong>功效</strong>：  </p>
<ul>
<li><strong>缓解疼痛</strong>：作为天然止痛药，通过抑制疼痛信号传递减轻痛感。  </li>
<li><strong>产生欣快感</strong>：与吗啡受体结合，带来类似吗啡的愉悦感和放松效果。  </li>
<li><strong>增强运动耐力</strong>：在运动中帮助隐藏身体痛苦，促进持续锻炼。  </li>
<li><strong>提升免疫力和睡眠</strong>：改善免疫功能，调节睡眠质量。</li>
</ul>
<p><strong>触发行为</strong>：  </p>
<ul>
<li><strong>高强度运动</strong>：如跑步、游泳、举铁（持续30分钟以上）。  </li>
<li><strong>饮食</strong>：食用黑巧克力、辛辣食物（如辣椒）。  </li>
<li><strong>情绪行为</strong>：大笑（如看喜剧）、按摩。</li>
</ul>
<hr>
<h4 id="2-催产素（Oxytocin）"><a href="#2-催产素（Oxytocin）" class="headerlink" title="2. 催产素（Oxytocin）"></a><strong>2. 催产素（Oxytocin）</strong></h4><p><strong>功效</strong>：  </p>
<ul>
<li><strong>促进母婴关系</strong>：刺激乳汁分泌、激发母爱，帮助分娩时子宫收缩。  </li>
<li><strong>增强社交信任</strong>：被称为“爱的激素”或“信任激素”，抑制恐惧和防御心理，增进人际信任。  </li>
<li><strong>缓解压力</strong>：降低压力激素（如肾上腺酮）水平，稳定血压和心率。  </li>
<li><strong>促进亲密关系</strong>：在拥抱、性行为等亲密互动中释放，巩固情感纽带。</li>
</ul>
<p><strong>触发行为</strong>：  </p>
<ul>
<li><strong>身体接触</strong>：拥抱、亲吻、性行为。  </li>
<li><strong>社交互动</strong>：与亲友对话、陪伴、参与集体活动。  </li>
<li><strong>利他行为</strong>：帮助他人、表达善意（如赞美、捐赠）。  </li>
<li><strong>母婴行为</strong>：哺乳、母婴皮肤接触。  </li>
<li><strong>宠物互动</strong>：与宠物玩耍或凝视。</li>
</ul>
<hr>
<h4 id="3-多巴胺（Dopamine）"><a href="#3-多巴胺（Dopamine）" class="headerlink" title="3. 多巴胺（Dopamine）"></a><strong>3. 多巴胺（Dopamine）</strong></h4><p><strong>功效</strong>：  </p>
<ul>
<li><strong>激励与奖赏</strong>：驱动目标导向行为，带来短暂的兴奋感和满足感（如完成目标后的愉悦）。  </li>
<li><strong>调节情绪</strong>：缺乏时可能导致抑郁、冲动或动力不足。  </li>
<li><strong>影响成瘾机制</strong>：与愉悦和“上瘾”行为（如游戏、爱情初期）密切相关。  </li>
<li><strong>促进运动与学习</strong>：通过奖赏机制鼓励重复有益行为（如运动、学习）。</li>
</ul>
<p><strong>触发行为</strong>：  </p>
<ul>
<li><strong>达成目标</strong>：完成小任务、获得成就（如工作、学习）。  </li>
<li><strong>饮食与娱乐</strong>：吃巧克力、听音乐、观看喜剧。  </li>
<li><strong>运动与性行为</strong>：锻炼、性爱。  </li>
<li><strong>社交互动</strong>：与朋友聚会、建立新关系。</li>
</ul>
]]></content>
      <tags>
        <tag>思考</tag>
        <tag>杂谈</tag>
        <tag>生活</tag>
        <tag>运动</tag>
      </tags>
  </entry>
  <entry>
    <title>设定系推理的减法艺术：为何我不喜欢《献给名侦探的甜美死亡》</title>
    <url>/2025/03/20/%E8%AE%BE%E5%AE%9A%E7%B3%BB%E6%8E%A8%E7%90%86%E7%9A%84%E5%87%8F%E6%B3%95%E8%89%BA%E6%9C%AF/</url>
    <content><![CDATA[<p>最近读完了方丈贵惠的《献给名侦探的甜美死亡》，正好借此聊一聊设定系推理小说。近年来，日本设定系推理小说以“规则创新”为旗号，掀起了一股“万物皆可设定”的狂潮。从时间循环到超能力预言，从丧尸围城到AI破案，作家们不断用天马行空的框架重构本格推理的边界。然而，当我在阅读方丈贵惠的《献给名侦探的甜美死亡》时，却感受到一种被“过度设定”反噬的疲惫——这部作品将VR游戏、双重暴风雪山庄、狼人杀机制、现实与虚拟空间交互等元素堆砌成一座繁复的迷宫，最终让我迷失在规则的泥潭中。相比之下，白井智之的和今村昌弘的却以“极简规则”创造出令人拍案叫绝的诡计。这种反差促使我反思：设定系推理的魅力，或许不在于规则的复杂程度，而在于如何用最少的“砖石”搭建出最精妙的“逻辑之塔”。</p>
<h3 id="正文（不涉及剧透部分）"><a href="#正文（不涉及剧透部分）" class="headerlink" title="正文（不涉及剧透部分）"></a>正文（不涉及剧透部分）</h3><p>《献给名侦探的甜美死亡》讲述的故事是，加茂冬马 &amp; 龙泉佑树（作者同系列作品的两个主角），接受了VR游戏《谜案创造者》开发商巨齿鲨游戏游戏试玩会的邀请会，来到孤岛上的巨齿鲨山庄。但是游戏还没开始，每个人被告知自己最重要的人都被当作人质，要想解救家人、平安回去，就必须同时解开发生在现实世界及VR世界里的命案。</p>
<p>在还没开始看之前，我一下就想到了山口雅也1989年的《克莱因壶》，以及我心目中的二次元最经典作品《刀剑神域》和，之前火爆的电影《头号玩家》。1935年，美国科幻小说家斯坦利·威因鲍姆（Stanley Weinbaum）就发表了《皮格马利翁的眼镜》（Pygmalion’s Spectacles）。这部小说被认为是第一个探讨虚拟现实系统的科幻作品，描述了一种包括嗅觉、触觉和全息护目镜的虚拟现实系统。到此为止，好像我们只是单纯的谈论了VR这个科幻元素，还没有谈及设定系这一说法。事实上，科幻元素可以算是设定系中的一种非常常见的流派。</p>
<p>那么什么是设定系推理呢？设定系推理指通过引入科幻、奇幻或恐怖等非现实元素，在特殊世界观规则下展开的推理作品。它源自于英国”诺克斯十诫”对超自然元素的排斥：</p>
<ul>
<li>故事中不可存有超自然力量。</li>
<li>故事中不应出现不存在的毒药、以及太复杂需要长篇解说的犯案工具。</li>
<li>故事中不可有中国人角色。（实际上是说静止角色拥有超能力）</li>
</ul>
<p>虽然这些信条不乏有些错误的认知，但在古典推理小说的黄金期时曾被奉为圭臬。逻辑也很简单，因为引入这些元素无法让读者信服。比如说，在解答受害人怎么死的时候，假如作者说“犯人有超能力，直接远程杀死受害人不留下痕迹”，那正常的读者都很难满意。因为一般来讲，推理小说默认了现实世界中的物理定律，所以如果谜底是之前从未提及的超能力的话，那答案其实有无数种，推理这个过程其实可有可无（外星人杀的人，受害者是活死人本来就死了…）。当然，如果事先告诉读者犯人有超能力，且只有一个犯人这种定律，那通过引入这种悬疑的设定反而会让小说变得更有意思。这些元素还不够达成一个好的设定系推理作品。对于推理小说而言，作者会制造一个谜题（比如说杀人案），然后提供一个合乎逻辑的谜底（解答/推理）。假设我们以上讨论的超自然元素和谜面谜底没关系的话，那其实也不能算是设定系小说。比如柯南里有变小药，阿笠博士的地精科技系列：滑板，足力健，ikun背带裤，但这些要素与解开谜团无直接关系，在揭露诡计或找出真凶时基本不会考虑它们的存在，因此通常不被视为特殊设定谜团。假如柯南运用了高科技或者变小药去犯案，那柯南就可以是设定系推理作品。</p>
<p>至此，我们就引入了设定系推理的完整定义：</p>
<ol>
<li>包含现实相异的物理法则、现象、超能力、高科技等设定，但是需预先建立清晰世界观规则并遵循由此产生的规则（如《死亡笔记》使用手册，或者若存在超能力者，需限定”每区域仅1人”等约束条件</li>
<li>谜题必须基于设定规则展开，也是说之前提到的超自然规则不能和谜题无关</li>
</ol>
<p>另外，即使没有任何科幻或奇幻元素，以孤岛、外国或过去为背景，讲述只有在该背景中才能解决的谜团和解决办法的推理小说，也可以广义上称为设定系推理小说。 实际上，从2010年代后半期开始，与特殊设定推理小说的繁荣相伴，也出现了以过去时代为背景，以那个时代才有的谜题为主题的严肃推理小说受到高度评价的趋势，比如古城诚二的《战争的底层》、亚门伊吹的《剑与伞》、辻正树的《只是谋杀而已》、米泽帆信的《黑牢城》。</p>
<p>日本的设定系推理发源自1987年绫辻行人出道后开始的新本格运动，在旨在复兴古典侦探乐趣的新侦探类型中，出现了一部里程碑式的作品：山口雅也的《活死人之死》（1989年） ，该作品讲述“发生在死者复活的世界里的谋杀案件”。 《活死人之死》的开创性之处在于它“为了解开谜团而创造了一个完全特殊的世界”。 接下来，1995年出道的西泽康彦推出了一系列科幻悬疑小说，包括具有里程碑意义的时间循环推理小说《死去七次的男人》、描述人物性格相继互换的杀人案的《人格转移杀人案》、以及揭露心灵感应者犯罪的《上雅嗣子的心灵感应事件簿》系列。他宣扬“即使你引入科幻背景，只要你明确规定规则，你也可以写出悬疑小说”的想法。</p>
<p>漫画中，智力战、死亡游戏、基于特殊规则的生存故事，例如前面提到的《死亡笔记》和《未来日记》等，都颇为流行，“基于特殊规则的智力战”的形式也逐渐普及。回顾这段历史，可以说对 现代奇幻推理小说影响最强的，就是《JoJo的奇妙冒险》和福本伸行的作品。 《JoJo》中的替身战斗，以及《赌博》等福本作品中出现的众多特殊游戏，构成了奇幻推理小说的思维方式的基础。 另外，说到特殊设定的推理小说的历史，不能不提的就是2001年开始的热播游戏《逆转裁判》。绫里真宵运用超能力解决谜题的方式，以及因超能力而发生的意想不到的事件，对后来出道的年轻作家产生了巨大的影响。</p>
<p>随后在2009年，绫辻行人创作的《Another》问世，将十足的悬疑、寻找真凶以及基于恐怖般的规则的剧情转折结合在一起。随后， 2010年，米泽穂信的《折断的龙骨》问世，这是一部以剑与魔法的奇幻世界为背景的全方位悬疑小说，其中需要猜测罪犯。“科幻悬疑”一词已不再能够涵盖这些作品。 米泽在《破碎的龙骨》后记中所使用的“特殊设定推理小说”这一术语，后来作为融合了科幻、奇幻、恐怖等元素的推理小说的统称而广为流传。</p>
<p>与此同时，严肃推理小说界也兴起了多重解决方式的风潮 ，追求“出乎意料的逻辑”逐渐成为严肃推理小说的主流。在严肃侦探小说的世界里，一切出乎意料的凶手和诡计早已穷尽，使用叙事技巧的出人意料的叙述和情节也已进入了瓶颈，用读者绝对想不到的逻辑来呈现让他们吃惊的“出乎意料的逻辑”是严肃侦探小说仅存的最后边疆。 而为了呈现这种“意想不到的逻辑”，作品中有意无意引入非现实的设定也变得越来越普遍，比如《煽动磨坊》和《丸太町卢浮宫》等为逻辑战斗而特设的场景，以及森川友树的《白雪公主》中引入“揭示真相的镜子”等。</p>
<p>此次多解热潮由于涉及“多种”解决方案，不可避免地朝着“以逻辑步骤数取胜”的方向发展，并在2010年代中期以深见怜一郎的《不可思议的竞技场》和井上正树的《我已经考虑过那种可能性》达到高潮，之后便陷入停顿。取而代之的是， 白井智之的《晚安人面疮》、 市川忧人的《水母不会结冰》等作品相继出现。这些作品通过引入独特的设定，并根据这些规则展开谜题，不断推出展现“意想不到的逻辑”的作品。近十年来讲，今村昌宏的处女作《尸人庄迷案》和白井智之《象之首》是我个人设定系推理里最喜欢的作品，除此之外方丈贵恵和早坂吝老师也都有不错的作品。</p>
<p>下方内容涉及剧透，请自行选择观看。</p>
<h3 id="正文（包含剧透）"><a href="#正文（包含剧透）" class="headerlink" title="正文（包含剧透）"></a>正文（包含剧透）</h3><p>回到正题，《献给名侦探的甜美死亡》这部小说以<strong>VR虚拟现实</strong>为核心设定，构建了“现实世界”与“虚拟世界”双重暴风雪山庄，结合狼人杀、剧本杀规则，展开一场名侦探与凶手的智力对决。主角<strong>加茂冬马</strong>因家人被绑架，被迫参与VR游戏《谜案创造者》内测。游戏舞台是现实中的<strong>巨齿鲨山庄</strong>与虚拟的<strong>玩偶屋馆</strong>，玩家需通过VR设备在虚拟空间中破解案件，而现实世界中的玩家生死与游戏结果直接绑定。8名参与者中，1人扮演“凶手”（虚拟世界的凶手），1人是“执行人”（现实世界的真凶），其余为“侦探”。凶手需在虚拟空间作案，侦探需在限定时间内破案，否则现实中对应玩家将被处决。  虚拟空间被杀死，现实世界并不会死亡。虚拟世界的物理规则与现实不同。虚拟案件与现实中连环谋杀同步发生，主角需在破解VR密室的同时，揭露执行人<strong>良田千景姐弟</strong>的复仇计划——他们利用游戏规则制造混乱，试图将龙泉家族的诅咒公之于众。</p>
<p>具体案件和谜题我会放在后面的附录里，感兴趣的可以去观看。我觉得这部分逻辑性和创新性尚可，比上不足（白井和今村）比下有余。然而，这本作品最大的缺陷还是在设定本身上。设定系推理的核心，是通过限定特殊规则，将特殊的规则逻辑压缩到某个极端场景中，从而激发出传统本格难以实现的诡计可能性。今村昌弘的《尸人庄迷案》只运用了“丧尸围城”这个设定，就构建出一场颠覆传统的连环谋杀：丧尸的存在既制造了暴雪山庄模式，又成为“伪造死亡时间”的关键道具（例如凶手利用丧尸咬痕掩盖尸体真实死因）。当读者以为丧尸只是氛围工具时，它们却成了诡计的核心。这种“设定即诡计”的创作思维，让规则不再是装饰，而是推理迷宫中不可或缺的承重墙。</p>
<p>反观《献给名侦探的甜美死亡》，方丈贵惠显然选择了一条截然相反的道路。这部作品试图将VR游戏、现实绑架、狼人杀角色扮演、虚拟与现实空间同步谋杀等元素全部塞进同一容器，结果却让核心诡计被淹没在庞杂的设定中。</p>
<p>小说中，玩家需通过VR设备在“虚拟玩偶屋馆”破解案件，而现实世界的生死与游戏结果绑定。这一设定本身已包含“虚拟空间物理规则”“现实与虚拟时间差”“VR设备机能限制”等多重变量。但作者进一步叠加了“狼人杀式角色分配”（凶手、侦探、执行人）、“管理员权限篡改代码”、“现实绑架者胁迫玩家”等规则，导致读者不得不耗费大量精力厘清“什么能做、什么不能做”。例如“佑树之死”一案中，凶手利用VR镜头焦距切换隐藏尸体转移路径，这一诡计的前提是读者必须完全理解“玩偶屋馆的虚拟空间具有缩放功能”——但这一设定在案件发生前仅被一笔带过，最终解答更像是“作者突然翻开一张隐藏规则卡”。</p>
<p>在优秀设定系作品中，规则与诡计应是“骨肉相连”的整体。然而《献名》的多数案件却呈现出“设定归设定，诡计归诡计”的割裂感。例如“乾山之死”的核心手法是“凶手破坏现实世界的VR设备导致玩家失足坠亡”，这一解答本质上只需“现实与虚拟联动”的基础设定即可成立，但作者偏要引入“虚拟重力模拟系统”“反重力绳索误导”等冗余设定，反而让诡计显得牵强。相比之下，白井智之在《我为妖怪你为怪物》中仅用“妖怪必须遵守承诺”这一条规则，就构建出凶手利用语言陷阱诱骗妖怪自杀的惊天逆转——简单规则的深度挖掘，远胜于复杂设定的浮夸堆砌。</p>
<p>并且当规则过于复杂时，作品往往沦为冰冷的公式说明书，而作品中展现人性的思考的部分很容易被忽略。今村昌弘的《屍人荘》之所以动人，正是因为丧尸危机放大了人性的挣扎（如角色为保护他人主动被咬）。而《献名》中幕后黑手良田姐弟的复仇动机，却因VR规则、家族诅咒、前作关联等多重信息干扰，显得苍白无力。当读者还在纠结“虚拟空间如何同步现实谋杀”时，早已无暇感受角色的绝望与救赎。</p>
<p>设定系推理的初衷，本是为了挣脱现实逻辑的束缚，开辟新的诡计领域。但近年来部分作品陷入“为设定而设定”的误区，仿佛规则的复杂度与作品的创新性成正比。这种倾向的危险性在于，当作家沉迷于搭建规则迷宫时，可能会忘记迷宫的终点必须有一枚璀璨的宝石——那个让人豁然开朗的逻辑核心。读者期待的是，掀开谜底那一瞬间的豁然开朗：原来是这么简单，但是这么意想不到；而不是当谜题揭晓还在困扰：这条谜题是什么意思，和设定有什么关系。</p>
<p>《献给名侦探的甜美死亡》无疑是一部有意思的作品，但它也暴露出设定系推理的潜在危机：当规则复杂到需要说明书才能理解时，推理小说便从“智力的游戏”异化为“设定的奴隶”。相比之下，我更喜欢白井智之笔下那个无限循环的西斯玛，以及今村昌弘镜头前的电梯和丧尸——它们用最简单的规则，撕开了逻辑最深邃的裂缝。他们的成功，恰恰在于他们手握“奥卡姆剃刀”，果断剃除了一切不必要的规则设定。当设定系推理重新学会做“减法”，或许我们才能迎来下一个黄金时代。</p>
<h3 id="Appendix-《献给名侦探的甜美死亡》案件与谜题解析"><a href="#Appendix-《献给名侦探的甜美死亡》案件与谜题解析" class="headerlink" title="Appendix 《献给名侦探的甜美死亡》案件与谜题解析**"></a>Appendix 《献给名侦探的甜美死亡》案件与谜题解析**</h3><p>小说共设计了<strong>五起核心案件</strong>，每案均结合VR设定与物理诡计，呈现“虚拟作案→现实联动→多重反转”的复杂结构。</p>
<h5 id="1-未知之死（虚拟密室：冰块与风压）"><a href="#1-未知之死（虚拟密室：冰块与风压）" class="headerlink" title="1. 未知之死（虚拟密室：冰块与风压）"></a><strong>1. 未知之死（虚拟密室：冰块与风压）</strong></h5><ul>
<li><strong>手法</strong>：虚拟空间中，玩家“未知”被发现死于封闭房间，门窗从内部反锁。凶手利用冰块堵塞通风口，通过空调制造室内外气压差，使门锁在冰块融化后自动闭合，形成密室假象。  </li>
<li><strong>关键点</strong>：VR环境模拟物理规则，但玩家需意识到“虚拟空间的密室可通过现实物理原理破解”。</li>
</ul>
<h5 id="2-佑树之死（双重空间与玩偶屋馆）"><a href="#2-佑树之死（双重空间与玩偶屋馆）" class="headerlink" title="2. 佑树之死（双重空间与玩偶屋馆）"></a><strong>2. 佑树之死（双重空间与玩偶屋馆）</strong></h5><ul>
<li><strong>手法</strong>：虚拟空间中的“佑树”死于玩偶屋馆的缩小模型房间。凶手利用VR视角切换的盲区，将尸体从正常空间转移到缩小模型内，制造“不可能位移”。  </li>
<li><strong>解答</strong>：玩家发现玩偶屋馆的模型与真实空间比例一致，通过调整VR镜头焦距，可隐藏尸体转移路径。</li>
</ul>
<h5 id="3-乾山之死（绳索与重力误导）"><a href="#3-乾山之死（绳索与重力误导）" class="headerlink" title="3. 乾山之死（绳索与重力误导）"></a><strong>3. 乾山之死（绳索与重力误导）</strong></h5><ul>
<li><strong>手法</strong>：虚拟空间中，玩家“乾山”的尸体悬挂于高塔，现场无攀爬工具。凶手利用VR服的重力模拟功能，在虚拟环境中伪造“反重力绳索”，误导侦探认为凶手具备飞行能力。  </li>
<li><strong>反转</strong>：实际是凶手在现实世界破坏VR设备，导致乾山在虚拟空间中失重坠亡。</li>
</ul>
<h5 id="4-栋方之死（VR服毒杀与时间差）"><a href="#4-栋方之死（VR服毒杀与时间差）" class="headerlink" title="4. 栋方之死（VR服毒杀与时间差）"></a><strong>4. 栋方之死（VR服毒杀与时间差）</strong></h5><ul>
<li><strong>手法</strong>：玩家“栋方”在虚拟空间中中毒身亡，但VR环境无法直接下毒。凶手提前在现实世界对栋方的VR服注射神经毒素，利用游戏时间与现实时间的延迟，制造“虚拟中毒”假象。  </li>
<li><strong>关键线索</strong>：VR服内置的生命监测系统显示栋方在进入游戏前已出现中毒体征。</li>
</ul>
<h5 id="5-不破之死（虚拟场景重构与执行人身份）"><a href="#5-不破之死（虚拟场景重构与执行人身份）" class="headerlink" title="5. 不破之死（虚拟场景重构与执行人身份）"></a><strong>5. 不破之死（虚拟场景重构与执行人身份）</strong></h5><ul>
<li><strong>手法</strong>：执行人“不破”在虚拟空间中被杀，凶手通过篡改游戏代码，在案件发生后重构VR场景，掩盖作案痕迹。最终揭露不破实为良田千景的替身，其死亡是千景为混淆视线设计的“伪解答”。  </li>
<li><strong>核心诡计</strong>：执行人利用管理员权限，在虚拟与现实之间切换身份，制造不在场证明。</li>
</ul>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p><a href="https://dic.nicovideo.jp/t/a/%E7%89%B9%E6%AE%8A%E8%A8%AD%E5%AE%9A%E3%83%9F%E3%82%B9%E3%83%86%E3%83%AA">niconico百科</a><br><a href="https://www.douban.com/note/843805948/?_i=2430319ZegaBIr">关于设定系推理的碎碎念</a><br><a href="https://www.douban.com/note/866785996/?_i=2430323ZegaBIr">设定之外的世界</a></p>
]]></content>
      <tags>
        <tag>杂谈</tag>
        <tag>推理小说</tag>
        <tag>读书</tag>
      </tags>
  </entry>
  <entry>
    <title>2025-03-23 本周播客记录</title>
    <url>/2025/03/23/2025-03-23%20%E6%9C%AC%E5%91%A8%E6%92%AD%E5%AE%A2%E8%AE%B0%E5%BD%95/</url>
    <content><![CDATA[<h3 id="起朱楼宴宾客-vol-120-日本医疗体系"><a href="#起朱楼宴宾客-vol-120-日本医疗体系" class="headerlink" title="起朱楼宴宾客 vol:120 日本医疗体系"></a>起朱楼宴宾客 vol:120 日本医疗体系</h3><p>本集博客讲述日本怎么样走出医保崩溃，对于中国的现状和未来起到了一定的启示作用。日本的医保可以分为90-05年的崩溃期，以及05到如今的重生期。从需求端 - 人来讲，日本政府其实很早就预料到了老龄化会加重，但是由于低估了老龄化的速度和老龄人慢性病对医疗资源的占用，医保政策几近崩溃。而现在，虽然老龄化更严重了，但是日本医生采用免费健诊和提前防控，大大降低了慢性病的出现速度和概率。需求端 - 资金来讲，日本现在医保是全覆盖，有上限，独立的老年人保险制度。供给端分为三个部分，医疗服务，资金注入和药品。在崩溃时期，日本有着巨大的医生缺口和剧烈的医患矛盾。在重生时期，日本实行了分诊制，社区化，提升医疗了效率。在资金供给方面，日本在崩溃时期像现在的中国一样，有着医疗费亡国危机。因此，政府实行了医疗削减法案和廉价医疗，但是效果十分糟糕。在经历改革之后，实行了dpc和康复理疗的发展。药品的政策是中国最能借鉴的，在崩溃时期，集采的低价招标策略带来了药品质量危机和创新药危机，日本制药行业大萧条。于是，政府实行了定价改革（政府给定价格，在同价格选择质量最好的），中小企业兼并重组（有创新能力的企业收购仿制药），和药剂师改革。结论是，不可能三角：质量，价格，规模 可能被平衡。</p>
<h3 id="声东击西-339-中国短剧登录好莱坞"><a href="#声东击西-339-中国短剧登录好莱坞" class="headerlink" title="声东击西 #339 中国短剧登录好莱坞"></a>声东击西 #339 中国短剧登录好莱坞</h3><p>短剧现在大批量进入美国市场，一般是中国国内火的剧本直接拿到LA找当地演员拍摄。相比于传统电视剧，短剧成本低，时间短，剧情没什么深度。并且，资方权限大，导演只是负责剧本：资方会根据大数据选择演员，按照特征（发色/瞳色）来挑选演员。好莱坞工会的罢工使得岗位减少，最好的人只能去争取次一级的剧，导致很多导演，工作人员，演员流动到短剧。</p>
<h3 id="科技早知道-S9E07-特斯拉暴跌，美股回调"><a href="#科技早知道-S9E07-特斯拉暴跌，美股回调" class="headerlink" title="科技早知道 S9E07 特斯拉暴跌，美股回调"></a>科技早知道 S9E07 特斯拉暴跌，美股回调</h3><p>特斯拉暴跌，跟很多🐎粉声称索罗斯等人的恶意做空没什么关系。主要原因是，川普上任后表现不及预期，导致川普溢价 - 特斯拉，数字货币和川普公司等都跌回选前水平。英伟达需要新的叙事，虽然deepseek出来以后正反观点依然在博弈，但是已经不像以前那样无人质疑。只要有人开始质疑，那需要新的所有人都认同的叙事才能支持这种高估值。</p>
<h3 id="科技早知道S8E32-谷歌量子计算芯片willow"><a href="#科技早知道S8E32-谷歌量子计算芯片willow" class="headerlink" title="科技早知道S8E32 谷歌量子计算芯片willow"></a>科技早知道S8E32 谷歌量子计算芯片willow</h3><p>谷歌的新芯片有100个量子比特，并且能支持纠错，但是大规模商用的芯片需要100万个，任重道远。长期来讲，区块链和现在的加密方式有被破译的可能。</p>
<h3 id="晚点聊107-Haivivi月入千万的ai-jellycat"><a href="#晚点聊107-Haivivi月入千万的ai-jellycat" class="headerlink" title="晚点聊107 Haivivi月入千万的ai jellycat"></a>晚点聊107 Haivivi月入千万的ai jellycat</h3><p>嘉宾以前是天猫精灵的团队领导，他们发现跟天猫精灵互动最多的是孩子，所以出来创业做ai毛绒玩具。主打的是陪伴市场，因为不想跟大公司竞争教育，并且大模型的能力更适合作为毛绒玩具陪伴。在孩子眼里，毛绒玩具说话很正常，所以不需要教育市场。对于成年来说也是，因为现在毛绒玩具其实最大的买家是年轻人，因为能提供情绪价值。</p>
]]></content>
      <tags>
        <tag>podcast</tag>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title>Seedream 2.0: A Native Chinese-English Bilingual Image Generation Foundation Model</title>
    <url>/2025/03/16/Doubao%20Seedram%202.0/</url>
    <content><![CDATA[<p>原文链接 <a href="https://arxiv.org/pdf/2503.07703">https://arxiv.org/pdf/2503.07703</a></p>
<h2 id="导言"><a href="#导言" class="headerlink" title="导言"></a>导言</h2><p>豆包团队针对现有flux、Midjourney、SD3.5等模型对于1.模型长文本和多语言（中文）能力不足；2.不能理解中国文化 的问题，提出了seedream 2.0中英双语大模型。模型的创新性在于数据处理平台，双语言编码器以及后训练。这是一份33页的技术报告，写的非常详细。数据环节的解释非常清晰，编码器的结构和后训练环节的创新也很有亮点。尤其是后训练部分，细节多到令人感动。这篇文章让我感受到字节/豆包的底蕴，不愧是不惜血本挖人的宇宙厂，科研能力和产品能力都没得说。</p>
<h3 id="数据"><a href="#数据" class="headerlink" title="数据"></a>数据</h3><p>数据的组成包括高质量数据，分布保持数据，知识注入，以及一些针对性补充数据。高质量数据和其他模型的数据集差不多（clarity,aesthetic)，分布保持是做down sampling，在保持原始数据分布情况下减少低质量数据。知识注入包括了很多高质量的中文图文数据，并且其中一部分是只有中国文化有的数据。</p>
<p>数据清理分三步的漏斗系统。第一步，计算quality score, structure score(水印，logo)，然后用ocr去identify text。不符合的数据会被剔除；第二步，分层的进一步筛选。第三步，captioning 和 re-captioning。captioning的部分，豆包会对每一张图做 generic （长句子，短句子） 和 specialized （图片中的文字，美学，想象力）标注。</p>
<p>豆包还设计了一个active learning engine，先标注少量数据训练分类器，再利用分类器从无标注图像中挑选有价值的样本继续标注，形成 “标注 — 训练 — 再筛选” 的循环，逐步完善数据集。</p>
<h3 id="双语言编码器"><a href="#双语言编码器" class="headerlink" title="双语言编码器"></a>双语言编码器</h3><p><img src="/img/2025/03/5.png"> </p>
<p>现有扩散模型一般用clip或者t5当作text encoder，因为他们的embeddings 分布比较符合扩散模型。LLM虽然能力很强，但是它的数据分布不对。为了解决这个情况，豆包收集了高质量中文数据微调了decoder only 大模型，并针对渲染文本的字形特征，同时使用 LLM（大语言模型，作为文本编码器）和 ByT5 模型进行编码。</p>
<p>LLM 擅长捕捉文本的整体语义，尤其对中文复杂语境（如诗词、传统民俗描述）、文化内涵有深度理解。它能从海量数据中学习中文文化特征，确保生成图像准确表达文本语义，例如在生成包含中国传统元素的图像时，精准传递文化细节。作为双语编码器，LLM 支持中英双语语义对齐，使模型在处理双语提示时，保持跨语言生成的一致性。</p>
<p>Glyph-Aligned ByT5专注于字符级特征处理，解决文本渲染中的布局混乱、字符重复等问题。例如，在长文本或复杂排版（如竖排中文、书法字体）中，通过字符级嵌入对齐，实现高精度的文本布局生成，确保文字排列符合视觉逻辑。对多语言字符的细节处理更精细，提升模型在不同语言文本渲染任务中的普适性，尤其在非英文文本（如中文、日文）的排版中表现更优。</p>
<p>Diffusion的架构是dit，运用了针对分辨率的Scaling ROPE，使得同样图片在不同尺寸下能有相似的positional encoding。</p>
<h3 id="后训练"><a href="#后训练" class="headerlink" title="后训练"></a>后训练</h3><p>后训练分为四个阶段：</p>
<ol>
<li>Continue Training (CT) and Supervised fine-tuning (SFT) stages remarkably enhance the aesthetic appeal of the model; </li>
<li>Human Feedback Alignment (RLHF) stage significantly improves the model’s overall performance across all aspects via self-developed reward models and feedback learning algorithms; </li>
<li>Prompt Engineering (PE) further improves the performance on aesthetics and diversity by leveraging a fine-tuned LLM; </li>
<li>Finally, a refiner model is developed to scale up the resolution of an output image generated from our base model, and at the same time fix some minor structural errors.</li>
</ol>
<p>CT用了两种数据，机器从训练数据里筛选的高质量数据，以及人工选择的艺术/摄影/设计作品，按照一定的比例混合。训练的时候用了Value Mixing Control (VMix) Adapter，能更好的区分内容和美学的prompting，使得整体模型生成的图片更好看。SFT 整合了一些有标签的正样本，和一些模型生成的负样本来继续训练。</p>
<p>RLHF用了一个支持双语的clip作为reward mode，同时也用了 a image-text alignment RM, an aesthetic RM, and a text-rendering RM。</p>
<p>PE也分为两个阶段。第一个阶段是supervised llm fine-tuning，建立了一个pe模型 u -&gt; r，u是原始的prompt，r是模型改良的prompt。训练方法一是不断改进r，使得 u能通过r生成一个好的图片。二是找高质量文本对，不断地减少r的描述来还原u。第二个阶段是rlhf，通过第一阶段的pe生成很多prompt，然后人工选取positive negative pairs来做rl。</p>
<p>Refiner仍然是两个阶段。第一阶段是1024分辨率scaling，第二阶段找了一些高质量texture数据做downgrade，然后用这些数据训练了一个texture模型用来guide refiner 模型。</p>
<h3 id="Instruction-Based-Editing"><a href="#Instruction-Based-Editing" class="headerlink" title="Instruction-Based Editing"></a>Instruction-Based Editing</h3><p>运用了自研的SeedEdit，区别于其他solution，SeedEdit用diffusion作为encoder。为了改善人脸一致性的问题，用了内部的 ID/IP 模型，以及收集了很多ID/IP在不同条件下的图片。同时，模型结构引入了perception loss（face loss）来保持人脸一致性。</p>
<h3 id="模型加速"><a href="#模型加速" class="headerlink" title="模型加速"></a>模型加速</h3><p>Trajectory Segmented Consistency Distillation (TSCD) methodology，把 [0,T] 的时间段分为k segment，在训练的过程中逐渐减少。Quantization上也做了微调，支持不同模型部分的量化。</p>
]]></content>
      <tags>
        <tag>技术</tag>
        <tag>豆包</tag>
        <tag>论文</tag>
      </tags>
  </entry>
  <entry>
    <title>为什么要写博客</title>
    <url>/2025/03/14/%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E5%86%99%E5%8D%9A%E5%AE%A2/</url>
    <content><![CDATA[<p>最早有这个想法是一段时间之前的失眠，当时读了一本书讲到写日记/记录可以帮助睡眠。原理大概如下，如果每天睡觉前把今天的想法，和对未来的预期都写写下来，那大脑就会更放松。这样，也就不会在床上翻来覆去脑子里有很多想法。（“给思考减负：把日常的思考和琐事都记录下来。脑子需要操心的变少了，灵感变多了”， Sheng Xu 2025）我觉得确实，人的脑容量极其有限，但是和gpu集群相比虽然我们能耗很低，但是我们的学习能力应该是这一台基于backprop的模型不能比较的。那么我们确实没有必要和大模型去竞争记忆力，而是应该关注思考模型和学习模型。不重要的东西那就应该记下来，没必要占用大脑的缓存。</p>
<p>而在llm来了以后，我意识到了我对于llm有一点过于依赖了，能用llm解决的绝对不自己想。当然，一方面效率确实提高了，我在很短的时间学习到了很多知识。不过这种知识真的有用吗？一个人再怎么学也没有大模型学的快学的多吧？可能更多还是需要学习思维模型。另外一方面，由于过于依赖，我一时到了我的思考能力和语言能力都有不同程度的下滑。比如说，以前我的文章写得也不好，但是现在倒是变成提笔之后脑子完全崩不住来几个字了。鉴于这种情况，并且最近偶然翻到四火老师早期的文章，我意识到了可能在一些时候我需要脱离ai来保证我个人的状态，而其中一个方式就是不借助ai写博客。中文和英文我感觉没有特别大的所谓哈哈，中英夹杂更好一点。</p>
<p>除了保持自己的思考和语言能力，我也希望建立一个个人的知识库，来记录我不同时期的想法，并且可以在未来进行复盘。这个可以追溯到我早期的投资笔记，我忘了在哪里学习到把自己每次做决定的想法，交易决定的内容，和后面的回顾记录下来看。这样，能够更好的复盘反思，因为如果不记录的话人的回忆是不靠谱的。另外呢，之前在别人博客也看到可以直接用curosr + markdown当作一个rag系统。</p>
]]></content>
      <tags>
        <tag>思考</tag>
        <tag>杂谈</tag>
      </tags>
  </entry>
  <entry>
    <title>Seedream 3.0 Technical Report</title>
    <url>/2025/04/21/Doubao%20Seedram%203.0/</url>
    <content><![CDATA[<p>原文链接 <a href="https://arxiv.org/pdf/2504.11346">https://arxiv.org/pdf/2504.11346</a></p>
<h2 id="导言"><a href="#导言" class="headerlink" title="导言"></a>导言</h2><p>Seedream 2.0 虽然已经很好了，但是还有一些问题：模型在复杂prompt上的对齐有待提高，尤其是在数字精度和多物体空间关系的情况下；2.0对于图片内文字的生成能力有待提升；图片美学上的问题；以及生成图片的清晰度问题。对于以上问题，豆包做了一下提升：在数据层面，引入了双倍的高质量数据；增加了训练步骤和技巧，比如混合resolution training，多模态rope，新的representation alignment loss，以及resolution aware sampling。最后，也对后训练和生成加速做了提升。总而言之，3.0是一次对于2.0的incremental change，但是仅仅才过了一个月。从这能看出来字节豆包组的含金量，以及好的ai infra对于持续research和产品迭代的重要性。</p>
<h2 id="数据"><a href="#数据" class="headerlink" title="数据"></a>数据</h2><p>文章提到，2.0阶段运用了严格的数据筛选机制，所以这限制了训练数据的数量。在3.0中，豆包运用了新的筛选机制，把defect小于20%的数据保留下来，并且在训练的时候运用了spatial attention mask使得这些区域会被排除出训练，在保证模型稳定性的情况下成功扩展了差不多20%的训练数据。</p>
<h2 id="预训练"><a href="#预训练" class="headerlink" title="预训练"></a>预训练</h2><p>模型结构沿用了2.0，只是增加了训练参数和以下的技巧：<br>1.混合清晰度（resolution）训练。具体的讲，因为transformer天然支持不同长度的sequence，豆包组先用 256^2 的数据做pre-train，然后再更高清晰度（512^82 to 2048^2）的数据上做微调。并且，额外添加了size embedding作为额外的condition（应该是做了crocs attention？），使得模型能在没见过的清晰度情况下依然表现出色。<br>2.Cross-modality Rope。在2.0中，运用的是scaling rope。在3.0中，对于这个技巧做了提升。以往我们会对text做1d rope，对图片做2d。但是在cm rope里，会把text也当作一维的2d，做2d rope并投射到2d空间和图片关联起来。<br>3.运用了flow matching的损失函数，并且增加了alignment loss （用来对齐自己的mmdit和dinov2）可以让加速模型收敛。<br>4.Resolution-aware Timestep Sampling是一项diffusion模型训练的技巧，原理是在不同的resolution下对于我们sample的distribution做改变：high resolution图片会让sampling dist更偏向于lower snrs/higher noise levels。在训练阶段是用数据集的平均的resolution，inference的时候用期望的resolution来决定shift factor。具体做法是先从log-normal sample，然后根据我们算出来的shift factor做shifting。</p>
<h2 id="后训练"><a href="#后训练" class="headerlink" title="后训练"></a>后训练</h2><p>相比于2.0，3.0取消的refiner阶段因为模型本身已经能够生成不同resolution的图片。除此之外，还做了以下提升：为了ct和sft的阶段训练了更多的captioning model，能更好的让模型理解prompt中的美学，style和layout；平衡了数据里不同resolution数据的数量。</p>
<p>还有一点是用了vlm而不是clip作为奖励函数，具体做法如下：<br>1.Instruction as Query: The model receives a prompt, such as “A cat sitting on a couch.”​<br>2.Formulating the Question: This prompt is transformed into a question like, “Does this image depict a cat sitting on a couch? Please answer Yes or No.“<br>3.Evaluating with VLM: The VLM processes the generated image and the question, outputting probabilities for “Yes” and “No.”​<br>4.Deriving the Reward: The probability assigned to “Yes” is normalized and used as the reward signal. A higher probability indicates better alignment between the image and the prompt.</p>
<h3 id="模型加速"><a href="#模型加速" class="headerlink" title="模型加速"></a>模型加速</h3><p>seedream3.0的模型加速基于Hyper-SD和RayFlow，相比于传统的扩散模型在降噪过程中所有的样本都是通过一样的高斯分布路径，seedream对不同样本实现了个性化的单一通路，提升了模型的稳定性和生成的多元化。并且使用了一个预训练的模型来对噪声进行预估，这个方法使得模型在加噪和去噪的过程中能以最大可能性进行收敛，使得模型现在可以用较少的步数得到非常好的结果。在训练加速上，训练了一个结合Stochastic Stein Discrepancy (SSD)的neural net来预测哪个timestamp会产生最大的training loss，所以在训练采样的时候区别于传统的uniform sampling可以更高效的采样most important timesteamps。结合以上的工作，豆包的模型得以更高的效率达到普通扩散模型采样50步才能到达的效果。</p>
]]></content>
      <tags>
        <tag>技术</tag>
        <tag>豆包</tag>
        <tag>论文</tag>
      </tags>
  </entry>
  <entry>
    <title>Minimax Speech 2.0</title>
    <url>/2025/06/04/Minimax%20Speech%202.0/</url>
    <content><![CDATA[<p>原文链接 <a href="https://arxiv.org/pdf/2505.07916">https://arxiv.org/pdf/2505.07916</a></p>
<h2 id="导言"><a href="#导言" class="headerlink" title="导言"></a>导言</h2><p>我个人对Minimax这个公司还是比较友好感的：之前听过几次他们ceo和cto的podcast，能感受到他们不仅有商业上的布局，在技术上也有坚定的追求（linear attention）。所以，我对他们的新模型还是蛮期待的。事实证明，这一次新的tts模型用起来确实很优秀，尤其是在中文语音中。不过这篇技术报告内容一般，一半以上的篇幅都在讲自己效果怎么怎么好，感觉目的可能是秀肌肉居多而不是分享，猜测可能公司有融资方面的压力。</p>
<p>Minimax Speech 2.0是一个自回归的transformer架构tts（Text to Speech）模型，并且达到了sota的结果。这个模型的创新点在于，运用了一个科学系的speaker encoder使得0-shot learning成为可能，并且有也支持one shot。不仅如此，模型还运用了flow matching和flow vae decoder使得生成的效果更好。</p>
<h2 id="数据"><a href="#数据" class="headerlink" title="数据"></a>数据</h2><p>很可惜，这篇技术报告并没有提到训练数据的细节，只是含糊的讲了大家都知道的一些数据组成和预处理的方法：训练用了32种语言的数据；采用了两个独立地向ASR(Auto Speech Recognition)模型进行音频转录，加入结果接近可以认为是准确的，否则进一步将处理；用VAD（Voice Activity Detection）配合asr输出时间戳以及标点符号；保留录音中的背景稳态噪声，提高模型在真实环境下的鲁棒性；用SVR（Speaker Verification Model。目前我印象里比较全的TTS模型数据的描述还是来自几年前的open-ai whisper，感觉国内厂商在这一方面还是比较保守。</p>
<h2 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h2><p>模型的架构是经典的多模态架构：分别将不同模态压缩到一个unified space，然后decode出output。具体来讲，文字是用了经典的bpe作为encoder，语音则是用了Speaker Encoder + Audio Tokenizer，一个用来提取声音特征一个用来提取内容。与其他tts模型不同的是，minimax没有用一个pre-trained的audio encoder，而是把这个encoder和ar transformer用来一起训练。这么做的优点在于，pre-trained encoder的语料数据不够丰富，个人猜测可能对于中文的效果不好，minimax这一次应该是在数据方面加强了中文语料。</p>
<p>架构上的创新使得minimax可以实现高质量的0-shot learning，也就是用户只需要上传一段reference的语音就可以直接通过文字输出想要的声音克隆片段。相比之下，传统的语音模型需要 语音-文本 对进行 1-shot或者fine-tuning 才能达到不错的效果。</p>
<h2 id="flow-matching"><a href="#flow-matching" class="headerlink" title="flow matching"></a>flow matching</h2><p>Flow Matching模型是一种生成模型，本质是学习一种连续变换将简单的分布变成复杂的连续分布，tts模型一般会把ar transformer生成的离散token转换成连续的分布。</p>
<hr>
<h3 id="1-自回归-Transformer：生成离散音频-tokens"><a href="#1-自回归-Transformer：生成离散音频-tokens" class="headerlink" title="1. 自回归 Transformer：生成离散音频 tokens"></a><strong>1. 自回归 Transformer：生成离散音频 tokens</strong></h3><ul>
<li><p><strong>输入条件</strong>：</p>
<ul>
<li>文本编码后的 tokens（记为 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="0.98ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 433 453"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path></g></g></g></svg></mjx-container>）。</li>
<li>说话人编码器输出的条件向量（记为 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.097ex" height="1.027ex" role="img" focusable="false" viewBox="0 -443 485 454"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path></g></g></g></svg></mjx-container>），用于指定目标说话人的音色和风格。</li>
</ul>
</li>
<li><p><strong>处理逻辑</strong>：<br>自回归 Transformer 以文本 tokens 为输入，结合说话人条件向量 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.097ex" height="1.027ex" role="img" focusable="false" viewBox="0 -443 485 454"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path></g></g></g></svg></mjx-container>，通过注意力机制逐步生成离散音频 tokens（记为 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.052ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 465 453"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"></path></g></g></g></svg></mjx-container>）。这一过程模仿人类语音生成的时序特性，擅长捕捉韵律和语调的自然变化。</p>
</li>
<li><p><strong>优势</strong>：<br>相比非自回归模型，自回归架构无需显式建模音素持续时间对齐，通过隐式学习生成更自然的语音节奏。</p>
</li>
</ul>
<hr>
<h3 id="2-Latent-Flow-Matching-模块：从离散-tokens-到连续语音特征"><a href="#2-Latent-Flow-Matching-模块：从离散-tokens-到连续语音特征" class="headerlink" title="2. Latent Flow Matching 模块：从离散 tokens 到连续语音特征"></a><strong>2. Latent Flow Matching 模块：从离散 tokens 到连续语音特征</strong></h3><p>自回归 Transformer 输出的离散音频 tokens <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.052ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 465 453"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"></path></g></g></g></svg></mjx-container> 随后进入 Latent Flow Matching 模块，该模块包含两个关键组件：</p>
<h4 id="1-Flow-VAE：优化潜在特征表示"><a href="#1-Flow-VAE：优化潜在特征表示" class="headerlink" title="(1) Flow-VAE：优化潜在特征表示"></a><strong>(1) Flow-VAE：优化潜在特征表示</strong></h4><ul>
<li><p><strong>结构与功能</strong>：</p>
<ul>
<li><strong>Encoder</strong>：将离散音频 tokens <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.052ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 465 453"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"></path></g></g></g></svg></mjx-container> 转换为连续语音特征（潜在变量 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.052ex" height="1.722ex" role="img" focusable="false" viewBox="0 -750 465 761"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mover"><g data-mml-node="mi"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"></path></g><g data-mml-node="mo" transform="translate(288.1,332) translate(-250 0)"><path data-c="7E" d="M179 251Q164 251 151 245T131 234T111 215L97 227L83 238Q83 239 95 253T121 283T142 304Q165 318 187 318T253 300T320 282Q335 282 348 288T368 299T388 318L402 306L416 295Q375 236 344 222Q330 215 313 215Q292 215 248 233T179 251Z"></path></g></g></g></g></g></svg></mjx-container>），捕捉音频的声学细节（如音高、音色）。</li>
<li><strong>Flow Model</strong>：对潜在变量 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.052ex" height="1.722ex" role="img" focusable="false" viewBox="0 -750 465 761"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mover"><g data-mml-node="mi"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"></path></g><g data-mml-node="mo" transform="translate(288.1,332) translate(-250 0)"><path data-c="7E" d="M179 251Q164 251 151 245T131 234T111 215L97 227L83 238Q83 239 95 253T121 283T142 304Q165 318 187 318T253 300T320 282Q335 282 348 288T368 299T388 318L402 306L416 295Q375 236 344 222Q330 215 313 215Q292 215 248 233T179 251Z"></path></g></g></g></g></g></svg></mjx-container> 的分布进行可逆变换，将其映射到标准正态分布，以增强特征的表达能力和分布拟合能力。</li>
<li><strong>Decoder（神经声码器）</strong>：将潜在变量 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.052ex" height="1.722ex" role="img" focusable="false" viewBox="0 -750 465 761"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mover"><g data-mml-node="mi"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"></path></g><g data-mml-node="mo" transform="translate(288.1,332) translate(-250 0)"><path data-c="7E" d="M179 251Q164 251 151 245T131 234T111 215L97 227L83 238Q83 239 95 253T121 283T142 304Q165 318 187 318T253 300T320 282Q335 282 348 288T368 299T388 318L402 306L416 295Q375 236 344 222Q330 215 313 215Q292 215 248 233T179 251Z"></path></g></g></g></g></g></svg></mjx-container> 还原为音频波形 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.294ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 572 453"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g></g></g></svg></mjx-container>，通过 KL 散度约束确保重建精度。</li>
</ul>
</li>
<li><p><strong>创新点</strong>：<br>传统 VAE 假设潜在空间为标准正态分布，而 Flow-VAE 通过流模型的可逆变换（如仿射变换、置换），学习更复杂的后验分布，从而更准确地捕捉语音数据的多模态特征。<br>实验表明，Flow-VAE 的波形重建误差低于传统 VAE，且生成的语音特征更紧凑、信息更丰富。</p>
</li>
</ul>
<hr>
<h4 id="2-流匹配模型（Flow-Matching-Model）"><a href="#2-流匹配模型（Flow-Matching-Model）" class="headerlink" title="(2) 流匹配模型（Flow Matching Model）"></a><strong>(2) 流匹配模型（Flow Matching Model）</strong></h4><ul>
<li><p><strong>输入条件</strong>：</p>
<ul>
<li>自回归 Transformer 生成的离散音频 tokens <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.052ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 465 453"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"></path></g></g></g></svg></mjx-container>（经 Flow-VAE 编码为潜在变量 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.052ex" height="1.722ex" role="img" focusable="false" viewBox="0 -750 465 761"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mover"><g data-mml-node="mi"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"></path></g><g data-mml-node="mo" transform="translate(288.1,332) translate(-250 0)"><path data-c="7E" d="M179 251Q164 251 151 245T131 234T111 215L97 227L83 238Q83 239 95 253T121 283T142 304Q165 318 187 318T253 300T320 282Q335 282 348 288T368 299T388 318L402 306L416 295Q375 236 344 222Q330 215 313 215Q292 215 248 233T179 251Z"></path></g></g></g></g></g></svg></mjx-container>）。</li>
<li>说话人条件向量 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.097ex" height="1.027ex" role="img" focusable="false" viewBox="0 -443 485 454"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path></g></g></g></svg></mjx-container> 和文本编码后的上下文信息 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="0.98ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 433 453"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path></g></g></g></svg></mjx-container>（用于引导合成语音的风格和内容对齐）。</li>
</ul>
</li>
<li><p><strong>处理逻辑</strong>：<br>流匹配模型基于 Transformer 架构，对潜在变量 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.052ex" height="1.722ex" role="img" focusable="false" viewBox="0 -750 465 761"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mover"><g data-mml-node="mi"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"></path></g><g data-mml-node="mo" transform="translate(288.1,332) translate(-250 0)"><path data-c="7E" d="M179 251Q164 251 151 245T131 234T111 215L97 227L83 238Q83 239 95 253T121 283T142 304Q165 318 187 318T253 300T320 282Q335 282 348 288T368 299T388 318L402 306L416 295Q375 236 344 222Q330 215 313 215Q292 215 248 233T179 251Z"></path></g></g></g></g></g></svg></mjx-container> 的分布进行建模，通过匹配数据分布与先验分布（如标准正态分布），生成高质量的连续语音特征。该过程无需显式建模时长，而是通过隐式学习捕捉语音的时序依赖。</p>
</li>
<li><p><strong>优势</strong>：<br>相比直接预测下一个 token（Next Token Prediction），流匹配模型通过连续潜在空间的分布建模，避免了离散空间的量化误差，且能更灵活地处理语音的动态范围和细节变化。</p>
</li>
</ul>
]]></content>
      <tags>
        <tag>技术</tag>
        <tag>论文</tag>
        <tag>Minimax</tag>
      </tags>
  </entry>
  <entry>
    <title>Design Google Translate</title>
    <url>/2025/06/08/Design%20Google%20Translate/</url>
    <content><![CDATA[<h1 id="Google-Translate-System-Design"><a href="#Google-Translate-System-Design" class="headerlink" title="Google Translate System Design"></a>Google Translate System Design</h1><hr>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>Google Translate is a widely used language-translation service offered by Google. Powered by machine-learning (ML) models, it translates text between more than <strong>130 languages</strong> and serves <strong>over a billion users</strong> as of 2024.</p>
<p><img src="/img/2025/06/0301.webp" alt="Google Translate Overview"></p>
<hr>
<h2 id="Clarifying-Requirements"><a href="#Clarifying-Requirements" class="headerlink" title="Clarifying Requirements"></a>Clarifying Requirements</h2><ol>
<li>Real-Time translation vs Batch Translation (Model Architecture)</li>
<li>Text vs Audio vs Visual (Multi-modal)</li>
<li>Cloud vs On-Device (Model size, Inference Optimization)</li>
<li>Bilingual vs Multilingual</li>
</ol>
<p>To simplify the problem, we will limit the scope to Batch, Multilingual, Text and Cloud translation system.</p>
<hr>
<h2 id="Frame-the-Problem-as-an-ML-Task"><a href="#Frame-the-Problem-as-an-ML-Task" class="headerlink" title="Frame the Problem as an ML Task"></a>Frame the Problem as an ML Task</h2><h3 id="Specifying-the-System’s-Input-and-Output"><a href="#Specifying-the-System’s-Input-and-Output" class="headerlink" title="Specifying the System’s Input and Output"></a>Specifying the System’s Input and Output</h3><ul>
<li><strong>Input:</strong> A sequence of words in the <strong>source language</strong> and the <strong>desired target language</strong>.  </li>
<li><strong>Output:</strong> A sequence of words in the <strong>target language</strong>.</li>
</ul>
<h3 id="Choosing-a-Suitable-ML-Approach"><a href="#Choosing-a-Suitable-ML-Approach" class="headerlink" title="Choosing a Suitable ML Approach"></a>Choosing a Suitable ML Approach</h3><p>Language translation is a classic <strong>sequence-to-sequence (seq2seq)</strong> task. Modern systems favor <strong>Transformer-based encoder-decoder models</strong>:</p>
<ol>
<li><strong>Encoder</strong> – Converts the source sentence into contextual vectors.  </li>
<li><strong>Decoder</strong> – Generates the target sentence token-by-token, attending to the encoder’s output.</li>
</ol>
<p>Why Transformers?</p>
<ul>
<li>Handle <strong>long-range dependencies</strong> better than LSTMs/GRUs.  </li>
<li>The <strong>attention mechanism</strong> (originally proposed for translation [2]) lets the decoder focus on relevant source tokens.  </li>
<li>Encoder–decoder separation aligns naturally with translation.</li>
</ul>
<p>Other models Choices</p>
<ul>
<li>Non-Autoregressive Transformers (NAT): Decodes all tokens in parallel → much faster, but typically lower quality. Often used for speed-sensitive applications Can be combined with knowledge distillation to improve performance.</li>
<li>Prompt-based Translation using Decoder-only LLMs. Zero/few-shot with no retraining but Larger and slower than specialized MT models.</li>
</ul>
<hr>
<h2 id="Data-Preparation"><a href="#Data-Preparation" class="headerlink" title="Data Preparation"></a>Data Preparation</h2><p>Two data types feed the model:</p>
<ol>
<li><strong>General data</strong> – Large-scale multilingual text from the internet.  </li>
<li><strong>Translation data</strong> – ≈ 300 million sentence pairs (source + target).</li>
</ol>
<p>Preparation focuses on translation data.</p>
<h4 id="1-Primary-Data-Parallel-Corpora"><a href="#1-Primary-Data-Parallel-Corpora" class="headerlink" title="1. Primary Data: Parallel Corpora"></a>1. Primary Data: Parallel Corpora</h4><p>This is the most crucial data, consisting of texts that are direct, sentence-by-sentence translations of each other. It forms the core training material for the model.</p>
<ul>
<li>Public Datasets: High-quality, formal corpora from official sources like the United Nations (UN) and the European Parliament (Europarl). These provide a strong, clean baseline.</li>
<li>Web Crawling: Automatically crawling multilingual websites (like news sites or corporate pages) to find and extract translated pages. This provides a massive, diverse, but often “noisy” source of data that requires significant cleaning.</li>
</ul>
<h4 id="2-Data-Expansion-and-Augmentation"><a href="#2-Data-Expansion-and-Augmentation" class="headerlink" title="2. Data Expansion and Augmentation"></a>2. Data Expansion and Augmentation</h4><p>Because high-quality parallel data is limited, several techniques are used to create more training examples:</p>
<ul>
<li>User Feedback: Leveraging the “Suggest an edit” feature and contributions from the Translate Community. This creates a continuous improvement loop by using real-world corrections to fix model weaknesses.</li>
<li>Back-Translation: A powerful technique for low-resource languages. It involves taking monolingual text in the target language (e.g., German), using an existing model to translate it back to the source language (English), and then using this new synthetic {source, target} pair for training. This dramatically improves the translation’s fluency.</li>
<li>Mining Comparable Corpora: This method is used when texts are about the same topic but are not direct translations (e.g., English and German news articles about the same event).</li>
</ul>
<h3 id="1-·-Text-Pre-processing"><a href="#1-·-Text-Pre-processing" class="headerlink" title="1 · Text Pre-processing"></a>1 · Text Pre-processing</h3><ul>
<li>Remove <strong>missing</strong> or <strong>noisy</strong> pairs (HTML, wrong language).  </li>
<li><strong>Deduplicate</strong> sentence pairs.  </li>
<li><strong>Handle named entities</strong> with placeholders (e.g., <code>ENTITY_1</code>).  </li>
<li>Skip older steps (lower-casing, stop-word removal, stemming, punctuation stripping) because modern Transformers learn these patterns directly.</li>
</ul>
<hr>
<h2 id="Preprocessing-Steps-—-Modern-Translation-Pretraining"><a href="#Preprocessing-Steps-—-Modern-Translation-Pretraining" class="headerlink" title="Preprocessing Steps — Modern Translation Pretraining"></a><strong>Preprocessing Steps — Modern Translation Pretraining</strong></h2><h3 id="🔎-1️⃣-Data-Cleaning-Deduplication"><a href="#🔎-1️⃣-Data-Cleaning-Deduplication" class="headerlink" title="🔎 1️⃣ Data Cleaning & Deduplication"></a>🔎 <strong>1️⃣ Data Cleaning &amp; Deduplication</strong></h3><table>
<thead>
<tr>
<th>Task</th>
<th>Purpose</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Remove noisy sentences</strong></td>
<td>Drop sentence pairs with very long or very short sequences, or mismatched alignments.</td>
</tr>
<tr>
<td><strong>Filter profanity / sensitive content</strong></td>
<td>Ensure safe outputs.</td>
</tr>
<tr>
<td><strong>Deduplicate</strong></td>
<td>Remove duplicate sentence pairs and repeated monolingual data (web crawl has lots of duplication).</td>
</tr>
<tr>
<td><strong>Script normalization</strong></td>
<td>Normalize Unicode (NFC/NFD), convert different scripts consistently (e.g., Simplified ↔ Traditional Chinese).</td>
</tr>
</tbody></table>
<p>✅ <strong>Why:</strong> Reduces noise → improves model generalization.</p>
<hr>
<h3 id="🔎-2️⃣-Language-Identification-Tagging"><a href="#🔎-2️⃣-Language-Identification-Tagging" class="headerlink" title="🔎 2️⃣ Language Identification & Tagging"></a>🔎 <strong>2️⃣ Language Identification &amp; Tagging</strong></h3><table>
<thead>
<tr>
<th>Task</th>
<th>Purpose</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Language detection</strong></td>
<td>Auto-identify language of monolingual data (and confirm for parallel pairs).</td>
</tr>
<tr>
<td><strong>Assign language tags</strong></td>
<td>E.g., add &gt;&gt;fr&lt;&lt; or &gt;&gt;zh&lt;&lt; to source text so the model knows which language to translate into.</td>
</tr>
</tbody></table>
<p>✅ <strong>Why:</strong> Enables multilingual pretraining and zero-shot/few-shot transfer.</p>
<hr>
<h3 id="🔎-3️⃣-Tokenization"><a href="#🔎-3️⃣-Tokenization" class="headerlink" title="🔎 3️⃣ Tokenization"></a>🔎 <strong>3️⃣ Tokenization</strong></h3><table>
<thead>
<tr>
<th>Task</th>
<th>Purpose</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Subword tokenization (SentencePiece, BPE, WordPiece)</strong></td>
<td>Split words into common subword units to handle rare/unseen words.</td>
</tr>
<tr>
<td><strong>Multilingual vocabulary</strong></td>
<td>Train a shared tokenizer across all languages.</td>
</tr>
</tbody></table>
<p>✅ <strong>Why:</strong> Handles vocabulary for hundreds of languages in a scalable way.</p>
<hr>
<h3 id="🔎-4️⃣-Alignment-Verification-Parallel-Data"><a href="#🔎-4️⃣-Alignment-Verification-Parallel-Data" class="headerlink" title="🔎 4️⃣ Alignment Verification (Parallel Data)"></a>🔎 <strong>4️⃣ Alignment Verification (Parallel Data)</strong></h3><table>
<thead>
<tr>
<th>Task</th>
<th>Purpose</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Length ratio checks</strong></td>
<td>Filter out sentence pairs with extreme length mismatches.</td>
</tr>
<tr>
<td><strong>Translation quality scoring (optional)</strong></td>
<td>Use tools like LASER or BLEU filtering to keep only high-quality sentence pairs.</td>
</tr>
</tbody></table>
<p>✅ <strong>Why:</strong> Ensures that paired data teaches the model correct alignments.</p>
<hr>
<h3 id="🔎-5️⃣-Corruption-Masking-for-Monolingual"><a href="#🔎-5️⃣-Corruption-Masking-for-Monolingual" class="headerlink" title="🔎 5️⃣ Corruption / Masking (for Monolingual)"></a>🔎 <strong>5️⃣ Corruption / Masking (for Monolingual)</strong></h3><table>
<thead>
<tr>
<th>Task</th>
<th>Purpose</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Noise injection (shuffling, masking, deletion)</strong></td>
<td>Prepare monolingual data for Denoising Auto-Encoding (DAE) or MLM tasks.</td>
</tr>
</tbody></table>
<p>✅ <strong>Why:</strong> Teaches encoder-decoder models to recover from corrupted inputs → boosts robustness and learning.</p>
<hr>
<h3 id="🔎-6️⃣-Sampling-Balancing"><a href="#🔎-6️⃣-Sampling-Balancing" class="headerlink" title="🔎 6️⃣ Sampling & Balancing"></a>🔎 <strong>6️⃣ Sampling &amp; Balancing</strong></h3><table>
<thead>
<tr>
<th>Task</th>
<th>Purpose</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Upsampling low-resource languages</strong></td>
<td>Increase frequency of rare languages in the training stream.</td>
</tr>
<tr>
<td><strong>Downsampling high-resource languages</strong></td>
<td>Prevent overfitting to dominant languages like English or Spanish.</td>
</tr>
</tbody></table>
<p>✅ <strong>Why:</strong> Balances the multilingual training data.</p>
<hr>
<h3 id="🔎-7️⃣-Back-Translation-for-Monolingual-→-Synthetic-Parallel"><a href="#🔎-7️⃣-Back-Translation-for-Monolingual-→-Synthetic-Parallel" class="headerlink" title="🔎 7️⃣ Back-Translation (for Monolingual → Synthetic Parallel)"></a>🔎 <strong>7️⃣ Back-Translation (for Monolingual → Synthetic Parallel)</strong></h3><table>
<thead>
<tr>
<th>Task</th>
<th>Purpose</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Translate monolingual target sentences back into source languages</strong></td>
<td>Create synthetic parallel data when human-translated data is missing.</td>
</tr>
</tbody></table>
<p>✅ <strong>Why:</strong> Expands training data for low-resource pairs.</p>
<hr>
<h3 id="2-·-Text-Tokenization"><a href="#2-·-Text-Tokenization" class="headerlink" title="2 · Text Tokenization"></a>2 · Text Tokenization</h3><p>Word-level vocabularies explode in size across 130+ languages, so we use <strong>sub-word tokenization</strong>—specifically <strong>Byte-Pair Encoding (BPE)</strong> .</p>
<p>Example token-to-ID mapping:</p>
<table>
<thead>
<tr>
<th>Token</th>
<th>ID</th>
</tr>
</thead>
<tbody><tr>
<td><code>&lt;BOS&gt;</code></td>
<td>0</td>
</tr>
<tr>
<td><code>&lt;EOS&gt;</code></td>
<td>1</td>
</tr>
<tr>
<td>walking</td>
<td>2</td>
</tr>
<tr>
<td>bonjour</td>
<td>3</td>
</tr>
<tr>
<td>hello</td>
<td>4</td>
</tr>
<tr>
<td>fantastique</td>
<td>5</td>
</tr>
<tr>
<td>…</td>
<td>…</td>
</tr>
</tbody></table>
<hr>
<h3 id="1-Byte-Pair-Encoding-BPE"><a href="#1-Byte-Pair-Encoding-BPE" class="headerlink" title="1. Byte-Pair Encoding (BPE)"></a>1. <strong>Byte-Pair Encoding (BPE)</strong></h3><p><strong>Used in</strong>: GPT-2, RoBERTa</p>
<ul>
<li><strong>Idea</strong>: Start from characters, iteratively merge the most frequent pairs of tokens.</li>
<li><strong>Goal</strong>: Build a vocabulary of subwords to reduce OOV (out-of-vocabulary) issues.</li>
<li><strong>Deterministic</strong>: Given a trained vocabulary, tokenization is consistent.</li>
</ul>
<p><strong>Example</strong>:<br>Training corpus: <code>"low lower newest widest"</code></p>
<ol>
<li>Start with chars: <code>l o w</code>, <code>l o w e r</code>, etc.</li>
<li>Find most common pair (e.g., <code>'e'</code> + <code>'r'</code> → <code>'er'</code>), merge into <code>'er'</code></li>
<li>Repeat until vocab size is met.</li>
</ol>
<p>Tokenize <code>"lower"</code> → <code>['low', 'er']</code> if <code>'low'</code> and <code>'er'</code> exist in vocab.</p>
<hr>
<h3 id="2-Byte-Level-BPE-BBPE"><a href="#2-Byte-Level-BPE-BBPE" class="headerlink" title="2. Byte-Level BPE (BBPE)"></a>2. <strong>Byte-Level BPE (BBPE)</strong></h3><p><strong>Used in</strong>: GPT-3, GPT-4</p>
<ul>
<li><strong>Extension of BPE</strong>, but operates on <strong>bytes</strong>, not Unicode characters.</li>
<li>Handles all inputs (e.g., emojis, unseen scripts) without needing pre-normalization.</li>
<li>More robust to misspellings and multilingual input.</li>
<li>deals with unseen or non-standard characters best， since all text (even unknown Unicode symbols) can be represented as bytes</li>
</ul>
<p><strong>Example</strong>:<br>Input: <code>"apple 🍎"</code></p>
<ol>
<li>Break into raw bytes: <code>['a', 'p', 'p', 'l', 'e', ' ', '🍎']</code></li>
<li>Apply merges on bytes to form subwords: <code>['app', 'le', ' ', '🍎']</code> (assuming <code>'app'</code> and <code>'le'</code> exist)</li>
</ol>
<hr>
<h3 id="3-Unigram-Language-Model"><a href="#3-Unigram-Language-Model" class="headerlink" title="3. Unigram Language Model"></a>3. <strong>Unigram Language Model</strong></h3><p><strong>Used in</strong>: SentencePiece (T5, ALBERT)</p>
<ul>
<li>Trains a <strong>probabilistic model</strong> of subword units.</li>
<li>Starts with many subwords, gradually removes the least likely ones.</li>
<li>At inference, selects the most likely token sequence via Viterbi.</li>
</ul>
<p><strong>Example</strong>: <code>"international"</code><br>May be tokenized as <code>['inter', 'national']</code> or <code>['intern', 'ation', 'al']</code> depending on highest probability.</p>
<hr>
<h3 id="4-WordPiece"><a href="#4-WordPiece" class="headerlink" title="4. WordPiece"></a>4. <strong>WordPiece</strong></h3><p><strong>Used in</strong>: BERT</p>
<ul>
<li>Similar to BPE, but selects merges to <strong>maximize likelihood</strong> of training data under a language model.</li>
<li>Always starts with a <strong>whole word</strong>, splits into subwords with <code>##</code> prefix for non-initial parts.</li>
</ul>
<p><strong>Example</strong>: <code>"unhappiness"</code><br>→ <code>['un', '##happiness']</code> or <code>['un', '##hap', '##pi', '##ness']</code> depending on vocab.</p>
<hr>
<h3 id="Summary-Table"><a href="#Summary-Table" class="headerlink" title="Summary Table:"></a>Summary Table:</h3><table>
<thead>
<tr>
<th>Algorithm</th>
<th>Used In</th>
<th>Key Feature</th>
<th>Handles OOV?</th>
<th>Probabilistic?</th>
</tr>
</thead>
<tbody><tr>
<td>BPE</td>
<td>GPT-2, RoBERTa</td>
<td>Merges frequent char pairs</td>
<td>Yes</td>
<td>No</td>
</tr>
<tr>
<td>Byte-level BPE</td>
<td>GPT-3, GPT-4</td>
<td>Merges on byte sequences</td>
<td>Yes</td>
<td>No</td>
</tr>
<tr>
<td>WordPiece</td>
<td>BERT</td>
<td>Greedy merging with LM scoring</td>
<td>Yes</td>
<td>No</td>
</tr>
<tr>
<td>Unigram LM</td>
<td>T5, ALBERT</td>
<td>Picks best subwords probabilistically</td>
<td>Yes</td>
<td>Yes</td>
</tr>
</tbody></table>
<p>Let me know if you’d like to visualize token merges step by step.</p>
<hr>
<h2 id="Model-Development"><a href="#Model-Development" class="headerlink" title="Model Development"></a>Model Development</h2><h3 id="Architecture-Overview"><a href="#Architecture-Overview" class="headerlink" title="Architecture Overview"></a>Architecture Overview</h3><p>Encoder-decoder Transformer components:</p>
<table>
<thead>
<tr>
<th>Component</th>
<th>Encoder</th>
<th>Decoder</th>
</tr>
</thead>
<tbody><tr>
<td>Token Embedding</td>
<td>✅</td>
<td>✅</td>
</tr>
<tr>
<td>Positional Encoding</td>
<td>✅</td>
<td>✅</td>
</tr>
<tr>
<td><strong>Self-Attention</strong></td>
<td>Full (bi-directional)</td>
<td>Masked (causal)</td>
</tr>
<tr>
<td><strong>Cross-Attention</strong></td>
<td>—</td>
<td>✅</td>
</tr>
<tr>
<td>Prediction Head</td>
<td>—</td>
<td>✅</td>
</tr>
</tbody></table>
<p>Key differences:</p>
<ol>
<li><strong>Cross-Attention Layer</strong> – Decoder attends to encoder outputs.  </li>
<li><strong>Masked Self-Attention</strong> – Decoder can’t see future tokens.  </li>
<li><strong>Prediction Head</strong> – Linear + soft-max layer produces token probabilities.</li>
</ol>
<p>Other key information:</p>
<ul>
<li>Positional Embeddings: Rope, Sinusoidal</li>
<li>Residual Connection</li>
<li>MOE architecture</li>
<li>Self-Attention, Cross-Attention</li>
</ul>
<h3 id="Training-Strategy-by-Alex"><a href="#Training-Strategy-by-Alex" class="headerlink" title="Training Strategy by Alex"></a>Training Strategy by Alex</h3><table>
<thead>
<tr>
<th>Stage</th>
<th>Data</th>
<th>Objective</th>
<th>Loss</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Unsupervised Pre-training</strong></td>
<td>Multilingual general text</td>
<td><strong>Masked Language Modeling (MLM)</strong></td>
<td>Cross-entropy</td>
</tr>
<tr>
<td><strong>Supervised Finetuning</strong></td>
<td>Parallel sentence pairs (bilingual)</td>
<td><strong>Next-token prediction</strong></td>
<td>Cross-entropy</td>
</tr>
</tbody></table>
<ul>
<li>Bilingual vs. Multilingual:* This design opts for <strong>bilingual models</strong> (higher accuracy per pair, at higher ops cost).</li>
</ul>
<h3 id="ML-Objective-Loss-Function-—-Revised-✍️"><a href="#ML-Objective-Loss-Function-—-Revised-✍️" class="headerlink" title="ML Objective & Loss Function — Revised ✍️"></a>ML Objective &amp; Loss Function — Revised ✍️</h3><p>Below is a tighter, more technically precise version of the “ML objective and loss function” section, organised to show <em>why</em> each design choice matters when training a Neural Machine Translation (NMT) system.</p>
<hr>
<h4 id="1-Pre-training-Learning-Multilingual-Representations"><a href="#1-Pre-training-Learning-Multilingual-Representations" class="headerlink" title="1  |  Pre-training: Learning Multilingual Representations"></a>1  |  Pre-training: Learning Multilingual Representations</h4><p><strong>Data</strong><br>We keep the multilingual portions of large-scale corpora (C4, Wikipedia, Stack Exchange, Common-Crawl, etc.) and drop only the languages we never intend to translate. This maximises coverage while avoiding label noise from irrelevant scripts.</p>
<p><strong>Objective</strong> — <em>Masked Language Modeling</em> (MLM) with span corruption</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">Input  :  The quick brown &lt;mask&gt; jumps over the &lt;mask&gt; dog .</span><br><span class="line">Target :                    fox                      lazy</span><br></pre></td></tr></table></figure>

<ul>
<li>Why not next-token prediction? In unsupervised pre-training the decoder could simply echo the current token, so the encoder would learn little. MLM forces the encoder to build a genuine contextual representation that the decoder must rely on.</li>
<li>Span masking (T5-style) hides contiguous chunks, encouraging longer-range reasoning and reducing the number of prediction steps.</li>
<li>15 % of tokens are selected; 80 % are replaced by <code>&lt;mask&gt;</code>, 10 % by random tokens, 10 % left unchanged to give the model noise robustness.</li>
</ul>
<p><strong>Loss</strong> — Cross-entropy over <em>only</em> the masked positions</p>

<mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -2.83ex;" xmlns="http://www.w3.org/2000/svg" width="24.219ex" height="4.979ex" role="img" focusable="false" viewBox="0 -950 10704.7 2200.9"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="4C" d="M62 -22T47 -22T32 -11Q32 -1 56 24T83 55Q113 96 138 172T180 320T234 473T323 609Q364 649 419 677T531 705Q559 705 578 696T604 671T615 645T618 623V611Q618 582 615 571T598 548Q581 531 558 520T518 509Q503 509 503 520Q503 523 505 536T507 560Q507 590 494 610T452 630Q423 630 410 617Q367 578 333 492T271 301T233 170Q211 123 204 112L198 103L224 102Q281 102 369 79T509 52H523Q535 64 544 87T579 128Q616 152 641 152Q656 152 656 142Q656 101 588 40T433 -22Q381 -22 289 1T156 28L141 29L131 20Q111 0 87 -11Z"></path></g></g><g data-mml-node="TeXAtom" transform="translate(723,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mtext"><path data-c="4D" d="M132 622Q125 629 121 631T105 634T62 637H29V683H135Q221 683 232 682T249 675Q250 674 354 398L458 124L562 398Q666 674 668 675Q671 681 683 682T781 683H887V637H854Q814 636 803 634T785 622V61Q791 51 802 49T854 46H887V0H876Q855 3 736 3Q605 3 596 0H585V46H618Q660 47 669 49T688 61V347Q688 424 688 461T688 546T688 613L687 632Q454 14 450 7Q446 1 430 1T410 7Q409 9 292 316L176 624V606Q175 588 175 543T175 463T175 356L176 86Q187 50 261 46H278V0H269Q254 3 154 3Q52 3 37 0H29V46H46Q78 48 98 56T122 69T132 86V622Z"></path><path data-c="4C" d="M128 622Q121 629 117 631T101 634T58 637H25V683H36Q48 680 182 680Q324 680 348 683H360V637H333Q273 637 258 635T233 622L232 342V129Q232 57 237 52Q243 47 313 47Q384 47 410 53Q470 70 498 110T536 221Q536 226 537 238T540 261T542 272T562 273H582V268Q580 265 568 137T554 5V0H25V46H58Q100 47 109 49T128 61V622Z" transform="translate(917,0)"></path><path data-c="4D" d="M132 622Q125 629 121 631T105 634T62 637H29V683H135Q221 683 232 682T249 675Q250 674 354 398L458 124L562 398Q666 674 668 675Q671 681 683 682T781 683H887V637H854Q814 636 803 634T785 622V61Q791 51 802 49T854 46H887V0H876Q855 3 736 3Q605 3 596 0H585V46H618Q660 47 669 49T688 61V347Q688 424 688 461T688 546T688 613L687 632Q454 14 450 7Q446 1 430 1T410 7Q409 9 292 316L176 624V606Q175 588 175 543T175 463T175 356L176 86Q187 50 261 46H278V0H269Q254 3 154 3Q52 3 37 0H29V46H46Q78 48 98 56T122 69T132 86V622Z" transform="translate(1542,0)"></path></g></g></g><g data-mml-node="mo" transform="translate(2789.6,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mo" transform="translate(3845.3,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="munder" transform="translate(4790,0)"><g data-mml-node="mo" transform="translate(60.4,0)"><path data-c="2211" d="M60 948Q63 950 665 950H1267L1325 815Q1384 677 1388 669H1348L1341 683Q1320 724 1285 761Q1235 809 1174 838T1033 881T882 898T699 902H574H543H251L259 891Q722 258 724 252Q725 250 724 246Q721 243 460 -56L196 -356Q196 -357 407 -357Q459 -357 548 -357T676 -358Q812 -358 896 -353T1063 -332T1204 -283T1307 -196Q1328 -170 1348 -124H1388Q1388 -125 1381 -145T1356 -210T1325 -294L1267 -449L666 -450Q64 -450 61 -448Q55 -446 55 -439Q55 -437 57 -433L590 177Q590 178 557 222T452 366T322 544L56 909L55 924Q55 945 60 948Z"></path></g><g data-mml-node="TeXAtom" transform="translate(0,-1115.5) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(345,0)"><path data-c="2208" d="M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(1012,0)"><g data-mml-node="mi"><path data-c="4D" d="M28 9Q28 37 43 63T73 90Q77 90 83 84T103 70T141 57H146Q162 57 178 79T222 167Q266 279 295 371T334 513T349 598T358 651T371 677Q397 705 432 705Q442 705 445 699T452 666Q453 661 453 659Q475 538 509 405T568 207L574 192Q581 178 587 164T594 150Q596 150 635 189T693 248Q765 324 863 438T1024 626T1089 701Q1093 705 1100 705Q1111 705 1111 682Q1111 675 1108 660T1099 611T1086 540Q1041 277 1041 144Q1041 98 1044 75T1050 48T1059 42Q1064 41 1075 46Q1102 61 1121 61Q1137 61 1137 50Q1137 28 1087 0T1000 -29Q983 -29 972 -23T955 -9T945 16T942 45T941 83V96Q941 158 952 256T974 422L985 489Q984 489 939 436T821 300T698 164Q665 128 620 85T568 37Q564 34 558 34Q550 34 546 37T535 54Q512 91 496 127T450 259T389 498L384 518Q349 367 294 223T198 15Q155 -50 117 -50Q87 -50 61 -35T30 -6Q28 2 28 9Z"></path></g></g></g></g><g data-mml-node="mi" transform="translate(6521.5,0)"><path data-c="6C" d="M42 46H56Q95 46 103 60V68Q103 77 103 91T103 124T104 167T104 217T104 272T104 329Q104 366 104 407T104 482T104 542T103 586T103 603Q100 622 89 628T44 637H26V660Q26 683 28 683L38 684Q48 685 67 686T104 688Q121 689 141 690T171 693T182 694H185V379Q185 62 186 60Q190 52 198 49Q219 46 247 46H263V0H255L232 1Q209 2 183 2T145 3T107 3T57 1L34 0H26V46H42Z"></path><path data-c="6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z" transform="translate(278,0)"></path><path data-c="67" d="M329 409Q373 453 429 453Q459 453 472 434T485 396Q485 382 476 371T449 360Q416 360 412 390Q410 404 415 411Q415 412 416 414V415Q388 412 363 393Q355 388 355 386Q355 385 359 381T368 369T379 351T388 325T392 292Q392 230 343 187T222 143Q172 143 123 171Q112 153 112 133Q112 98 138 81Q147 75 155 75T227 73Q311 72 335 67Q396 58 431 26Q470 -13 470 -72Q470 -139 392 -175Q332 -206 250 -206Q167 -206 107 -175Q29 -140 29 -75Q29 -39 50 -15T92 18L103 24Q67 55 67 108Q67 155 96 193Q52 237 52 292Q52 355 102 398T223 442Q274 442 318 416L329 409ZM299 343Q294 371 273 387T221 404Q192 404 171 388T145 343Q142 326 142 292Q142 248 149 227T179 192Q196 182 222 182Q244 182 260 189T283 207T294 227T299 242Q302 258 302 292T299 343ZM403 -75Q403 -50 389 -34T348 -11T299 -2T245 0H218Q151 0 138 -6Q118 -15 107 -34T95 -74Q95 -84 101 -97T122 -127T170 -155T250 -167Q319 -167 361 -139T403 -75Z" transform="translate(778,0)"></path></g><g data-mml-node="mo" transform="translate(7799.5,0)"><path data-c="2061" d=""></path></g><g data-mml-node="msub" transform="translate(7966.2,0)"><g data-mml-node="mi"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></g><g data-mml-node="mi" transform="translate(536,-150) scale(0.707)"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g></g><g data-mml-node="mo" transform="translate(8883.8,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(9272.8,0)"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mi" transform="translate(749,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(10315.7,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container>

<p>with optional label smoothing (ε ≈ 0.1) for better generalisation.</p>
<hr>
<h4 id="2-Supervised-Fine-tuning-Turning-a-General-LM-into-a-Translator"><a href="#2-Supervised-Fine-tuning-Turning-a-General-LM-into-a-Translator" class="headerlink" title="2  |  Supervised Fine-tuning: Turning a General LM into a Translator"></a>2  |  Supervised Fine-tuning: Turning a General LM into a Translator</h4><p>With sentence-aligned pairs <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="5.513ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 2436.7 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(389,0)"><g data-mml-node="mi"><path data-c="1D431" d="M227 0Q212 3 121 3Q40 3 28 0H21V62H117L245 213L109 382H26V444H34Q49 441 143 441Q247 441 265 444H274V382H246L281 339Q315 297 316 297Q320 297 354 341L389 382H352V444H360Q375 441 466 441Q547 441 559 444H566V382H471L355 246L504 63L545 62H586V0H578Q563 3 469 3Q365 3 347 0H338V62H366Q366 63 326 112T285 163L198 63L217 62H235V0H227Z"></path></g></g><g data-mml-node="mo" transform="translate(996,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(1440.7,0)"><g data-mml-node="mi"><path data-c="1D432" d="M84 -102Q84 -110 87 -119T102 -138T133 -149Q148 -148 162 -143T186 -131T206 -114T222 -95T234 -76T243 -59T249 -45T252 -37L269 0L96 382H26V444H34Q49 441 146 441Q252 441 270 444H279V382H255Q232 382 232 380L337 151L442 382H394V444H401Q413 441 495 441Q568 441 574 444H580V382H510L406 152Q298 -84 297 -87Q269 -139 225 -169T131 -200Q85 -200 54 -172T23 -100Q23 -64 44 -50T87 -35Q111 -35 130 -50T152 -92V-100H84V-102Z"></path></g></g><g data-mml-node="mo" transform="translate(2047.7,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container>:</p>
<ol>
<li><strong>Encoder</strong> ingests the full source sentence <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="1.373ex" height="1.005ex" role="img" focusable="false" viewBox="0 -444 607 444"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D431" d="M227 0Q212 3 121 3Q40 3 28 0H21V62H117L245 213L109 382H26V444H34Q49 441 143 441Q247 441 265 444H274V382H246L281 339Q315 297 316 297Q320 297 354 341L389 382H352V444H360Q375 441 466 441Q547 441 559 444H566V382H471L355 246L504 63L545 62H586V0H578Q563 3 469 3Q365 3 347 0H338V62H366Q366 63 326 112T285 163L198 63L217 62H235V0H227Z"></path></g></g></g></g></svg></mjx-container>.  </li>
<li><strong>Decoder</strong> is trained with <em>teacher forcing</em>: it receives the gold target prefix <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.452ex;" xmlns="http://www.w3.org/2000/svg" width="3.383ex" height="1.457ex" role="img" focusable="false" viewBox="0 -444 1495.4 644"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D432" d="M84 -102Q84 -110 87 -119T102 -138T133 -149Q148 -148 162 -143T186 -131T206 -114T222 -95T234 -76T243 -59T249 -45T252 -37L269 0L96 382H26V444H34Q49 441 146 441Q252 441 270 444H279V382H255Q232 382 232 380L337 151L442 382H394V444H401Q413 441 495 441Q568 441 574 444H580V382H510L406 152Q298 -84 297 -87Q269 -139 225 -169T131 -200Q85 -200 54 -172T23 -100Q23 -64 44 -50T87 -35Q111 -35 130 -50T152 -92V-100H84V-102Z"></path></g></g><g data-mml-node="TeXAtom" transform="translate(640,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><path data-c="3C" d="M694 -11T694 -19T688 -33T678 -40Q671 -40 524 29T234 166L90 235Q83 240 83 250Q83 261 91 266Q664 540 678 540Q681 540 687 534T694 519T687 505Q686 504 417 376L151 250L417 124Q686 -4 687 -5Q694 -11 694 -19Z"></path></g><g data-mml-node="mi" transform="translate(778,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g></g></g></g></g></svg></mjx-container> and predicts the next token <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.452ex;" xmlns="http://www.w3.org/2000/svg" width="2.139ex" height="1.457ex" role="img" focusable="false" viewBox="0 -444 945.3 644"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D432" d="M84 -102Q84 -110 87 -119T102 -138T133 -149Q148 -148 162 -143T186 -131T206 -114T222 -95T234 -76T243 -59T249 -45T252 -37L269 0L96 382H26V444H34Q49 441 146 441Q252 441 270 444H279V382H255Q232 382 232 380L337 151L442 382H394V444H401Q413 441 495 441Q568 441 574 444H580V382H510L406 152Q298 -84 297 -87Q269 -139 225 -169T131 -200Q85 -200 54 -172T23 -100Q23 -64 44 -50T87 -35Q111 -35 130 -50T152 -92V-100H84V-102Z"></path></g></g><g data-mml-node="mi" transform="translate(640,-150) scale(0.707)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g></g></g></g></svg></mjx-container>.  </li>
<li><strong>Loss</strong> — Token-level cross-entropy on <em>all</em> target positions</li>
</ol>

<mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -2.819ex;" xmlns="http://www.w3.org/2000/svg" width="29.353ex" height="7.044ex" role="img" focusable="false" viewBox="0 -1867.4 12974.1 3113.3"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="4C" d="M62 -22T47 -22T32 -11Q32 -1 56 24T83 55Q113 96 138 172T180 320T234 473T323 609Q364 649 419 677T531 705Q559 705 578 696T604 671T615 645T618 623V611Q618 582 615 571T598 548Q581 531 558 520T518 509Q503 509 503 520Q503 523 505 536T507 560Q507 590 494 610T452 630Q423 630 410 617Q367 578 333 492T271 301T233 170Q211 123 204 112L198 103L224 102Q281 102 369 79T509 52H523Q535 64 544 87T579 128Q616 152 641 152Q656 152 656 142Q656 101 588 40T433 -22Q381 -22 289 1T156 28L141 29L131 20Q111 0 87 -11Z"></path></g></g><g data-mml-node="TeXAtom" transform="translate(723,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mtext"><path data-c="43" d="M56 342Q56 428 89 500T174 615T283 681T391 705Q394 705 400 705T408 704Q499 704 569 636L582 624L612 663Q639 700 643 704Q644 704 647 704T653 705H657Q660 705 666 699V419L660 413H626Q620 419 619 430Q610 512 571 572T476 651Q457 658 426 658Q322 658 252 588Q173 509 173 342Q173 221 211 151Q232 111 263 84T328 45T384 29T428 24Q517 24 571 93T626 244Q626 251 632 257H660L666 251V236Q661 133 590 56T403 -21Q262 -21 159 83T56 342Z"></path><path data-c="45" d="M128 619Q121 626 117 628T101 631T58 634H25V680H597V676Q599 670 611 560T625 444V440H585V444Q584 447 582 465Q578 500 570 526T553 571T528 601T498 619T457 629T411 633T353 634Q266 634 251 633T233 622Q233 622 233 621Q232 619 232 497V376H286Q359 378 377 385Q413 401 416 469Q416 471 416 473V493H456V213H416V233Q415 268 408 288T383 317T349 328T297 330Q290 330 286 330H232V196V114Q232 57 237 52Q243 47 289 47H340H391Q428 47 452 50T505 62T552 92T584 146Q594 172 599 200T607 247T612 270V273H652V270Q651 267 632 137T610 3V0H25V46H58Q100 47 109 49T128 61V619Z" transform="translate(722,0)"></path></g></g></g><g data-mml-node="mo" transform="translate(2042.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mo" transform="translate(3098.6,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="munderover" transform="translate(4043.3,0)"><g data-mml-node="mo"><path data-c="2211" d="M60 948Q63 950 665 950H1267L1325 815Q1384 677 1388 669H1348L1341 683Q1320 724 1285 761Q1235 809 1174 838T1033 881T882 898T699 902H574H543H251L259 891Q722 258 724 252Q725 250 724 246Q721 243 460 -56L196 -356Q196 -357 407 -357Q459 -357 548 -357T676 -358Q812 -358 896 -353T1063 -332T1204 -283T1307 -196Q1328 -170 1348 -124H1388Q1388 -125 1381 -145T1356 -210T1325 -294L1267 -449L666 -450Q64 -450 61 -448Q55 -446 55 -439Q55 -437 57 -433L590 177Q590 178 557 222T452 366T322 544L56 909L55 924Q55 945 60 948Z"></path></g><g data-mml-node="TeXAtom" transform="translate(142.5,-1087.9) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mo" transform="translate(361,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(1139,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><g data-mml-node="TeXAtom" transform="translate(310.8,1237.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo" transform="translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(278,0)"><g data-mml-node="mi"><path data-c="1D432" d="M84 -102Q84 -110 87 -119T102 -138T133 -149Q148 -148 162 -143T186 -131T206 -114T222 -95T234 -76T243 -59T249 -45T252 -37L269 0L96 382H26V444H34Q49 441 146 441Q252 441 270 444H279V382H255Q232 382 232 380L337 151L442 382H394V444H401Q413 441 495 441Q568 441 574 444H580V382H510L406 152Q298 -84 297 -87Q269 -139 225 -169T131 -200Q85 -200 54 -172T23 -100Q23 -64 44 -50T87 -35Q111 -35 130 -50T152 -92V-100H84V-102Z"></path></g></g><g data-mml-node="mo" transform="translate(885,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g></g></g><g data-mml-node="mi" transform="translate(5654,0)"><path data-c="6C" d="M42 46H56Q95 46 103 60V68Q103 77 103 91T103 124T104 167T104 217T104 272T104 329Q104 366 104 407T104 482T104 542T103 586T103 603Q100 622 89 628T44 637H26V660Q26 683 28 683L38 684Q48 685 67 686T104 688Q121 689 141 690T171 693T182 694H185V379Q185 62 186 60Q190 52 198 49Q219 46 247 46H263V0H255L232 1Q209 2 183 2T145 3T107 3T57 1L34 0H26V46H42Z"></path><path data-c="6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z" transform="translate(278,0)"></path><path data-c="67" d="M329 409Q373 453 429 453Q459 453 472 434T485 396Q485 382 476 371T449 360Q416 360 412 390Q410 404 415 411Q415 412 416 414V415Q388 412 363 393Q355 388 355 386Q355 385 359 381T368 369T379 351T388 325T392 292Q392 230 343 187T222 143Q172 143 123 171Q112 153 112 133Q112 98 138 81Q147 75 155 75T227 73Q311 72 335 67Q396 58 431 26Q470 -13 470 -72Q470 -139 392 -175Q332 -206 250 -206Q167 -206 107 -175Q29 -140 29 -75Q29 -39 50 -15T92 18L103 24Q67 55 67 108Q67 155 96 193Q52 237 52 292Q52 355 102 398T223 442Q274 442 318 416L329 409ZM299 343Q294 371 273 387T221 404Q192 404 171 388T145 343Q142 326 142 292Q142 248 149 227T179 192Q196 182 222 182Q244 182 260 189T283 207T294 227T299 242Q302 258 302 292T299 343ZM403 -75Q403 -50 389 -34T348 -11T299 -2T245 0H218Q151 0 138 -6Q118 -15 107 -34T95 -74Q95 -84 101 -97T122 -127T170 -155T250 -167Q319 -167 361 -139T403 -75Z" transform="translate(778,0)"></path></g><g data-mml-node="mo" transform="translate(6932,0)"><path data-c="2061" d=""></path></g><g data-mml-node="msub" transform="translate(7098.6,0)"><g data-mml-node="mi"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></g><g data-mml-node="mi" transform="translate(536,-150) scale(0.707)"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g></g><g data-mml-node="mstyle" transform="translate(8016.3,0)"><g data-mml-node="mspace"></g></g><g data-mml-node="TeXAtom" data-mjx-texclass="OPEN" transform="translate(7849.3,0)"><g data-mml-node="mo" transform="translate(0 -0.5)"><path data-c="28" d="M152 251Q152 646 388 850H416Q422 844 422 841Q422 837 403 816T357 753T302 649T255 482T236 250Q236 124 255 19T301 -147T356 -251T403 -315T422 -340Q422 -343 416 -349H388Q359 -325 332 -296T271 -213T212 -97T170 56T152 251Z"></path></g></g><g data-mml-node="msub" transform="translate(8307.3,0)"><g data-mml-node="mi"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(523,-150) scale(0.707)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g></g><g data-mml-node="mo" transform="translate(9413.3,0)"><path data-c="2223" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="msub" transform="translate(9969.1,0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D432" d="M84 -102Q84 -110 87 -119T102 -138T133 -149Q148 -148 162 -143T186 -131T206 -114T222 -95T234 -76T243 -59T249 -45T252 -37L269 0L96 382H26V444H34Q49 441 146 441Q252 441 270 444H279V382H255Q232 382 232 380L337 151L442 382H394V444H401Q413 441 495 441Q568 441 574 444H580V382H510L406 152Q298 -84 297 -87Q269 -139 225 -169T131 -200Q85 -200 54 -172T23 -100Q23 -64 44 -50T87 -35Q111 -35 130 -50T152 -92V-100H84V-102Z"></path></g></g><g data-mml-node="TeXAtom" transform="translate(640,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><path data-c="3C" d="M694 -11T694 -19T688 -33T678 -40Q671 -40 524 29T234 166L90 235Q83 240 83 250Q83 261 91 266Q664 540 678 540Q681 540 687 534T694 519T687 505Q686 504 417 376L151 250L417 124Q686 -4 687 -5Q694 -11 694 -19Z"></path></g><g data-mml-node="mi" transform="translate(778,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g></g></g><g data-mml-node="mo" transform="translate(11464.5,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(11909.1,0)"><g data-mml-node="mi"><path data-c="1D431" d="M227 0Q212 3 121 3Q40 3 28 0H21V62H117L245 213L109 382H26V444H34Q49 441 143 441Q247 441 265 444H274V382H246L281 339Q315 297 316 297Q320 297 354 341L389 382H352V444H360Q375 441 466 441Q547 441 559 444H566V382H471L355 246L504 63L545 62H586V0H578Q563 3 469 3Q365 3 347 0H338V62H366Q366 63 326 112T285 163L198 63L217 62H235V0H227Z"></path></g></g><g data-mml-node="TeXAtom" data-mjx-texclass="CLOSE" transform="translate(12516.1,0)"><g data-mml-node="mo" transform="translate(0 -0.5)"><path data-c="29" d="M305 251Q305 -145 69 -349H56Q43 -349 39 -347T35 -338Q37 -333 60 -307T108 -239T160 -136T204 27T221 250T204 473T160 636T108 740T60 807T35 839Q35 850 50 850H56H69Q197 743 256 566Q305 425 305 251Z"></path></g></g></g></g></svg></mjx-container>


<p>plus the same label-smoothing trick.</p>
<hr>
<h3 id="Training-Strategy-for-Modern-Multilingual-NMT-Systems"><a href="#Training-Strategy-for-Modern-Multilingual-NMT-Systems" class="headerlink" title="Training Strategy for Modern Multilingual NMT Systems"></a><strong>Training Strategy for Modern Multilingual NMT Systems</strong></h3><p>Modern multilingual Neural Machine Translation (NMT) models, such as those used in services like Google Translate, follow a multi-stage training process to achieve high performance across many language pairs. The pipeline typically includes the following stages:</p>
<hr>
<h3 id="1-Pretraining-Cross-Lingual-Language-Modeling"><a href="#1-Pretraining-Cross-Lingual-Language-Modeling" class="headerlink" title="1. Pretraining (Cross-Lingual Language Modeling)"></a><strong>1. Pretraining (Cross-Lingual Language Modeling)</strong></h3><p><strong>Purpose:</strong><br>Learn universal language patterns and cross-lingual representations before any translation-specific training.</p>
<p><strong>Data Used:</strong><br>Large-scale monolingual corpora from many languages, often mined from web sources.</p>
<p><strong>Training Objectives:</strong></p>
<ul>
<li><strong>Masked Language Modeling (MLM)</strong>: Predict masked tokens in a sentence.</li>
<li><strong>Denoising Auto-Encoding</strong>: Reconstruct corrupted text sequences.</li>
<li><strong>Translation Language Modeling</strong> (less common): Learn cross-lingual alignment using bilingual input.</li>
</ul>
<p><strong>Model Behavior:</strong><br>The model learns general linguistic features across languages, which helps especially in low-resource scenarios.</p>
<hr>
<h3 id="2-Supervised-Fine-Tuning-Translation-Modeling"><a href="#2-Supervised-Fine-Tuning-Translation-Modeling" class="headerlink" title="2. Supervised Fine-Tuning (Translation Modeling)"></a><strong>2. Supervised Fine-Tuning (Translation Modeling)</strong></h3><p><strong>Purpose:</strong><br>Teach the model how to translate between languages using sentence-aligned parallel corpora.</p>
<p><strong>Data Used:</strong><br>Large-scale parallel corpora, mostly English-centric but also covering non-English pairs. Synthetic data (e.g., back-translated text) is also used to supplement low-resource languages.</p>
<p><strong>Training Strategy:</strong></p>
<ul>
<li>All language pairs are trained together in a single model (many-to-many).</li>
<li>Special tokens are added to indicate target language.</li>
<li>Shared subword vocabulary (e.g., BPE or SentencePiece) is used.</li>
<li>Sampling strategies are applied to balance low-resource and high-resource languages.</li>
</ul>
<p><strong>Augmentation Techniques:</strong></p>
<ul>
<li><strong>Back-Translation:</strong> Use a trained model to generate synthetic source sentences from target-language monolingual text.</li>
<li><strong>Knowledge Distillation:</strong> Train on simplified outputs from a teacher model.</li>
<li><strong>Forward-Translation:</strong> Less common, used if target-language monolingual data is limited.</li>
</ul>
<p><strong>Zero-shot Capability:</strong><br>A well-trained multilingual model can often translate between unseen language pairs by transferring knowledge from related pairs.</p>
<hr>
<h3 id="3-Domain-Adaptation-Specialized-Fine-Tuning"><a href="#3-Domain-Adaptation-Specialized-Fine-Tuning" class="headerlink" title="3. Domain Adaptation (Specialized Fine-Tuning)"></a><strong>3. Domain Adaptation (Specialized Fine-Tuning)</strong></h3><p><strong>Purpose:</strong><br>Improve translation quality in specific domains like healthcare, law, or conversational text.</p>
<p><strong>Data Used:</strong><br>Smaller, domain-specific parallel corpora, sometimes supplemented by synthetic domain data via back-translation.</p>
<p><strong>Training Strategy:</strong></p>
<ul>
<li>Fine-tune the general multilingual model on in-domain data.</li>
<li>Use techniques like early stopping, data mixing, or regularization to avoid overfitting.</li>
<li>Multi-stage adaptation (e.g., from multilingual → language pair → domain) often performs best.</li>
<li>Lightweight adapter modules can be added to specialize models per domain without altering the base model.</li>
</ul>
<hr>
<h3 id="Recent-Trends-and-Innovations"><a href="#Recent-Trends-and-Innovations" class="headerlink" title="Recent Trends and Innovations"></a><strong>Recent Trends and Innovations</strong></h3><ul>
<li><strong>Scaling Up:</strong> Large sparse models (e.g., Mixture-of-Experts) enable translation across hundreds of languages efficiently.</li>
<li><strong>Data Mining:</strong> Automated mining of high-quality sentence pairs from web data improves coverage, especially for low-resource languages.</li>
<li><strong>Balancing Techniques:</strong> Sampling strategies and vocabulary sharing improve performance across imbalanced datasets.</li>
<li><strong>LLMs for Translation:</strong> Large language models trained on multilingual data (e.g., GPT-4, PaLM) show strong translation abilities even without dedicated parallel data. They’re now being adapted for NMT use or employed to generate synthetic training data.</li>
<li><strong>Multitask and Modular Architectures:</strong> Supporting multiple tasks or inserting domain-specific adapters increases flexibility and robustness.</li>
</ul>
<hr>
<h3 id="End-to-End-Summary"><a href="#End-to-End-Summary" class="headerlink" title="End-to-End Summary"></a><strong>End-to-End Summary</strong></h3><p>Multilingual NMT systems are trained in stages—starting with monolingual self-supervised learning, followed by supervised translation training using real and synthetic parallel data, and optionally domain-specific fine-tuning. The training pipeline is carefully designed to handle large-scale, multilingual data while balancing quality across languages and domains. Ongoing advances in architecture, data quality, and LLM integration continue to push translation quality and coverage.</p>
<hr>
<h3 id="2-Training-Data-Collection"><a href="#2-Training-Data-Collection" class="headerlink" title="2. Training Data Collection"></a><strong>2. Training Data Collection</strong></h3><h4 id="A-Parallel-Corpora-Supervised"><a href="#A-Parallel-Corpora-Supervised" class="headerlink" title="A. Parallel Corpora (Supervised)"></a><strong>A. Parallel Corpora (Supervised)</strong></h4><ul>
<li><p><strong>Sources:</strong></p>
<ul>
<li><strong>Public Datasets:</strong> WMT, OPUS, Europarl, UN Corpus.</li>
<li><strong>Web Crawls:</strong> Filtered bilingual websites (e.g., Common Crawl, ParaCrawl).</li>
<li><strong>Official Documents:</strong> Government/legal text in multiple languages.</li>
</ul>
</li>
</ul>
<h4 id="B-Monolingual-Corpora-Unsupervised-Pretraining"><a href="#B-Monolingual-Corpora-Unsupervised-Pretraining" class="headerlink" title="B. Monolingual Corpora (Unsupervised/Pretraining)"></a><strong>B. Monolingual Corpora (Unsupervised/Pretraining)</strong></h4><ul>
<li><p><strong>Sources:</strong></p>
<ul>
<li><strong>C4, Wikipedia, CCNet, OSCAR, Gigaword, BookCorpus.</strong></li>
<li>Cleaned and language-labeled for the relevant set of translation languages.</li>
</ul>
</li>
</ul>
<h4 id="C-Quality-Filtering-Techniques"><a href="#C-Quality-Filtering-Techniques" class="headerlink" title="C. Quality Filtering Techniques"></a><strong>C. Quality Filtering Techniques</strong></h4><ul>
<li><strong>Length Ratio Filtering:</strong> Removes sentence pairs with large length mismatch.</li>
<li><strong>Language Identification:</strong> Ensures text is in the expected language.</li>
<li><strong>Dual-Model Agreement / BLEU Filtering:</strong> Keep only pairs with agreement between two translation models or high BLEU.</li>
<li><strong>Noise Detection:</strong> Drop automatically translated or low-confidence samples.</li>
</ul>
<h4 id="D-Data-Balancing"><a href="#D-Data-Balancing" class="headerlink" title="D. Data Balancing"></a><strong>D. Data Balancing</strong></h4><ul>
<li><p>Address <strong>low-resource language</strong> imbalance by:</p>
<ul>
<li><strong>Oversampling rare languages.</strong></li>
<li><strong>Sampling temperature strategies</strong> (e.g., square-root or inverse frequency).</li>
<li><strong>Synthetic data augmentation</strong> via back-translation or pivoting.</li>
</ul>
</li>
</ul>
<hr>
<h3 id="Sampling-Strategies-in-Generative-Models"><a href="#Sampling-Strategies-in-Generative-Models" class="headerlink" title="Sampling Strategies in Generative Models"></a>Sampling Strategies in Generative Models</h3><ol>
<li><strong>Deterministic methods</strong> (e.g., <em>greedy search</em>, <em>beam search</em>)</li>
<li><strong>Stochastic methods</strong> (e.g., <em>multinomial sampling</em>, <em>top-k</em>, <em>top-p</em>)</li>
</ol>
<p>In this chapter, we choose <strong>beam search</strong> for two key reasons:</p>
<h3 id="Why-Beam-Search"><a href="#Why-Beam-Search" class="headerlink" title="Why Beam Search?"></a>Why Beam Search?</h3><ul>
<li><p><strong>Translation Accuracy</strong>:<br>Beam search evaluates multiple candidate sequences and selects the most probable one, often resulting in more accurate and fluent translations.</p>
</li>
<li><p><strong>Consistency</strong>:<br>As a deterministic method, beam search always produces the same output given the same input. This is crucial for translation systems, where consistency and reliability outweigh diversity. Unlike creative applications, translation rarely benefits from random or surprising outputs.</p>
</li>
</ul>
<p>In contrast, <strong>stochastic sampling methods</strong> are better suited for tasks where <strong>diversity and creativity</strong> are desired, such as storytelling or dialogue generation. </p>
<hr>
<h2 id="Comparison-Deterministic-vs-Stochastic-Sampling"><a href="#Comparison-Deterministic-vs-Stochastic-Sampling" class="headerlink" title="Comparison: Deterministic vs. Stochastic Sampling"></a>Comparison: Deterministic vs. Stochastic Sampling</h2><table>
<thead>
<tr>
<th><strong>Characteristic</strong></th>
<th><strong>Deterministic Methods</strong></th>
<th><strong>Stochastic Methods</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>Approach</strong></td>
<td>Follows a predictable decision process</td>
<td>Samples from a probability distribution</td>
</tr>
<tr>
<td><strong>Efficiency</strong></td>
<td>Less efficient (tracks multiple paths)</td>
<td>More efficient due to randomness</td>
</tr>
<tr>
<td><strong>Output Quality</strong></td>
<td>Coherent and predictable</td>
<td>Diverse and imaginative</td>
</tr>
<tr>
<td><strong>Risk</strong></td>
<td>May produce repetitive sequences</td>
<td>May generate inappropriate or off-topic text</td>
</tr>
<tr>
<td><strong>Best For</strong></td>
<td>Tasks requiring consistency (e.g. translation)</td>
<td>Tasks requiring creativity (e.g. story generation)</td>
</tr>
<tr>
<td><strong>Common Methods</strong></td>
<td>Greedy search, beam search</td>
<td>Multinomial sampling, top-k, top-p sampling</td>
</tr>
</tbody></table>
<hr>
<h2 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h2><h3 id="Offline-Metrics"><a href="#Offline-Metrics" class="headerlink" title="Offline Metrics"></a>Offline Metrics</h3><h4 id="Key-Metrics"><a href="#Key-Metrics" class="headerlink" title="Key Metrics:"></a>Key Metrics:</h4><ul>
<li>BLEU (Bilingual Evaluation Understudy): One of the oldest and most widely used metrics, BLEU measures the n-gram precision between the machine-translated text and the reference translations. It focuses on the similarity of the output to high-quality human translations. While fast and easy to calculate, it can sometimes penalize translations that are semantically correct but lexically different from the references.</li>
<li>METEOR (Metric for Evaluation of Translation with Explicit ORdering): METEOR is an improvement upon BLEU that considers synonymy and stemming, leading to a better correlation with human judgment. It aligns words between the machine and reference translations and calculates a score based on precision, recall, and a fragmentation penalty.</li>
<li>COMET (Cross-lingual Optimized Metric for Evaluation of Translation): A more recent and powerful metric, COMET leverages large-scale pre-trained language models to assess the semantic similarity between the source text, the machine translation, and the reference translation. It has shown a very high correlation with human ratings.</li>
<li>BERTScore: This metric utilizes contextual embeddings from BERT to compare the similarity of tokens in the candidate and reference translations. This allows it to capture semantic nuances that n-gram-based metrics might miss.</li>
</ul>
<h4 id="Human-Evaluation"><a href="#Human-Evaluation" class="headerlink" title="Human Evaluation"></a>Human Evaluation</h4><ul>
<li>Adequacy and Fluency: This is a classic approach where human judges rate translations on two scales:</li>
<li>Adequacy: How well the meaning of the source text is preserved in the translation.</li>
<li>Fluency: How natural and grammatically correct the translated text sounds in the target language.</li>
<li>Direct Assessment (DA): In this method, human annotators assign a single, continuous score to a translation, often on a scale of 0 to 100, reflecting its overall quality. This approach has become increasingly popular in major translation conferences like WMT (Workshop on Machine Translation).</li>
<li>Ranking: Evaluators are presented with translations from multiple NMT systems for the same source sentence and are asked to rank them from best to worst. This is a relative measure that is useful for comparing the performance of different models.</li>
</ul>
<h4 id="LLM-as-a-Judge"><a href="#LLM-as-a-Judge" class="headerlink" title="LLM as a Judge"></a>LLM as a Judge</h4><p>The typical setup involves prompting an LLM with the source sentence, the candidate translation(s), and optionally a reference translation. The LLM is then instructed (via a carefully designed prompt) to perform tasks such as:</p>
<ul>
<li>Scoring translations on dimensions like adequacy, fluency, and faithfulness.</li>
<li>Choosing the better translation between multiple candidates (preference ranking).</li>
<li>Providing natural language feedback on errors or strengths in the output.</li>
</ul>
<p>LLMs can handle multilingual input and adapt to complex linguistic nuances, making them especially valuable for evaluating low-resource or morphologically rich languages where human evaluation is expensive or infeasible.</p>
<p>Companies often use few-shot or zero-shot prompting, and sometimes fine-tune LLMs with supervised data (e.g., human-labeled translation comparisons) to improve consistency and alignment with human judgments. These LLM-based evaluations correlate more closely with human ratings than BLEU scores and offer significant efficiency and scalability benefits.</p>
<p>For example:</p>
<ul>
<li>Google uses LLMs internally to benchmark translation quality across hundreds of languages.</li>
<li>OpenAI and others use LLMs to evaluate summarization, QA, and translation outputs in human preference studies.</li>
<li>Meta’s SEED framework and Anthropic’s Constitutional AI both involve LLM-based feedback loops to refine and assess generation quality.</li>
</ul>
<h3 id="Online-Metrics"><a href="#Online-Metrics" class="headerlink" title="Online Metrics"></a>Online Metrics</h3><ul>
<li><strong>User feedback</strong> – Rating prompt after translation (<em>Figure 3.25</em>).  </li>
<li><strong>User engagement</strong> – Usage frequency, session length, retention.</li>
</ul>
<hr>
<h2 id="Overall-ML-System-Design"><a href="#Overall-ML-System-Design" class="headerlink" title="Overall ML System Design"></a>Overall ML System Design</h2><h3 id="1-·-Language-Detector"><a href="#1-·-Language-Detector" class="headerlink" title="1 · Language Detector"></a>1 · Language Detector</h3><p>An <strong>encoder-only Transformer</strong> classifies the input language via:</p>
<ul>
<li><strong>Average pooling</strong> → prediction head, or  </li>
<li><strong>Last-token representation</strong> → prediction head.</li>
</ul>
<p><img src="/img/2025/06/f4e0ac0aa42ac2c8e1f8bac85dcc60dc20926b7c047fe6c379ee7e72dae921ea.webp" alt="Language Detector"></p>
<h3 id="2-·-Translation-Service"><a href="#2-·-Translation-Service" class="headerlink" title="2 · Translation Service"></a>2 · Translation Service</h3><ol>
<li>Detect language.  </li>
<li>Route to correct bilingual model.  </li>
<li>Perform beam search decoding.  </li>
<li>Detokenize and return text.</li>
</ol>
<hr>
<h2 id="Other-Talking-Points"><a href="#Other-Talking-Points" class="headerlink" title="Other Talking Points"></a>Other Talking Points</h2><p>If time remains, discuss:</p>
<ul>
<li>Model compression (quantization, distillation).  </li>
<li>On-device translation (Apple, offline mode).  </li>
<li>Handling new languages with limited corpora (few-shot, adapter layers).  </li>
<li>Real-time constraints for mobile typing suggestions.</li>
</ul>
<hr>
<h2 id="Real-Time-Translation"><a href="#Real-Time-Translation" class="headerlink" title="Real-Time Translation:"></a><strong>Real-Time Translation</strong>:</h2><h3 id="Replace-Full-Sequence-Encoder-Decoder-with-Streaming-Models"><a href="#Replace-Full-Sequence-Encoder-Decoder-with-Streaming-Models" class="headerlink" title="Replace Full-Sequence Encoder-Decoder with Streaming Models:"></a>Replace Full-Sequence Encoder-Decoder with <strong>Streaming Models</strong>:</h3><ul>
<li>Traditional Transformer waits for the <strong>entire source sentence</strong> before producing output → <strong>too slow</strong> for real-time.</li>
<li>Use a <strong>streaming architecture</strong> to process input incrementally:</li>
</ul>
<h4 id="Architectural-Options"><a href="#Architectural-Options" class="headerlink" title="Architectural Options:"></a>Architectural Options:</h4><table>
<thead>
<tr>
<th>Strategy</th>
<th>Description</th>
<th>Example</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Chunk-based Transformer</strong></td>
<td>Break input into fixed-size overlapping windows</td>
<td>SimulTrans, STACL</td>
</tr>
<tr>
<td><strong>Monotonic Attention</strong></td>
<td>Enforces left-to-right or wait-k policy</td>
<td>MoChA, Wait-k Transformer</td>
</tr>
<tr>
<td><strong>Transducer models</strong></td>
<td>Encoder-decoder-decoder (non-attention based)</td>
<td>RNN-T, Recurrent Neural Aligner</td>
</tr>
<tr>
<td><strong>Streaming Conformer</strong> (for speech)</td>
<td>Use <strong>causal attention</strong> and limited context for audio</td>
<td>Google’s real-time ST model</td>
</tr>
</tbody></table>
<h3 id="🛠-Techniques-to-Apply"><a href="#🛠-Techniques-to-Apply" class="headerlink" title="🛠 Techniques to Apply:"></a>🛠 Techniques to Apply:</h3><ul>
<li>Use <strong>causal self-attention</strong> (no future tokens).</li>
<li>Use <strong>incremental decoding</strong> (no full-sequence softmax).</li>
<li>Add <strong>latency-controlled decoding policies</strong> (e.g., wait-k beam search).</li>
</ul>
<hr>
<h2 id="On-Device-Translation"><a href="#On-Device-Translation" class="headerlink" title="On-Device Translation:"></a><strong>On-Device Translation</strong>:</h2><h3 id="Replace-Heavy-Transformer-with-Lightweight-Quantized-and-Distilled-Models"><a href="#Replace-Heavy-Transformer-with-Lightweight-Quantized-and-Distilled-Models" class="headerlink" title="Replace Heavy Transformer with Lightweight, Quantized, and Distilled Models:"></a>Replace Heavy Transformer with <strong>Lightweight, Quantized, and Distilled Models</strong>:</h3><h4 id="Architectural-Options-1"><a href="#Architectural-Options-1" class="headerlink" title="Architectural Options:"></a>Architectural Options:</h4><table>
<thead>
<tr>
<th>Model</th>
<th>Type</th>
<th>Notes</th>
</tr>
</thead>
<tbody><tr>
<td><strong>MobileBERT</strong>, <strong>TinyBERT</strong></td>
<td>Lightweight Transformers</td>
<td>Good encoder for on-device NMT</td>
</tr>
<tr>
<td><strong>DistilBART</strong>, <strong>DistilmBART</strong></td>
<td>Distilled encoder-decoder</td>
<td>Smaller decoder with similar quality</td>
</tr>
<tr>
<td><strong>mBART + pruning/quantization</strong></td>
<td>Multilingual</td>
<td>Use only specific heads/layers</td>
</tr>
<tr>
<td><strong>FNet</strong></td>
<td>Replaces self-attn with Fourier Transform</td>
<td>Super fast &amp; small</td>
</tr>
<tr>
<td><strong>Linear Attention Models</strong> (Performer, Linformer)</td>
<td>Approximate attention for low-memory devices</td>
<td>Good for constrained decoding</td>
</tr>
</tbody></table>
<h3 id="🛠-Key-Optimization-Techniques"><a href="#🛠-Key-Optimization-Techniques" class="headerlink" title="🛠 Key Optimization Techniques:"></a>🛠 Key Optimization Techniques:</h3><ul>
<li><strong>INT8/INT4 quantization</strong> (QAT or PTQ)</li>
<li><strong>Weight pruning</strong> + <strong>layer dropout</strong></li>
<li><strong>LoRA/Adapters</strong> to allow per-language tuning without retraining the full model</li>
<li>Deploy with <strong>ONNX</strong>, <strong>TensorRT</strong>, <strong>TensorFlow Lite</strong>, or <strong>CoreML</strong></li>
</ul>
<hr>
]]></content>
      <tags>
        <tag>技术</tag>
        <tag>Sytem Design</tag>
      </tags>
  </entry>
</search>
